{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73024df1-2317-4f45-bd08-d2b5e257065b",
   "metadata": {},
   "source": [
    "# GAN for Molecular Generation Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af0ee7-d586-4163-bc26-849f2aab3518",
   "metadata": {},
   "source": [
    "## GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7276fdbc-d2ab-4056-afd5-4a14e11253ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363f1210-f25a-4a8b-b11c-0aff8f8f24e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2723, 128, 32, 1]),\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-3.5364208>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.512889>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0026531518>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LENGTH = 128\n",
    "DFF = 32\n",
    "\n",
    "# shape: (batch_size, length, dff)\n",
    "cb2_embeddings = np.load(\"./data/processed_cb2.npy\")\n",
    "cb2_embeddings = tf.expand_dims(cb2_embeddings, axis=-1)\n",
    "cb2_embeddings = tf.cast(cb2_embeddings, dtype=tf.float32)\n",
    "\n",
    "cb2_embeddings.shape, tf.math.reduce_min(cb2_embeddings), tf.math.reduce_max(cb2_embeddings), tf.math.reduce_mean(cb2_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec22aa-5e47-455b-b00e-ee5db7fa7455",
   "metadata": {},
   "source": [
    "##### Optional normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216341e6-e403-420b-a53e-c6eee8dd0d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2723, 128, 32, 1]),\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-3.5364208>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.512889>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0026531518>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLD_RANGE = tf.math.reduce_max(cb2_embeddings) - tf.math.reduce_min(cb2_embeddings)\n",
    "OLD_MIN = tf.math.reduce_min(cb2_embeddings)\n",
    "NEW_RANGE = 1.0 - 0.0\n",
    "\n",
    "# cb2_embeddings = tf.divide(\n",
    "#    tf.subtract(\n",
    "#       cb2_embeddings, \n",
    "#       tf.reduce_min(cb2_embeddings)\n",
    "#    ), \n",
    "#    tf.subtract(\n",
    "#       tf.reduce_max(cb2_embeddings), \n",
    "#       tf.reduce_min(cb2_embeddings)\n",
    "#    )\n",
    "# )\n",
    "\n",
    "cb2_embeddings.shape, tf.math.reduce_min(cb2_embeddings), tf.math.reduce_max(cb2_embeddings), tf.math.reduce_mean(cb2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b64605-bc0d-40a0-9f5c-7df4b9013ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2723, 128, 32, 1]),\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.50204545>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb2_embeddings = tf.divide(\n",
    "    tf.multiply( tf.subtract(cb2_embeddings, OLD_MIN), NEW_RANGE ), OLD_RANGE\n",
    ")\n",
    "\n",
    "cb2_embeddings.shape, tf.math.reduce_min(cb2_embeddings), tf.math.reduce_max(cb2_embeddings), tf.math.reduce_mean(cb2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a8cba2-0b0e-49c9-af3a-874fd48d0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cb2_embeddings = tf.add(\n",
    "    \n",
    "#     tf.divide(\n",
    "#         tf.multiply( tf.subtract(cb2_embeddings, 0), OLD_RANGE ), NEW_RANGE\n",
    "#     ),\n",
    "\n",
    "#     OLD_MIN\n",
    "    \n",
    "# )\n",
    "\n",
    "# cb2_embeddings.shape, tf.math.reduce_min(cb2_embeddings), tf.math.reduce_max(cb2_embeddings), tf.math.reduce_mean(cb2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1097a931-fe6d-49f8-a067-af8e0b4a5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cb2_embeddings = tf.multiply(cb2_embeddings, 3.55868)\n",
    "\n",
    "# cb2_embeddings.shape, tf.math.reduce_min(cb2_embeddings), tf.math.reduce_max(cb2_embeddings), tf.math.reduce_mean(cb2_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a265b-176c-4e2a-9fdd-c479472c29b7",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d028c7-f16f-4fca-90f5-fadb19e1cae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16384)             1638400   \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 16384)             65536     \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16384)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 8, 128)        819200    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 16, 64)        204800    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 128, 32, 1)        1600      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2729536 (10.41 MB)\n",
      "Trainable params: 2696768 (10.29 MB)\n",
      "Non-trainable params: 32768 (128.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(8 * 8 * 256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "    assert model.output_shape == (None, 8, 8, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 16, 8, 128)\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 32, 16, 64)\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(4, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 128, 32, 1)\n",
    "\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed496596-4ad4-46e2-939a-2d15c2136644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 16, 64)        1664      \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 64, 16, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 8, 128)        204928    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 8, 128)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32, 8, 128)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 4, 256)        819456    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16, 4, 256)        0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16, 4, 256)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 16385     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1042433 (3.98 MB)\n",
      "Trainable params: 1042433 (3.98 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[128, 32, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a3da7ab-d51e-48cf-8e2d-76a1541c576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    # fake_loss = -tf.reduce_mean(fake_output)\n",
    "    # real_loss = -tf.reduce_mean(fake_output)\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # return -tf.reduce_mean(fake_output)\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.legacy.Adam(.025)\n",
    "discriminator_optimizer = tf.keras.optimizers.legacy.Adam(0.0000025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d66bb7f-5863-4237-907b-e71d89c20241",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d978f09-ca13-4e1b-9a98-1c96c9eb4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_valid_smiles(smiles):\n",
    "    return tf.numpy_function(is_valid_smiles, [smiles], tf.int64)\n",
    "\n",
    "def penalize_invalid_smiles(smiles):\n",
    "    return tf.numpy_function(is_not_valid_smiles, [smiles], tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4278b455-c681-47d0-87bd-a948baf58364",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    BATCH_SIZE = 8\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        # Wasserstein Loss for Generator\n",
    "        gen_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "        # Wasserstein Loss for Discriminator\n",
    "        disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "        # Gradient Penalty (Optional but can improve WGAN training)\n",
    "        epsilon = tf.random.uniform(shape=[BATCH_SIZE, 1, 1, 1], minval=0.0, maxval=1.0)\n",
    "        epsilon = tf.broadcast_to(epsilon, images.shape)  # Match the shape of epsilon to images\n",
    "        interpolated_images = epsilon * images + (1 - epsilon) * generated_images\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_images)\n",
    "            pred_interpolated = discriminator(interpolated_images, training=True)\n",
    "        gradients = gp_tape.gradient(pred_interpolated, interpolated_images)\n",
    "        gradients_l2 = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(gradients_l2 - 1.0))\n",
    "        disc_loss += 50 * gradient_penalty  # lambda_gp is the gradient penalty weight\n",
    "\n",
    "        # Filter and Reward Valid SMILES\n",
    "        valid_indices = tf.map_fn(reward_valid_smiles, generated_images, dtype=tf.int64)\n",
    "        invalid_indices = tf.map_fn(penalize_invalid_smiles, generated_images, dtype=tf.int64)\n",
    "\n",
    "        # Calculate the average reward for valid SMILES\n",
    "        average_reward = tf.reduce_mean(valid_indices)\n",
    "        average_penalty = tf.reduce_mean(invalid_indices)\n",
    "\n",
    "        # Add the reward to the generator loss\n",
    "        gen_loss += tf.cast(average_reward, dtype=tf.float32) + tf.cast(average_penalty, dtype=tf.float32)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3988b566-1119-4bfa-850b-5ffa3a3c581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './gan_fresh_4_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e01e9d-fcc6-416c-9c0a-2fad9c1a56e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2a54f2450>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39d6ec4b-b6b8-453b-95b9-0ec14ebabacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2723, 128, 32, 1]), 2723)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb2_embeddings.shape, len(cb2_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410251c-ea93-4488-95f1-76a7c14197b6",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54eef971-183d-461f-90b6-2f23ccca699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7928d440-4051-4878-98c0-0997ac9b6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99cd0dfc-b28f-4783-a742-23f891f14784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2902f348-2598-4f35-869c-d03d56fd2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 32\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 510\n",
    "target_vocab_size = 510\n",
    "dropout_rate = 0.2\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9e7a86c-1a4b-40fb-b5a0-57e7e1bfc86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54cb265d-5acd-4106-859a-8dfb516873da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6479cec-fc83-4286-8269-79cc13619405",
   "metadata": {},
   "source": [
    "## SMILES Generation + Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2e3f2e7-af4e-44fb-ae38-f47ea9c7ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import SmilesTokenizer\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "\n",
    "def gen_smiles(generator, transformer):\n",
    "    input_sequence = np.random.randint(0, 60, size=(1, 128))\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_sequence, input_sequence)\n",
    "    enc_output = generator(tf.random.normal([1, 100]), training=False)\n",
    "    enc_output = tf.squeeze(enc_output, axis=-1)\n",
    "    decoder_output, _ = transformer.decoder(input_sequence, enc_output, False, combined_mask, dec_padding_mask)\n",
    "    final_output = transformer.final_layer(decoder_output)\n",
    "\n",
    "    smiles = []\n",
    "    for row in final_output[0]:\n",
    "        smiles.append(np.argmax(row))\n",
    "\n",
    "    smiles = tokenizer.decode(smiles).replace(' ', '')\n",
    "    return smiles[:smiles.find('[SEP]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90be1def-a65a-4f53-a8ab-e0deec4816fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tokenizer import SmilesTokenizer\n",
    "\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "EPOCH = 1\n",
    "norm_states = dict()\n",
    "def norm(smiles):\n",
    "    norm_states[EPOCH] = [[smiles.shape, np.min(smiles), np.max(smiles), np.mean(smiles)]]\n",
    "    \n",
    "    OLD_MIN_2 = np.min(smiles)\n",
    "    NEW_RANGE = np.max(smiles) - np.min(smiles)\n",
    "    smiles = ( ( (smiles - OLD_MIN_2) * OLD_RANGE) / NEW_RANGE) + OLD_MIN\n",
    "    \n",
    "    norm_states[EPOCH].append( [smiles.shape, np.min(smiles), np.max(smiles), np.mean(smiles)] )\n",
    "    \n",
    "    return smiles\n",
    "\n",
    "def is_valid_smiles(smiles):\n",
    "    smiles = norm(smiles)\n",
    "    \n",
    "    input_sequence = np.random.randint(0, 60, size=(1, 128))\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_sequence, input_sequence)\n",
    "    enc_output = tf.squeeze(smiles, axis=-1)\n",
    "    decoder_output, _ = transformer.decoder(input_sequence, enc_output, False, combined_mask, dec_padding_mask)\n",
    "    final_output = transformer.final_layer(decoder_output)\n",
    "\n",
    "    smiles = []\n",
    "    for row in final_output[0]:\n",
    "        smiles.append(np.argmax(row))\n",
    "\n",
    "    smiles = tokenizer.decode(smiles).replace(' ', '')\n",
    "    smiles = smiles[:smiles.find('[SEP]')]\n",
    "\n",
    "    x = Chem.MolFromSmiles(smiles)\n",
    "    if x is None or smiles == '' or 128 > len(smiles) > 10:\n",
    "        return 0\n",
    "    print(\"valid SMILES generated\", smiles)\n",
    "    return 1\n",
    "\n",
    "def is_not_valid_smiles(smiles):\n",
    "    smiles = norm(smiles)\n",
    "    \n",
    "    input_sequence = np.random.randint(0, 60, size=(1, 128))\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_sequence, input_sequence)\n",
    "    enc_output = tf.squeeze(smiles, axis=-1)\n",
    "    decoder_output, _ = transformer.decoder(input_sequence, enc_output, False, combined_mask, dec_padding_mask)\n",
    "    final_output = transformer.final_layer(decoder_output)\n",
    "\n",
    "    smiles = []\n",
    "    for row in final_output[0]:\n",
    "        smiles.append(np.argmax(row))\n",
    "\n",
    "    smiles = tokenizer.decode(smiles).replace(' ', '')\n",
    "    smiles = smiles[:smiles.find('[SEP]')]\n",
    "\n",
    "    x = Chem.MolFromSmiles(smiles)\n",
    "    if x is None or smiles == '' or 128 > len(smiles) > 10:\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09984e24-1a01-4ce5-a576-dbec6e36a6d5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad699fea-c390-4d22-8900-bf164ad50d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "batch_size = 8\n",
    "\n",
    "def train(dataset, epochs):\n",
    "  for EPOCH in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(batch_size, len(dataset), batch_size):\n",
    "      input = dataset[i-batch_size : i]\n",
    "      train_step(input)\n",
    "\n",
    "    if (EPOCH + 1) % 5 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "      print('saved checkpoint')\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(EPOCH + 1, time.time()-start))\n",
    "    \n",
    "    noise = tf.random.normal([1, 100])\n",
    "    generated_image = generator(noise, training=False)\n",
    "    decision = discriminator(generated_image)\n",
    "    print(decision.numpy()[0][0], gen_smiles(generator, transformer), '\\n' )\n",
    "    print(norm_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74d04052-eee1-4309-9c3a-a564b699e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from rdkit import RDLogger\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_rdkit_warnings():\n",
    "    RDLogger.DisableLog('rdApp.error')\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        RDLogger.EnableLog('rdApp.error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d7d5cd5-dd85-43b0-bde8-75a50892d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/preetam/anaconda3/envs/deepmgm2/lib/python3.11/site-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "valid SMILES generated C\n",
      "valid SMILES generated OCO\n",
      "valid SMILES generated OOO\n",
      "Time for epoch 1 is 203.0675241947174 sec\n",
      "0.5069204 )C)n))))nn))n)))))nnnn)nNnnnn)nnccnc)cc)=ccc(Occccc(ccc(=c(( \n",
      "\n",
      "{1: [[(128, 32, 1), -0.99999976, 0.99999976, 0.015136715], [TensorShape([128, 32, 1]), -3.5364208, 3.512889, 0.04158578]]}\n"
     ]
    }
   ],
   "source": [
    "with suppress_rdkit_warnings():\n",
    "    train(cb2_embeddings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27b94488-4fda-46fe-a72b-dffd6272ce33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./gan_fresh_4_checkpoints/ckpt-7'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b26f37-256f-47b1-811d-fed88e0d8ce6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939f4cd-b80c-4297-8d9f-ac7aaa73463c",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1b4ac-3c85-415d-8dfb-67fe0fa8a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d329e-c214-4b3e-a12d-69ae827da7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a415a1-36f1-4e4b-8755-c471ff0b509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983be41-d0b1-4521-954f-63b46677e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2643223-0938-42e2-8c83-97be22a21131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0dfd8-bb40-4d72-b939-8b3c202f5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1e91b-2a5c-4cdc-b40e-363838a721a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e632f13-4123-4904-8961-dac21065fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae452f-f076-4dc1-9683-49bdceddbf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31f848-c4f7-48ad-a1ff-96757f2efb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 32\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 510\n",
    "target_vocab_size = 510\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744e16b-3910-4d62-906f-9fe32ac80c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f436af9-13d5-42aa-98e4-7e619c585f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99c1ed-36b4-459d-adb8-90b89c1cd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377813f6-016c-4d6e-8af3-fc8350ad57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30528512-60b6-4756-8af0-24398941baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887c818-03e0-407b-bf80-8339bd90ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d53ba3-a364-46db-9bde-447e606e624f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generate SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa3468-b2aa-4c8d-927a-9e83678ed23b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tokenizer import SmilesTokenizer\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "\n",
    "def gen_smiles(generator, transformer):\n",
    "    input_sequence = np.random.randint(0, 60, size=(1, 128))\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_sequence, input_sequence)\n",
    "    enc_output = generator(tf.random.normal([1, 100]), training=False)\n",
    "    enc_output = tf.squeeze(enc_output, axis=-1)\n",
    "    decoder_output, _ = transformer.decoder(input_sequence, enc_output, False, combined_mask, dec_padding_mask)\n",
    "    final_output = transformer.final_layer(decoder_output)\n",
    "\n",
    "    smiles = []\n",
    "    for row in final_output[0]:\n",
    "        smiles.append(np.argmax(row))\n",
    "\n",
    "    smiles = tokenizer.decode(smiles).replace(' ', '')\n",
    "    return smiles[:smiles.find('[SEP]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40cecd-5eeb-4c7e-85e0-b25d91e425da",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = []\n",
    "for _ in range(1000):\n",
    "    smiles.append( gen_smiles(generator, transformer) )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469546e-f672-4f94-a8dd-6c951574069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "def is_valid_smiles(smiles):\n",
    "    smiles_str = smiles.decode('utf-8')  # Convert from byte string to Python string\n",
    "    x = Chem.MolFromSmiles(smiles_str)\n",
    "    if x is None:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# valids = 0\n",
    "# for smile in smiles:\n",
    "#     if is_valid_smiles(smile):\n",
    "#         valids += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d9e48-924a-4f0c-8898-c061dc154322",
   "metadata": {},
   "outputs": [],
   "source": [
    "valids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d6d85-f18c-4bc2-9577-78c5890064f9",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9e43369-cb0f-47da-a708-f0bf46da684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tokenizer import SmilesTokenizer\n",
    "\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "\n",
    "def gen_smiles(generator, transformer):\n",
    "    input_sequence = np.random.randint(0, 60, size=(1, 128))\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_sequence, input_sequence)\n",
    "    enc_output = generator(tf.random.normal([1, 100]), training=False)\n",
    "    \n",
    "    enc_output = norm(enc_output)\n",
    "    \n",
    "    enc_output = tf.squeeze(enc_output, axis=-1)\n",
    "    decoder_output, _ = transformer.decoder(input_sequence, enc_output, False, combined_mask, dec_padding_mask)\n",
    "    final_output = transformer.final_layer(decoder_output)\n",
    "\n",
    "    smiles = []\n",
    "    for row in final_output[0]:\n",
    "        smiles.append(np.argmax(row))\n",
    "\n",
    "    smiles = tokenizer.decode(smiles).replace(' ', '')\n",
    "\n",
    "    x = Chem.MolFromSmiles(smiles, sanitize=False)\n",
    "    if x is None or smiles == '':\n",
    "        return (smiles[:smiles.find('[SEP]')], 0)\n",
    "        \n",
    "    x = Chem.MolFromSmiles(smiles)\n",
    "    if x is None:\n",
    "        return (smiles[:smiles.find('[SEP]')], 2)\n",
    "\n",
    "    return (smiles[:smiles.find('[SEP]')], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ebde6cb-846e-4381-846b-946515e9408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 smiles generated\n",
      "100 smiles generated\n",
      "200 smiles generated\n",
      "300 smiles generated\n",
      "400 smiles generated\n",
      "500 smiles generated\n",
      "600 smiles generated\n",
      "700 smiles generated\n",
      "800 smiles generated\n",
      "900 smiles generated\n",
      "1000 smiles generated\n",
      "1100 smiles generated\n",
      "1200 smiles generated\n",
      "1300 smiles generated\n",
      "1400 smiles generated\n",
      "1500 smiles generated\n",
      "1600 smiles generated\n",
      "1700 smiles generated\n",
      "1800 smiles generated\n",
      "1900 smiles generated\n",
      "2000 smiles generated\n",
      "2100 smiles generated\n",
      "2200 smiles generated\n",
      "2300 smiles generated\n",
      "2400 smiles generated\n",
      "2500 smiles generated\n",
      "2600 smiles generated\n",
      "2700 smiles generated\n",
      "2800 smiles generated\n",
      "2900 smiles generated\n",
      "3000 smiles generated\n",
      "3100 smiles generated\n",
      "3200 smiles generated\n",
      "3300 smiles generated\n",
      "3400 smiles generated\n",
      "3500 smiles generated\n",
      "3600 smiles generated\n",
      "3700 smiles generated\n",
      "3800 smiles generated\n",
      "3900 smiles generated\n",
      "4000 smiles generated\n",
      "4100 smiles generated\n",
      "4200 smiles generated\n",
      "4300 smiles generated\n",
      "4400 smiles generated\n",
      "4500 smiles generated\n",
      "4600 smiles generated\n",
      "4700 smiles generated\n",
      "4800 smiles generated\n",
      "4900 smiles generated\n",
      "5000 smiles generated\n",
      "5100 smiles generated\n",
      "5200 smiles generated\n",
      "5300 smiles generated\n",
      "5400 smiles generated\n",
      "5500 smiles generated\n",
      "5600 smiles generated\n",
      "5700 smiles generated\n",
      "5800 smiles generated\n",
      "5900 smiles generated\n",
      "6000 smiles generated\n",
      "6100 smiles generated\n",
      "6200 smiles generated\n",
      "6300 smiles generated\n",
      "6400 smiles generated\n",
      "6500 smiles generated\n",
      "6600 smiles generated\n",
      "6700 smiles generated\n",
      "6800 smiles generated\n",
      "6900 smiles generated\n",
      "7000 smiles generated\n",
      "7100 smiles generated\n",
      "7200 smiles generated\n",
      "7300 smiles generated\n",
      "7400 smiles generated\n",
      "7500 smiles generated\n",
      "7600 smiles generated\n",
      "7700 smiles generated\n",
      "7800 smiles generated\n",
      "7900 smiles generated\n",
      "8000 smiles generated\n",
      "8100 smiles generated\n",
      "8200 smiles generated\n",
      "8300 smiles generated\n",
      "8400 smiles generated\n",
      "8500 smiles generated\n",
      "8600 smiles generated\n",
      "8700 smiles generated\n",
      "8800 smiles generated\n",
      "8900 smiles generated\n",
      "9000 smiles generated\n",
      "9100 smiles generated\n",
      "9200 smiles generated\n",
      "9300 smiles generated\n",
      "9400 smiles generated\n",
      "9500 smiles generated\n",
      "9600 smiles generated\n",
      "9700 smiles generated\n",
      "9800 smiles generated\n",
      "9900 smiles generated\n",
      "10000 smiles generated\n",
      "10100 smiles generated\n",
      "10200 smiles generated\n",
      "10300 smiles generated\n",
      "10400 smiles generated\n",
      "10500 smiles generated\n",
      "10600 smiles generated\n",
      "10700 smiles generated\n",
      "10800 smiles generated\n",
      "10900 smiles generated\n",
      "11000 smiles generated\n",
      "11100 smiles generated\n",
      "11200 smiles generated\n",
      "11300 smiles generated\n",
      "11400 smiles generated\n",
      "11500 smiles generated\n",
      "11600 smiles generated\n",
      "11700 smiles generated\n",
      "11800 smiles generated\n",
      "11900 smiles generated\n",
      "12000 smiles generated\n",
      "12100 smiles generated\n",
      "12200 smiles generated\n",
      "12300 smiles generated\n",
      "12400 smiles generated\n",
      "12500 smiles generated\n",
      "12600 smiles generated\n",
      "12700 smiles generated\n",
      "12800 smiles generated\n",
      "12900 smiles generated\n",
      "13000 smiles generated\n",
      "13100 smiles generated\n",
      "13200 smiles generated\n",
      "13300 smiles generated\n",
      "13400 smiles generated\n",
      "13500 smiles generated\n",
      "13600 smiles generated\n",
      "13700 smiles generated\n",
      "13800 smiles generated\n",
      "13900 smiles generated\n",
      "14000 smiles generated\n",
      "14100 smiles generated\n",
      "14200 smiles generated\n",
      "14300 smiles generated\n",
      "14400 smiles generated\n",
      "14500 smiles generated\n",
      "14600 smiles generated\n",
      "14700 smiles generated\n",
      "14800 smiles generated\n",
      "14900 smiles generated\n",
      "15000 smiles generated\n",
      "15100 smiles generated\n",
      "15200 smiles generated\n",
      "15300 smiles generated\n",
      "15400 smiles generated\n",
      "15500 smiles generated\n",
      "15600 smiles generated\n",
      "15700 smiles generated\n",
      "15800 smiles generated\n",
      "15900 smiles generated\n",
      "16000 smiles generated\n",
      "16100 smiles generated\n",
      "16200 smiles generated\n",
      "16300 smiles generated\n",
      "16400 smiles generated\n",
      "16500 smiles generated\n",
      "16600 smiles generated\n",
      "16700 smiles generated\n",
      "16800 smiles generated\n",
      "16900 smiles generated\n",
      "17000 smiles generated\n",
      "17100 smiles generated\n",
      "17200 smiles generated\n",
      "17300 smiles generated\n",
      "17400 smiles generated\n",
      "17500 smiles generated\n",
      "17600 smiles generated\n",
      "17700 smiles generated\n",
      "17800 smiles generated\n",
      "17900 smiles generated\n",
      "18000 smiles generated\n",
      "18100 smiles generated\n",
      "18200 smiles generated\n",
      "18300 smiles generated\n",
      "18400 smiles generated\n",
      "18500 smiles generated\n",
      "18600 smiles generated\n",
      "18700 smiles generated\n",
      "18800 smiles generated\n",
      "18900 smiles generated\n",
      "19000 smiles generated\n",
      "19100 smiles generated\n",
      "19200 smiles generated\n",
      "19300 smiles generated\n",
      "19400 smiles generated\n",
      "19500 smiles generated\n",
      "19600 smiles generated\n",
      "19700 smiles generated\n",
      "19800 smiles generated\n",
      "19900 smiles generated\n",
      "20000 smiles generated\n",
      "20100 smiles generated\n",
      "20200 smiles generated\n",
      "20300 smiles generated\n",
      "20400 smiles generated\n",
      "20500 smiles generated\n",
      "20600 smiles generated\n",
      "CCnnnCOOOOCOnCCCC[nH]nOCnCnCCnnnnCCOCOCnnnnnC(CCnCCnCnnnnOOnOC)nnnOOOOOOnnnnnnnnnCnnnnnnnnnnCnnnnnnCnCCOCCnnCnnnnnnnnnnnnnnnnOOnOC 2\n",
      "20700 smiles generated\n",
      "20800 smiles generated\n",
      "20900 smiles generated\n",
      "21000 smiles generated\n",
      "21100 smiles generated\n",
      "21200 smiles generated\n",
      "21300 smiles generated\n",
      "21400 smiles generated\n",
      "21500 smiles generated\n",
      "21600 smiles generated\n",
      "21700 smiles generated\n",
      "21800 smiles generated\n",
      "21900 smiles generated\n",
      "22000 smiles generated\n",
      "22100 smiles generated\n",
      "22200 smiles generated\n",
      "22300 smiles generated\n",
      "22400 smiles generated\n",
      "22500 smiles generated\n",
      "22600 smiles generated\n",
      "22700 smiles generated\n",
      "22800 smiles generated\n",
      "22900 smiles generated\n",
      "23000 smiles generated\n",
      "23100 smiles generated\n",
      "23200 smiles generated\n",
      "23300 smiles generated\n",
      "23400 smiles generated\n",
      "23500 smiles generated\n",
      "23600 smiles generated\n",
      "23700 smiles generated\n",
      "23800 smiles generated\n",
      "23900 smiles generated\n",
      "24000 smiles generated\n",
      "24100 smiles generated\n",
      "24200 smiles generated\n",
      "24300 smiles generated\n",
      "24400 smiles generated\n",
      "24500 smiles generated\n",
      "24600 smiles generated\n",
      "24700 smiles generated\n",
      "24800 smiles generated\n",
      "24900 smiles generated\n",
      "25000 smiles generated\n",
      "25100 smiles generated\n",
      "25200 smiles generated\n",
      "25300 smiles generated\n",
      "25400 smiles generated\n",
      "25500 smiles generated\n",
      "25600 smiles generated\n",
      "25700 smiles generated\n",
      "25800 smiles generated\n",
      "25900 smiles generated\n",
      "26000 smiles generated\n",
      "26100 smiles generated\n",
      "26200 smiles generated\n",
      "26300 smiles generated\n",
      "26400 smiles generated\n",
      "26500 smiles generated\n",
      "26600 smiles generated\n",
      "26700 smiles generated\n",
      "26800 smiles generated\n",
      "26900 smiles generated\n",
      "27000 smiles generated\n",
      "27100 smiles generated\n",
      "27200 smiles generated\n",
      "27300 smiles generated\n",
      "27400 smiles generated\n",
      "27500 smiles generated\n",
      "27600 smiles generated\n",
      "27700 smiles generated\n",
      "27800 smiles generated\n",
      "27900 smiles generated\n",
      "28000 smiles generated\n",
      "28100 smiles generated\n",
      "28200 smiles generated\n",
      "28300 smiles generated\n",
      "28400 smiles generated\n",
      "28500 smiles generated\n",
      "28600 smiles generated\n",
      "28700 smiles generated\n",
      "28800 smiles generated\n",
      "28900 smiles generated\n",
      "29000 smiles generated\n",
      "29100 smiles generated\n",
      "29200 smiles generated\n",
      "29300 smiles generated\n",
      "29400 smiles generated\n",
      "29500 smiles generated\n",
      "29600 smiles generated\n",
      "29700 smiles generated\n",
      "29800 smiles generated\n",
      "29900 smiles generated\n",
      "30000 smiles generated\n",
      "30100 smiles generated\n",
      "30200 smiles generated\n",
      "30300 smiles generated\n",
      "30400 smiles generated\n",
      "30500 smiles generated\n",
      "30600 smiles generated\n",
      "30700 smiles generated\n",
      "30800 smiles generated\n",
      "30900 smiles generated\n",
      "31000 smiles generated\n",
      "31100 smiles generated\n",
      "31200 smiles generated\n",
      "31300 smiles generated\n",
      "31400 smiles generated\n",
      "31500 smiles generated\n",
      "31600 smiles generated\n",
      "31700 smiles generated\n",
      "31800 smiles generated\n",
      "31900 smiles generated\n",
      "32000 smiles generated\n",
      "32100 smiles generated\n",
      "32200 smiles generated\n",
      "32300 smiles generated\n",
      "32400 smiles generated\n",
      "32500 smiles generated\n",
      "32600 smiles generated\n",
      "32700 smiles generated\n",
      "32800 smiles generated\n",
      "32900 smiles generated\n",
      "33000 smiles generated\n",
      "33100 smiles generated\n",
      "33200 smiles generated\n",
      "33300 smiles generated\n",
      "33400 smiles generated\n",
      "33500 smiles generated\n",
      "33600 smiles generated\n",
      "33700 smiles generated\n",
      "33800 smiles generated\n",
      "33900 smiles generated\n",
      "34000 smiles generated\n",
      "34100 smiles generated\n",
      "34200 smiles generated\n",
      "34300 smiles generated\n",
      "34400 smiles generated\n",
      "34500 smiles generated\n",
      "34600 smiles generated\n",
      "34700 smiles generated\n",
      "34800 smiles generated\n",
      "34900 smiles generated\n",
      "35000 smiles generated\n",
      "35100 smiles generated\n",
      "35200 smiles generated\n",
      "35300 smiles generated\n",
      "35400 smiles generated\n",
      "35500 smiles generated\n",
      "35600 smiles generated\n",
      "35700 smiles generated\n",
      "35800 smiles generated\n",
      "35900 smiles generated\n",
      "36000 smiles generated\n",
      "36100 smiles generated\n",
      "36200 smiles generated\n",
      "36300 smiles generated\n",
      "36400 smiles generated\n",
      "36500 smiles generated\n",
      "36600 smiles generated\n",
      "36700 smiles generated\n",
      "36800 smiles generated\n",
      "36900 smiles generated\n",
      "37000 smiles generated\n",
      "37100 smiles generated\n",
      "37200 smiles generated\n",
      "37300 smiles generated\n",
      "37400 smiles generated\n",
      "37500 smiles generated\n",
      "37600 smiles generated\n",
      "37700 smiles generated\n",
      "37800 smiles generated\n",
      "37900 smiles generated\n",
      "38000 smiles generated\n",
      "38100 smiles generated\n",
      "38200 smiles generated\n",
      "38300 smiles generated\n",
      "38400 smiles generated\n",
      "38500 smiles generated\n",
      "38600 smiles generated\n",
      "38700 smiles generated\n",
      "38800 smiles generated\n",
      "38900 smiles generated\n",
      "39000 smiles generated\n",
      "39100 smiles generated\n",
      "39200 smiles generated\n",
      "39300 smiles generated\n",
      "39400 smiles generated\n",
      "39500 smiles generated\n",
      "39600 smiles generated\n",
      "39700 smiles generated\n",
      "39800 smiles generated\n",
      "39900 smiles generated\n",
      "40000 smiles generated\n",
      "40100 smiles generated\n",
      "40200 smiles generated\n",
      "40300 smiles generated\n",
      "40400 smiles generated\n",
      "40500 smiles generated\n",
      "40600 smiles generated\n",
      "40700 smiles generated\n",
      "40800 smiles generated\n",
      "40900 smiles generated\n",
      "41000 smiles generated\n",
      "nnnClnOClClCCClClClCClCClClClCClClClClCOClClCClClClClCCnClnClnClClClClClClOONOnClnClOOOOCnClClnClnnOClClnClClOClClOnOnOOnnOCOnnOOnnOOOOnOOOClOnOOnnnnnnOnnOOOOClnnnClOOnCl 2\n",
      "41100 smiles generated\n",
      "41200 smiles generated\n",
      "41300 smiles generated\n",
      "41400 smiles generated\n",
      "41500 smiles generated\n",
      "41600 smiles generated\n",
      "41700 smiles generated\n",
      "41800 smiles generated\n",
      "41900 smiles generated\n",
      "42000 smiles generated\n",
      "42100 smiles generated\n",
      "42200 smiles generated\n",
      "42300 smiles generated\n",
      "42400 smiles generated\n",
      "42500 smiles generated\n",
      "42600 smiles generated\n",
      "42700 smiles generated\n",
      "42800 smiles generated\n",
      "42900 smiles generated\n",
      "43000 smiles generated\n",
      "43100 smiles generated\n",
      "43200 smiles generated\n",
      "43300 smiles generated\n",
      "43400 smiles generated\n",
      "43500 smiles generated\n",
      "43600 smiles generated\n",
      "43700 smiles generated\n",
      "43800 smiles generated\n",
      "43900 smiles generated\n",
      "44000 smiles generated\n",
      "44100 smiles generated\n",
      "44200 smiles generated\n",
      "44300 smiles generated\n",
      "44400 smiles generated\n",
      "44500 smiles generated\n",
      "44600 smiles generated\n",
      "44700 smiles generated\n",
      "44800 smiles generated\n",
      "44900 smiles generated\n",
      "45000 smiles generated\n",
      "45100 smiles generated\n",
      "45200 smiles generated\n",
      "45300 smiles generated\n",
      "45400 smiles generated\n",
      "45500 smiles generated\n",
      "45600 smiles generated\n",
      "45700 smiles generated\n",
      "45800 smiles generated\n",
      "45900 smiles generated\n",
      "46000 smiles generated\n",
      "46100 smiles generated\n",
      "46200 smiles generated\n",
      "46300 smiles generated\n",
      "46400 smiles generated\n",
      "46500 smiles generated\n",
      "46600 smiles generated\n",
      "46700 smiles generated\n",
      "46800 smiles generated\n",
      "46900 smiles generated\n",
      "47000 smiles generated\n",
      "47100 smiles generated\n",
      "47200 smiles generated\n",
      "47300 smiles generated\n",
      "47400 smiles generated\n",
      "47500 smiles generated\n",
      "47600 smiles generated\n",
      "47700 smiles generated\n",
      "47800 smiles generated\n",
      "47900 smiles generated\n",
      "48000 smiles generated\n",
      "48100 smiles generated\n",
      "48200 smiles generated\n",
      "48300 smiles generated\n",
      "48400 smiles generated\n",
      "48500 smiles generated\n",
      "48600 smiles generated\n",
      "48700 smiles generated\n",
      "48800 smiles generated\n",
      "48900 smiles generated\n",
      "49000 smiles generated\n",
      "49100 smiles generated\n",
      "49200 smiles generated\n",
      "49300 smiles generated\n",
      "49400 smiles generated\n",
      "49500 smiles generated\n",
      "49600 smiles generated\n",
      "49700 smiles generated\n",
      "49800 smiles generated\n",
      "49900 smiles generated\n"
     ]
    }
   ],
   "source": [
    "total_gen = 50000\n",
    "valid_gen = 0\n",
    "valid_smi = []\n",
    "grammar_gen = 0\n",
    "grammar_smi = []\n",
    "\n",
    "with suppress_rdkit_warnings():\n",
    "    for i in range(total_gen):\n",
    "        smi, v = gen_smiles(generator, transformer)\n",
    "        \n",
    "        if v == 1:\n",
    "            print(smi, 1)\n",
    "            valid_gen += 1\n",
    "            valid_smi.append(smi)\n",
    "        elif v == 2:\n",
    "            print(smi, 2)\n",
    "            grammar_gen += 1\n",
    "            grammar_smi.append(smi)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{i} smiles generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "349cfe42-d6c1-4aa1-8207-e27bbd2924bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 0, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_gen, valid_gen, grammar_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d276f36-3867-4417-bd15-8fb1e875d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07594887-3349-4e30-ba43-14d59d726184",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated_image)\n",
    "\n",
    "decision.numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c113798-cc4c-468b-9795-87df2524f5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
