{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07db2244-96d6-4ab8-a3dc-a443b1a7c92c",
   "metadata": {},
   "source": [
    "# Transformer Notebook\n",
    "Goal: Using SMILES of known CB1 ligands from the ChEMBL database, train an unsupervised Transformer to learn embeddings for these tokenized SMILES sequences to be used in other deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63445f-1a52-4892-9a41-7929a5261739",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ed18d28a-a44a-4ea0-9039-ca5dcb736078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3151, 'N=C(NS(=O)(=O)c1ccc(F)cc1)N1CC(c2ccccc2)C(c2ccc(Cl)cc2)=N1')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb1_smiles = open(\"./data/CB1_SMILES.txt\", 'r')\n",
    "cb1_smiles = cb1_smiles.read().splitlines()\n",
    "\n",
    "len(cb1_smiles), cb1_smiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb26c4c3-9677-44cf-a187-37ad5f4019e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=C(NS(=O)(=O)c1ccc(F)cc1)N1CC(c2ccccc2)C(c2ccc(Cl)cc2)=N1 58\n",
      "[12, 23, 22, 16, 17, 23, 34, 17, 22, 19, 18, 17, 22, 19, 18, 15, 20, 15, 15, 15, 17, 27, 18, 15, 15, 20, 18, 23, 20, 16, 16, 17, 15, 21, 15, 15, 15, 15, 15, 21, 18, 16, 17, 15, 21, 15, 15, 15, 17, 28, 18, 15, 15, 21, 18, 22, 23, 20, 13] 59\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import SmilesTokenizer\n",
    "\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "print( cb1_smiles[0], len(cb1_smiles[0]) )\n",
    "print( tokenizer.encode(cb1_smiles[0]), len(tokenizer.encode(cb1_smiles[0])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3084569-b664-4c24-8270-cf886d480444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 ----> [CLS]\n",
      "23 ----> N\n",
      "22 ----> =\n",
      "16 ----> C\n",
      "17 ----> (\n",
      "23 ----> N\n",
      "34 ----> S\n",
      "17 ----> (\n",
      "22 ----> =\n",
      "19 ----> O\n",
      "18 ----> )\n",
      "17 ----> (\n",
      "22 ----> =\n",
      "19 ----> O\n",
      "18 ----> )\n",
      "15 ----> c\n",
      "20 ----> 1\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "17 ----> (\n",
      "27 ----> F\n",
      "18 ----> )\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "20 ----> 1\n",
      "18 ----> )\n",
      "23 ----> N\n",
      "20 ----> 1\n",
      "16 ----> C\n",
      "16 ----> C\n",
      "17 ----> (\n",
      "15 ----> c\n",
      "21 ----> 2\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "21 ----> 2\n",
      "18 ----> )\n",
      "16 ----> C\n",
      "17 ----> (\n",
      "15 ----> c\n",
      "21 ----> 2\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "17 ----> (\n",
      "28 ----> Cl\n",
      "18 ----> )\n",
      "15 ----> c\n",
      "15 ----> c\n",
      "21 ----> 2\n",
      "18 ----> )\n",
      "22 ----> =\n",
      "23 ----> N\n",
      "20 ----> 1\n",
      "13 ----> [SEP]\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenizer.encode(cb1_smiles[0]):\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8914bab0-5838-4c9f-9630-e034374566d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3151, 59, 69)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "tokenized_smiles = []\n",
    "\n",
    "for smiles in cb1_smiles:\n",
    "    tokenized_smiles.append( tokenizer.encode(smiles) )\n",
    "\n",
    "len(tokenized_smiles), len(tokenized_smiles[0]), len(tokenized_smiles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974b7954-1aae-4c66-8c72-92eb4c331f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3151, 123), (123,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_smiles = pad_sequences(tokenized_smiles, padding='post')\n",
    "padded_smiles = tf.cast(padded_smiles, dtype=\"float32\")\n",
    "padded_smiles = np.stack(padded_smiles, axis=0)\n",
    "\n",
    "padded_smiles.shape, padded_smiles[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4688f1-8d5f-4ad9-9b47-aa77d60bc402",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a05015-285b-4dbc-bad3-ef2a48168286",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83e2205-3758-4a6f-b2f4-84ff84e8a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739d781-934f-4ad0-b5cc-7f81569a2907",
   "metadata": {},
   "source": [
    "#### Mask Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ce402a-61af-4f3f-8ce5-aa790253a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd89a134-3d83-4553-a7f6-0ef1398a0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a97fec-be46-4175-9921-fa335ea5b51e",
   "metadata": {},
   "source": [
    "#### Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f06aa66-1deb-4ded-a44e-bcfc2bc77b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bae62c-8b0d-4c09-af54-4036cc9b3f7d",
   "metadata": {},
   "source": [
    "#### Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ecb5fbb-c423-4040-af2c-27776a5775e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09cd465c-8bc0-4699-80c6-9dd9eda7aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3ac5a1-4eb4-4d26-a8dc-3ed22135558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb156b37-650d-4590-af9d-832289f8ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817115b8-639d-47a8-9081-cb97decc5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48197357-5abb-4290-83e4-80bb496fd905",
   "metadata": {},
   "source": [
    "#### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e84ea6b3-80f5-451e-9e6e-b31e27ce9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f85d50-3b48-4406-9581-29550f9ee56d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dede78a-1de0-4fba-83b0-c3ee5198cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 32\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 510\n",
    "target_vocab_size = 510\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d559c6fb-541e-4f01-bb3c-c6ab853c727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55c17be2-a513-46c2-b28b-b0c573f2bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61e2ff19-22ef-4f34-b990-b3bcbf262e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b578353d-7d7c-487a-9ff3-ebd3d7ab03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "029ce54d-b683-44c1-81dc-04868c429a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcc58c27-ebb2-4b38-b801-891dfc94aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84025e1d-fba3-4021-9477-e52651a0496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ea71835-4d0c-41b5-9787-db506e81b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96e95f41-3f05-4ba1-9676-11f5c4e0c176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 64 Loss 6.1865 Accuracy 0.0005\n",
      "Epoch 1 Batch 128 Loss 6.1924 Accuracy 0.0003\n",
      "Epoch 1 Batch 192 Loss 6.2112 Accuracy 0.0002\n",
      "Epoch 1 Batch 256 Loss 6.2067 Accuracy 0.0003\n",
      "Epoch 1 Batch 320 Loss 6.2115 Accuracy 0.0002\n",
      "Epoch 1 Batch 384 Loss 6.2147 Accuracy 0.0002\n",
      "Epoch 1 Batch 448 Loss 6.2116 Accuracy 0.0002\n",
      "Epoch 1 Batch 512 Loss 6.2088 Accuracy 0.0002\n",
      "Epoch 1 Batch 576 Loss 6.2089 Accuracy 0.0002\n",
      "Epoch 1 Batch 640 Loss 6.2074 Accuracy 0.0002\n",
      "Epoch 1 Batch 704 Loss 6.2074 Accuracy 0.0002\n",
      "Epoch 1 Batch 768 Loss 6.2067 Accuracy 0.0002\n",
      "Epoch 1 Batch 832 Loss 6.2068 Accuracy 0.0002\n",
      "Epoch 1 Batch 896 Loss 6.2057 Accuracy 0.0002\n",
      "Epoch 1 Batch 960 Loss 6.2032 Accuracy 0.0002\n",
      "Epoch 1 Batch 1024 Loss 6.2010 Accuracy 0.0002\n",
      "Epoch 1 Batch 1088 Loss 6.1999 Accuracy 0.0002\n",
      "Epoch 1 Batch 1152 Loss 6.2005 Accuracy 0.0002\n",
      "Epoch 1 Batch 1216 Loss 6.1989 Accuracy 0.0002\n",
      "Epoch 1 Batch 1280 Loss 6.1973 Accuracy 0.0002\n",
      "Epoch 1 Batch 1344 Loss 6.1948 Accuracy 0.0002\n",
      "Epoch 1 Batch 1408 Loss 6.1933 Accuracy 0.0002\n",
      "Epoch 1 Batch 1472 Loss 6.1919 Accuracy 0.0002\n",
      "Epoch 1 Batch 1536 Loss 6.1901 Accuracy 0.0002\n",
      "Epoch 1 Batch 1600 Loss 6.1890 Accuracy 0.0002\n",
      "Epoch 1 Batch 1664 Loss 6.1867 Accuracy 0.0002\n",
      "Epoch 1 Batch 1728 Loss 6.1837 Accuracy 0.0002\n",
      "Epoch 1 Batch 1792 Loss 6.1806 Accuracy 0.0002\n",
      "Epoch 1 Batch 1856 Loss 6.1789 Accuracy 0.0002\n",
      "Epoch 1 Batch 1920 Loss 6.1760 Accuracy 0.0002\n",
      "Epoch 1 Batch 1984 Loss 6.1730 Accuracy 0.0002\n",
      "Epoch 1 Batch 2048 Loss 6.1704 Accuracy 0.0002\n",
      "Epoch 1 Batch 2112 Loss 6.1674 Accuracy 0.0002\n",
      "Epoch 1 Batch 2176 Loss 6.1646 Accuracy 0.0002\n",
      "Epoch 1 Batch 2240 Loss 6.1613 Accuracy 0.0002\n",
      "Epoch 1 Batch 2304 Loss 6.1590 Accuracy 0.0003\n",
      "Epoch 1 Batch 2368 Loss 6.1552 Accuracy 0.0003\n",
      "Epoch 1 Batch 2432 Loss 6.1516 Accuracy 0.0003\n",
      "Epoch 1 Batch 2496 Loss 6.1485 Accuracy 0.0004\n",
      "Epoch 1 Batch 2560 Loss 6.1455 Accuracy 0.0004\n",
      "Epoch 1 Batch 2624 Loss 6.1433 Accuracy 0.0004\n",
      "Epoch 1 Batch 2688 Loss 6.1404 Accuracy 0.0004\n",
      "Epoch 1 Batch 2752 Loss 6.1372 Accuracy 0.0005\n",
      "Epoch 1 Batch 2816 Loss 6.1341 Accuracy 0.0006\n",
      "Epoch 1 Batch 2880 Loss 6.1307 Accuracy 0.0007\n",
      "Epoch 1 Batch 2944 Loss 6.1271 Accuracy 0.0007\n",
      "Epoch 1 Batch 3008 Loss 6.1229 Accuracy 0.0009\n",
      "Epoch 1 Batch 3072 Loss 6.1194 Accuracy 0.0010\n",
      "Epoch 1 Batch 3136 Loss 6.1165 Accuracy 0.0012\n",
      "Epoch 1 Loss 6.1165 Accuracy 0.0012\n",
      "Time taken for 1 epoch: 42.92309522628784 secs\n",
      "\n",
      "Epoch 2 Batch 64 Loss 5.9396 Accuracy 0.0122\n",
      "Epoch 2 Batch 128 Loss 5.9353 Accuracy 0.0125\n",
      "Epoch 2 Batch 192 Loss 5.9311 Accuracy 0.0152\n",
      "Epoch 2 Batch 256 Loss 5.9269 Accuracy 0.0173\n",
      "Epoch 2 Batch 320 Loss 5.9222 Accuracy 0.0187\n",
      "Epoch 2 Batch 384 Loss 5.9203 Accuracy 0.0208\n",
      "Epoch 2 Batch 448 Loss 5.9174 Accuracy 0.0222\n",
      "Epoch 2 Batch 512 Loss 5.9139 Accuracy 0.0240\n",
      "Epoch 2 Batch 576 Loss 5.9111 Accuracy 0.0255\n",
      "Epoch 2 Batch 640 Loss 5.9080 Accuracy 0.0270\n",
      "Epoch 2 Batch 704 Loss 5.9046 Accuracy 0.0283\n",
      "Epoch 2 Batch 768 Loss 5.9025 Accuracy 0.0302\n",
      "Epoch 2 Batch 832 Loss 5.8983 Accuracy 0.0322\n",
      "Epoch 2 Batch 896 Loss 5.8956 Accuracy 0.0344\n",
      "Epoch 2 Batch 960 Loss 5.8927 Accuracy 0.0359\n",
      "Epoch 2 Batch 1024 Loss 5.8900 Accuracy 0.0379\n",
      "Epoch 2 Batch 1088 Loss 5.8863 Accuracy 0.0404\n",
      "Epoch 2 Batch 1152 Loss 5.8834 Accuracy 0.0422\n",
      "Epoch 2 Batch 1216 Loss 5.8797 Accuracy 0.0442\n",
      "Epoch 2 Batch 1280 Loss 5.8765 Accuracy 0.0457\n",
      "Epoch 2 Batch 1344 Loss 5.8729 Accuracy 0.0477\n",
      "Epoch 2 Batch 1408 Loss 5.8701 Accuracy 0.0492\n",
      "Epoch 2 Batch 1472 Loss 5.8671 Accuracy 0.0509\n",
      "Epoch 2 Batch 1536 Loss 5.8644 Accuracy 0.0527\n",
      "Epoch 2 Batch 1600 Loss 5.8611 Accuracy 0.0540\n",
      "Epoch 2 Batch 1664 Loss 5.8581 Accuracy 0.0551\n",
      "Epoch 2 Batch 1728 Loss 5.8553 Accuracy 0.0564\n",
      "Epoch 2 Batch 1792 Loss 5.8519 Accuracy 0.0577\n",
      "Epoch 2 Batch 1856 Loss 5.8487 Accuracy 0.0592\n",
      "Epoch 2 Batch 1920 Loss 5.8454 Accuracy 0.0608\n",
      "Epoch 2 Batch 1984 Loss 5.8426 Accuracy 0.0621\n",
      "Epoch 2 Batch 2048 Loss 5.8405 Accuracy 0.0632\n",
      "Epoch 2 Batch 2112 Loss 5.8373 Accuracy 0.0642\n",
      "Epoch 2 Batch 2176 Loss 5.8344 Accuracy 0.0652\n",
      "Epoch 2 Batch 2240 Loss 5.8317 Accuracy 0.0663\n",
      "Epoch 2 Batch 2304 Loss 5.8286 Accuracy 0.0671\n",
      "Epoch 2 Batch 2368 Loss 5.8256 Accuracy 0.0681\n",
      "Epoch 2 Batch 2432 Loss 5.8227 Accuracy 0.0692\n",
      "Epoch 2 Batch 2496 Loss 5.8204 Accuracy 0.0702\n",
      "Epoch 2 Batch 2560 Loss 5.8176 Accuracy 0.0711\n",
      "Epoch 2 Batch 2624 Loss 5.8150 Accuracy 0.0718\n",
      "Epoch 2 Batch 2688 Loss 5.8124 Accuracy 0.0725\n",
      "Epoch 2 Batch 2752 Loss 5.8099 Accuracy 0.0730\n",
      "Epoch 2 Batch 2816 Loss 5.8073 Accuracy 0.0736\n",
      "Epoch 2 Batch 2880 Loss 5.8049 Accuracy 0.0741\n",
      "Epoch 2 Batch 2944 Loss 5.8024 Accuracy 0.0746\n",
      "Epoch 2 Batch 3008 Loss 5.7998 Accuracy 0.0753\n",
      "Epoch 2 Batch 3072 Loss 5.7972 Accuracy 0.0757\n",
      "Epoch 2 Batch 3136 Loss 5.7947 Accuracy 0.0760\n",
      "Epoch 2 Loss 5.7947 Accuracy 0.0760\n",
      "Time taken for 1 epoch: 35.429831981658936 secs\n",
      "\n",
      "Epoch 3 Batch 64 Loss 5.6729 Accuracy 0.1027\n",
      "Epoch 3 Batch 128 Loss 5.6667 Accuracy 0.1055\n",
      "Epoch 3 Batch 192 Loss 5.6592 Accuracy 0.1058\n",
      "Epoch 3 Batch 256 Loss 5.6545 Accuracy 0.1062\n",
      "Epoch 3 Batch 320 Loss 5.6539 Accuracy 0.1053\n",
      "Epoch 3 Batch 384 Loss 5.6508 Accuracy 0.1053\n",
      "Epoch 3 Batch 448 Loss 5.6484 Accuracy 0.1061\n",
      "Epoch 3 Batch 512 Loss 5.6440 Accuracy 0.1064\n",
      "Epoch 3 Batch 576 Loss 5.6412 Accuracy 0.1058\n",
      "Epoch 3 Batch 640 Loss 5.6372 Accuracy 0.1068\n",
      "Epoch 3 Batch 704 Loss 5.6354 Accuracy 0.1073\n",
      "Epoch 3 Batch 768 Loss 5.6334 Accuracy 0.1064\n",
      "Epoch 3 Batch 832 Loss 5.6317 Accuracy 0.1062\n",
      "Epoch 3 Batch 896 Loss 5.6299 Accuracy 0.1061\n",
      "Epoch 3 Batch 960 Loss 5.6289 Accuracy 0.1064\n",
      "Epoch 3 Batch 1024 Loss 5.6264 Accuracy 0.1064\n",
      "Epoch 3 Batch 1088 Loss 5.6244 Accuracy 0.1062\n",
      "Epoch 3 Batch 1152 Loss 5.6220 Accuracy 0.1064\n",
      "Epoch 3 Batch 1216 Loss 5.6202 Accuracy 0.1063\n",
      "Epoch 3 Batch 1280 Loss 5.6177 Accuracy 0.1066\n",
      "Epoch 3 Batch 1344 Loss 5.6157 Accuracy 0.1066\n",
      "Epoch 3 Batch 1408 Loss 5.6130 Accuracy 0.1067\n",
      "Epoch 3 Batch 1472 Loss 5.6109 Accuracy 0.1067\n",
      "Epoch 3 Batch 1536 Loss 5.6085 Accuracy 0.1064\n",
      "Epoch 3 Batch 1600 Loss 5.6061 Accuracy 0.1064\n",
      "Epoch 3 Batch 1664 Loss 5.6036 Accuracy 0.1065\n",
      "Epoch 3 Batch 1728 Loss 5.6014 Accuracy 0.1065\n",
      "Epoch 3 Batch 1792 Loss 5.5992 Accuracy 0.1067\n",
      "Epoch 3 Batch 1856 Loss 5.5967 Accuracy 0.1070\n",
      "Epoch 3 Batch 1920 Loss 5.5950 Accuracy 0.1066\n",
      "Epoch 3 Batch 1984 Loss 5.5924 Accuracy 0.1065\n",
      "Epoch 3 Batch 2048 Loss 5.5895 Accuracy 0.1068\n",
      "Epoch 3 Batch 2112 Loss 5.5875 Accuracy 0.1070\n",
      "Epoch 3 Batch 2176 Loss 5.5855 Accuracy 0.1069\n",
      "Epoch 3 Batch 2240 Loss 5.5833 Accuracy 0.1070\n",
      "Epoch 3 Batch 2304 Loss 5.5807 Accuracy 0.1070\n",
      "Epoch 3 Batch 2368 Loss 5.5782 Accuracy 0.1072\n",
      "Epoch 3 Batch 2432 Loss 5.5756 Accuracy 0.1072\n",
      "Epoch 3 Batch 2496 Loss 5.5728 Accuracy 0.1074\n",
      "Epoch 3 Batch 2560 Loss 5.5701 Accuracy 0.1078\n",
      "Epoch 3 Batch 2624 Loss 5.5679 Accuracy 0.1081\n",
      "Epoch 3 Batch 2688 Loss 5.5649 Accuracy 0.1081\n",
      "Epoch 3 Batch 2752 Loss 5.5623 Accuracy 0.1082\n",
      "Epoch 3 Batch 2816 Loss 5.5592 Accuracy 0.1083\n",
      "Epoch 3 Batch 2880 Loss 5.5563 Accuracy 0.1084\n",
      "Epoch 3 Batch 2944 Loss 5.5536 Accuracy 0.1087\n",
      "Epoch 3 Batch 3008 Loss 5.5509 Accuracy 0.1087\n",
      "Epoch 3 Batch 3072 Loss 5.5483 Accuracy 0.1089\n",
      "Epoch 3 Batch 3136 Loss 5.5457 Accuracy 0.1088\n",
      "Epoch 3 Loss 5.5457 Accuracy 0.1088\n",
      "Time taken for 1 epoch: 37.33286499977112 secs\n",
      "\n",
      "Epoch 4 Batch 64 Loss 5.4177 Accuracy 0.1124\n",
      "Epoch 4 Batch 128 Loss 5.4139 Accuracy 0.1149\n",
      "Epoch 4 Batch 192 Loss 5.4046 Accuracy 0.1149\n",
      "Epoch 4 Batch 256 Loss 5.4046 Accuracy 0.1145\n",
      "Epoch 4 Batch 320 Loss 5.3976 Accuracy 0.1169\n",
      "Epoch 4 Batch 384 Loss 5.3940 Accuracy 0.1160\n",
      "Epoch 4 Batch 448 Loss 5.3928 Accuracy 0.1159\n",
      "Epoch 4 Batch 512 Loss 5.3882 Accuracy 0.1176\n",
      "Epoch 4 Batch 576 Loss 5.3853 Accuracy 0.1174\n",
      "Epoch 4 Batch 640 Loss 5.3821 Accuracy 0.1176\n",
      "Epoch 4 Batch 704 Loss 5.3768 Accuracy 0.1182\n",
      "Epoch 4 Batch 768 Loss 5.3729 Accuracy 0.1183\n",
      "Epoch 4 Batch 832 Loss 5.3680 Accuracy 0.1190\n",
      "Epoch 4 Batch 896 Loss 5.3653 Accuracy 0.1190\n",
      "Epoch 4 Batch 960 Loss 5.3619 Accuracy 0.1193\n",
      "Epoch 4 Batch 1024 Loss 5.3582 Accuracy 0.1195\n",
      "Epoch 4 Batch 1088 Loss 5.3554 Accuracy 0.1200\n",
      "Epoch 4 Batch 1152 Loss 5.3511 Accuracy 0.1202\n",
      "Epoch 4 Batch 1216 Loss 5.3479 Accuracy 0.1201\n",
      "Epoch 4 Batch 1280 Loss 5.3445 Accuracy 0.1204\n",
      "Epoch 4 Batch 1344 Loss 5.3408 Accuracy 0.1206\n",
      "Epoch 4 Batch 1408 Loss 5.3379 Accuracy 0.1205\n",
      "Epoch 4 Batch 1472 Loss 5.3339 Accuracy 0.1207\n",
      "Epoch 4 Batch 1536 Loss 5.3309 Accuracy 0.1207\n",
      "Epoch 4 Batch 1600 Loss 5.3270 Accuracy 0.1210\n",
      "Epoch 4 Batch 1664 Loss 5.3233 Accuracy 0.1212\n",
      "Epoch 4 Batch 1728 Loss 5.3195 Accuracy 0.1213\n",
      "Epoch 4 Batch 1792 Loss 5.3156 Accuracy 0.1217\n",
      "Epoch 4 Batch 1856 Loss 5.3119 Accuracy 0.1218\n",
      "Epoch 4 Batch 1920 Loss 5.3087 Accuracy 0.1217\n",
      "Epoch 4 Batch 1984 Loss 5.3057 Accuracy 0.1214\n",
      "Epoch 4 Batch 2048 Loss 5.3025 Accuracy 0.1216\n",
      "Epoch 4 Batch 2112 Loss 5.2986 Accuracy 0.1218\n",
      "Epoch 4 Batch 2176 Loss 5.2956 Accuracy 0.1219\n",
      "Epoch 4 Batch 2240 Loss 5.2913 Accuracy 0.1221\n",
      "Epoch 4 Batch 2304 Loss 5.2880 Accuracy 0.1221\n",
      "Epoch 4 Batch 2368 Loss 5.2837 Accuracy 0.1223\n",
      "Epoch 4 Batch 2432 Loss 5.2801 Accuracy 0.1224\n",
      "Epoch 4 Batch 2496 Loss 5.2767 Accuracy 0.1222\n",
      "Epoch 4 Batch 2560 Loss 5.2731 Accuracy 0.1222\n",
      "Epoch 4 Batch 2624 Loss 5.2694 Accuracy 0.1224\n",
      "Epoch 4 Batch 2688 Loss 5.2654 Accuracy 0.1226\n",
      "Epoch 4 Batch 2752 Loss 5.2615 Accuracy 0.1228\n",
      "Epoch 4 Batch 2816 Loss 5.2575 Accuracy 0.1231\n",
      "Epoch 4 Batch 2880 Loss 5.2538 Accuracy 0.1231\n",
      "Epoch 4 Batch 2944 Loss 5.2500 Accuracy 0.1230\n",
      "Epoch 4 Batch 3008 Loss 5.2461 Accuracy 0.1230\n",
      "Epoch 4 Batch 3072 Loss 5.2421 Accuracy 0.1231\n",
      "Epoch 4 Batch 3136 Loss 5.2382 Accuracy 0.1232\n",
      "Epoch 4 Loss 5.2382 Accuracy 0.1232\n",
      "Time taken for 1 epoch: 36.582512855529785 secs\n",
      "\n",
      "Epoch 5 Batch 64 Loss 5.0452 Accuracy 0.1303\n",
      "Epoch 5 Batch 128 Loss 5.0367 Accuracy 0.1308\n",
      "Epoch 5 Batch 192 Loss 5.0346 Accuracy 0.1303\n",
      "Epoch 5 Batch 256 Loss 5.0322 Accuracy 0.1309\n",
      "Epoch 5 Batch 320 Loss 5.0283 Accuracy 0.1294\n",
      "Epoch 5 Batch 384 Loss 5.0217 Accuracy 0.1291\n",
      "Epoch 5 Batch 448 Loss 5.0177 Accuracy 0.1274\n",
      "Epoch 5 Batch 512 Loss 5.0118 Accuracy 0.1290\n",
      "Epoch 5 Batch 576 Loss 5.0071 Accuracy 0.1287\n",
      "Epoch 5 Batch 640 Loss 5.0024 Accuracy 0.1290\n",
      "Epoch 5 Batch 704 Loss 4.9974 Accuracy 0.1289\n",
      "Epoch 5 Batch 768 Loss 4.9926 Accuracy 0.1290\n",
      "Epoch 5 Batch 832 Loss 4.9872 Accuracy 0.1291\n",
      "Epoch 5 Batch 896 Loss 4.9827 Accuracy 0.1291\n",
      "Epoch 5 Batch 960 Loss 4.9777 Accuracy 0.1294\n",
      "Epoch 5 Batch 1024 Loss 4.9741 Accuracy 0.1294\n",
      "Epoch 5 Batch 1088 Loss 4.9698 Accuracy 0.1293\n",
      "Epoch 5 Batch 1152 Loss 4.9660 Accuracy 0.1292\n",
      "Epoch 5 Batch 1216 Loss 4.9623 Accuracy 0.1291\n",
      "Epoch 5 Batch 1280 Loss 4.9573 Accuracy 0.1291\n",
      "Epoch 5 Batch 1344 Loss 4.9531 Accuracy 0.1293\n",
      "Epoch 5 Batch 1408 Loss 4.9482 Accuracy 0.1294\n",
      "Epoch 5 Batch 1472 Loss 4.9430 Accuracy 0.1297\n",
      "Epoch 5 Batch 1536 Loss 4.9379 Accuracy 0.1298\n",
      "Epoch 5 Batch 1600 Loss 4.9328 Accuracy 0.1300\n",
      "Epoch 5 Batch 1664 Loss 4.9283 Accuracy 0.1302\n",
      "Epoch 5 Batch 1728 Loss 4.9240 Accuracy 0.1299\n",
      "Epoch 5 Batch 1792 Loss 4.9195 Accuracy 0.1301\n",
      "Epoch 5 Batch 1856 Loss 4.9147 Accuracy 0.1305\n",
      "Epoch 5 Batch 1920 Loss 4.9098 Accuracy 0.1305\n",
      "Epoch 5 Batch 1984 Loss 4.9054 Accuracy 0.1304\n",
      "Epoch 5 Batch 2048 Loss 4.9006 Accuracy 0.1305\n",
      "Epoch 5 Batch 2112 Loss 4.8958 Accuracy 0.1306\n",
      "Epoch 5 Batch 2176 Loss 4.8910 Accuracy 0.1307\n",
      "Epoch 5 Batch 2240 Loss 4.8864 Accuracy 0.1310\n",
      "Epoch 5 Batch 2304 Loss 4.8811 Accuracy 0.1312\n",
      "Epoch 5 Batch 2368 Loss 4.8760 Accuracy 0.1313\n",
      "Epoch 5 Batch 2432 Loss 4.8708 Accuracy 0.1312\n",
      "Epoch 5 Batch 2496 Loss 4.8658 Accuracy 0.1312\n",
      "Epoch 5 Batch 2560 Loss 4.8609 Accuracy 0.1313\n",
      "Epoch 5 Batch 2624 Loss 4.8561 Accuracy 0.1314\n",
      "Epoch 5 Batch 2688 Loss 4.8516 Accuracy 0.1315\n",
      "Epoch 5 Batch 2752 Loss 4.8464 Accuracy 0.1314\n",
      "Epoch 5 Batch 2816 Loss 4.8412 Accuracy 0.1316\n",
      "Epoch 5 Batch 2880 Loss 4.8358 Accuracy 0.1317\n",
      "Epoch 5 Batch 2944 Loss 4.8310 Accuracy 0.1319\n",
      "Epoch 5 Batch 3008 Loss 4.8258 Accuracy 0.1319\n",
      "Epoch 5 Batch 3072 Loss 4.8203 Accuracy 0.1322\n",
      "Epoch 5 Batch 3136 Loss 4.8154 Accuracy 0.1323\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
      "Epoch 5 Loss 4.8154 Accuracy 0.1323\n",
      "Time taken for 1 epoch: 36.232476234436035 secs\n",
      "\n",
      "Epoch 6 Batch 64 Loss 4.5458 Accuracy 0.1420\n",
      "Epoch 6 Batch 128 Loss 4.5479 Accuracy 0.1397\n",
      "Epoch 6 Batch 192 Loss 4.5467 Accuracy 0.1389\n",
      "Epoch 6 Batch 256 Loss 4.5434 Accuracy 0.1403\n",
      "Epoch 6 Batch 320 Loss 4.5388 Accuracy 0.1401\n",
      "Epoch 6 Batch 384 Loss 4.5326 Accuracy 0.1401\n",
      "Epoch 6 Batch 448 Loss 4.5291 Accuracy 0.1396\n",
      "Epoch 6 Batch 512 Loss 4.5227 Accuracy 0.1397\n",
      "Epoch 6 Batch 576 Loss 4.5170 Accuracy 0.1396\n",
      "Epoch 6 Batch 640 Loss 4.5119 Accuracy 0.1398\n",
      "Epoch 6 Batch 704 Loss 4.5051 Accuracy 0.1404\n",
      "Epoch 6 Batch 768 Loss 4.5004 Accuracy 0.1401\n",
      "Epoch 6 Batch 832 Loss 4.4953 Accuracy 0.1397\n",
      "Epoch 6 Batch 896 Loss 4.4899 Accuracy 0.1402\n",
      "Epoch 6 Batch 960 Loss 4.4830 Accuracy 0.1402\n",
      "Epoch 6 Batch 1024 Loss 4.4781 Accuracy 0.1403\n",
      "Epoch 6 Batch 1088 Loss 4.4728 Accuracy 0.1403\n",
      "Epoch 6 Batch 1152 Loss 4.4665 Accuracy 0.1406\n",
      "Epoch 6 Batch 1216 Loss 4.4598 Accuracy 0.1413\n",
      "Epoch 6 Batch 1280 Loss 4.4545 Accuracy 0.1415\n",
      "Epoch 6 Batch 1344 Loss 4.4482 Accuracy 0.1417\n",
      "Epoch 6 Batch 1408 Loss 4.4417 Accuracy 0.1417\n",
      "Epoch 6 Batch 1472 Loss 4.4356 Accuracy 0.1419\n",
      "Epoch 6 Batch 1536 Loss 4.4306 Accuracy 0.1420\n",
      "Epoch 6 Batch 1600 Loss 4.4252 Accuracy 0.1422\n",
      "Epoch 6 Batch 1664 Loss 4.4197 Accuracy 0.1422\n",
      "Epoch 6 Batch 1728 Loss 4.4137 Accuracy 0.1423\n",
      "Epoch 6 Batch 1792 Loss 4.4082 Accuracy 0.1423\n",
      "Epoch 6 Batch 1856 Loss 4.4021 Accuracy 0.1426\n",
      "Epoch 6 Batch 1920 Loss 4.3966 Accuracy 0.1428\n",
      "Epoch 6 Batch 1984 Loss 4.3919 Accuracy 0.1430\n",
      "Epoch 6 Batch 2048 Loss 4.3856 Accuracy 0.1434\n",
      "Epoch 6 Batch 2112 Loss 4.3802 Accuracy 0.1437\n",
      "Epoch 6 Batch 2176 Loss 4.3738 Accuracy 0.1439\n",
      "Epoch 6 Batch 2240 Loss 4.3683 Accuracy 0.1441\n",
      "Epoch 6 Batch 2304 Loss 4.3625 Accuracy 0.1443\n",
      "Epoch 6 Batch 2368 Loss 4.3569 Accuracy 0.1446\n",
      "Epoch 6 Batch 2432 Loss 4.3509 Accuracy 0.1447\n",
      "Epoch 6 Batch 2496 Loss 4.3453 Accuracy 0.1447\n",
      "Epoch 6 Batch 2560 Loss 4.3394 Accuracy 0.1448\n",
      "Epoch 6 Batch 2624 Loss 4.3336 Accuracy 0.1450\n",
      "Epoch 6 Batch 2688 Loss 4.3282 Accuracy 0.1453\n",
      "Epoch 6 Batch 2752 Loss 4.3225 Accuracy 0.1454\n",
      "Epoch 6 Batch 2816 Loss 4.3163 Accuracy 0.1455\n",
      "Epoch 6 Batch 2880 Loss 4.3106 Accuracy 0.1458\n",
      "Epoch 6 Batch 2944 Loss 4.3047 Accuracy 0.1459\n",
      "Epoch 6 Batch 3008 Loss 4.2990 Accuracy 0.1461\n",
      "Epoch 6 Batch 3072 Loss 4.2931 Accuracy 0.1463\n",
      "Epoch 6 Batch 3136 Loss 4.2875 Accuracy 0.1465\n",
      "Epoch 6 Loss 4.2875 Accuracy 0.1465\n",
      "Time taken for 1 epoch: 36.15507411956787 secs\n",
      "\n",
      "Epoch 7 Batch 64 Loss 4.0253 Accuracy 0.1447\n",
      "Epoch 7 Batch 128 Loss 3.9999 Accuracy 0.1479\n",
      "Epoch 7 Batch 192 Loss 3.9875 Accuracy 0.1495\n",
      "Epoch 7 Batch 256 Loss 3.9816 Accuracy 0.1506\n",
      "Epoch 7 Batch 320 Loss 3.9760 Accuracy 0.1516\n",
      "Epoch 7 Batch 384 Loss 3.9678 Accuracy 0.1512\n",
      "Epoch 7 Batch 448 Loss 3.9646 Accuracy 0.1508\n",
      "Epoch 7 Batch 512 Loss 3.9578 Accuracy 0.1508\n",
      "Epoch 7 Batch 576 Loss 3.9506 Accuracy 0.1517\n",
      "Epoch 7 Batch 640 Loss 3.9446 Accuracy 0.1522\n",
      "Epoch 7 Batch 704 Loss 3.9396 Accuracy 0.1525\n",
      "Epoch 7 Batch 768 Loss 3.9338 Accuracy 0.1525\n",
      "Epoch 7 Batch 832 Loss 3.9272 Accuracy 0.1536\n",
      "Epoch 7 Batch 896 Loss 3.9204 Accuracy 0.1535\n",
      "Epoch 7 Batch 960 Loss 3.9139 Accuracy 0.1532\n",
      "Epoch 7 Batch 1024 Loss 3.9073 Accuracy 0.1538\n",
      "Epoch 7 Batch 1088 Loss 3.9018 Accuracy 0.1538\n",
      "Epoch 7 Batch 1152 Loss 3.8958 Accuracy 0.1539\n",
      "Epoch 7 Batch 1216 Loss 3.8893 Accuracy 0.1541\n",
      "Epoch 7 Batch 1280 Loss 3.8842 Accuracy 0.1537\n",
      "Epoch 7 Batch 1344 Loss 3.8780 Accuracy 0.1540\n",
      "Epoch 7 Batch 1408 Loss 3.8717 Accuracy 0.1542\n",
      "Epoch 7 Batch 1472 Loss 3.8660 Accuracy 0.1543\n",
      "Epoch 7 Batch 1536 Loss 3.8590 Accuracy 0.1547\n",
      "Epoch 7 Batch 1600 Loss 3.8534 Accuracy 0.1547\n",
      "Epoch 7 Batch 1664 Loss 3.8480 Accuracy 0.1546\n",
      "Epoch 7 Batch 1728 Loss 3.8419 Accuracy 0.1546\n",
      "Epoch 7 Batch 1792 Loss 3.8365 Accuracy 0.1547\n",
      "Epoch 7 Batch 1856 Loss 3.8314 Accuracy 0.1550\n",
      "Epoch 7 Batch 1920 Loss 3.8262 Accuracy 0.1548\n",
      "Epoch 7 Batch 1984 Loss 3.8195 Accuracy 0.1551\n",
      "Epoch 7 Batch 2048 Loss 3.8132 Accuracy 0.1551\n",
      "Epoch 7 Batch 2112 Loss 3.8071 Accuracy 0.1552\n",
      "Epoch 7 Batch 2176 Loss 3.8016 Accuracy 0.1552\n",
      "Epoch 7 Batch 2240 Loss 3.7951 Accuracy 0.1552\n",
      "Epoch 7 Batch 2304 Loss 3.7893 Accuracy 0.1553\n",
      "Epoch 7 Batch 2368 Loss 3.7834 Accuracy 0.1555\n",
      "Epoch 7 Batch 2432 Loss 3.7776 Accuracy 0.1555\n",
      "Epoch 7 Batch 2496 Loss 3.7722 Accuracy 0.1555\n",
      "Epoch 7 Batch 2560 Loss 3.7660 Accuracy 0.1556\n",
      "Epoch 7 Batch 2624 Loss 3.7605 Accuracy 0.1557\n",
      "Epoch 7 Batch 2688 Loss 3.7547 Accuracy 0.1558\n",
      "Epoch 7 Batch 2752 Loss 3.7493 Accuracy 0.1559\n",
      "Epoch 7 Batch 2816 Loss 3.7438 Accuracy 0.1558\n",
      "Epoch 7 Batch 2880 Loss 3.7386 Accuracy 0.1560\n",
      "Epoch 7 Batch 2944 Loss 3.7333 Accuracy 0.1558\n",
      "Epoch 7 Batch 3008 Loss 3.7279 Accuracy 0.1559\n",
      "Epoch 7 Batch 3072 Loss 3.7220 Accuracy 0.1559\n",
      "Epoch 7 Batch 3136 Loss 3.7161 Accuracy 0.1560\n",
      "Epoch 7 Loss 3.7161 Accuracy 0.1560\n",
      "Time taken for 1 epoch: 37.04681921005249 secs\n",
      "\n",
      "Epoch 8 Batch 64 Loss 3.4114 Accuracy 0.1580\n",
      "Epoch 8 Batch 128 Loss 3.4343 Accuracy 0.1568\n",
      "Epoch 8 Batch 192 Loss 3.4250 Accuracy 0.1565\n",
      "Epoch 8 Batch 256 Loss 3.4178 Accuracy 0.1564\n",
      "Epoch 8 Batch 320 Loss 3.4115 Accuracy 0.1571\n",
      "Epoch 8 Batch 384 Loss 3.4064 Accuracy 0.1582\n",
      "Epoch 8 Batch 448 Loss 3.4003 Accuracy 0.1587\n",
      "Epoch 8 Batch 512 Loss 3.3932 Accuracy 0.1581\n",
      "Epoch 8 Batch 576 Loss 3.3884 Accuracy 0.1572\n",
      "Epoch 8 Batch 640 Loss 3.3838 Accuracy 0.1573\n",
      "Epoch 8 Batch 704 Loss 3.3776 Accuracy 0.1579\n",
      "Epoch 8 Batch 768 Loss 3.3705 Accuracy 0.1586\n",
      "Epoch 8 Batch 832 Loss 3.3664 Accuracy 0.1583\n",
      "Epoch 8 Batch 896 Loss 3.3613 Accuracy 0.1582\n",
      "Epoch 8 Batch 960 Loss 3.3557 Accuracy 0.1584\n",
      "Epoch 8 Batch 1024 Loss 3.3483 Accuracy 0.1589\n",
      "Epoch 8 Batch 1088 Loss 3.3420 Accuracy 0.1594\n",
      "Epoch 8 Batch 1152 Loss 3.3375 Accuracy 0.1592\n",
      "Epoch 8 Batch 1216 Loss 3.3311 Accuracy 0.1595\n",
      "Epoch 8 Batch 1280 Loss 3.3258 Accuracy 0.1595\n",
      "Epoch 8 Batch 1344 Loss 3.3203 Accuracy 0.1596\n",
      "Epoch 8 Batch 1408 Loss 3.3153 Accuracy 0.1595\n",
      "Epoch 8 Batch 1472 Loss 3.3102 Accuracy 0.1595\n",
      "Epoch 8 Batch 1536 Loss 3.3057 Accuracy 0.1595\n",
      "Epoch 8 Batch 1600 Loss 3.2994 Accuracy 0.1595\n",
      "Epoch 8 Batch 1664 Loss 3.2939 Accuracy 0.1594\n",
      "Epoch 8 Batch 1728 Loss 3.2896 Accuracy 0.1592\n",
      "Epoch 8 Batch 1792 Loss 3.2844 Accuracy 0.1595\n",
      "Epoch 8 Batch 1856 Loss 3.2796 Accuracy 0.1596\n",
      "Epoch 8 Batch 1920 Loss 3.2747 Accuracy 0.1595\n",
      "Epoch 8 Batch 1984 Loss 3.2692 Accuracy 0.1595\n",
      "Epoch 8 Batch 2048 Loss 3.2644 Accuracy 0.1596\n",
      "Epoch 8 Batch 2112 Loss 3.2584 Accuracy 0.1597\n",
      "Epoch 8 Batch 2176 Loss 3.2537 Accuracy 0.1596\n",
      "Epoch 8 Batch 2240 Loss 3.2481 Accuracy 0.1596\n",
      "Epoch 8 Batch 2304 Loss 3.2431 Accuracy 0.1597\n",
      "Epoch 8 Batch 2368 Loss 3.2382 Accuracy 0.1597\n",
      "Epoch 8 Batch 2432 Loss 3.2328 Accuracy 0.1597\n",
      "Epoch 8 Batch 2496 Loss 3.2273 Accuracy 0.1598\n",
      "Epoch 8 Batch 2560 Loss 3.2227 Accuracy 0.1598\n",
      "Epoch 8 Batch 2624 Loss 3.2176 Accuracy 0.1598\n",
      "Epoch 8 Batch 2688 Loss 3.2123 Accuracy 0.1599\n",
      "Epoch 8 Batch 2752 Loss 3.2073 Accuracy 0.1600\n",
      "Epoch 8 Batch 2816 Loss 3.2018 Accuracy 0.1602\n",
      "Epoch 8 Batch 2880 Loss 3.1967 Accuracy 0.1603\n",
      "Epoch 8 Batch 2944 Loss 3.1919 Accuracy 0.1605\n",
      "Epoch 8 Batch 3008 Loss 3.1866 Accuracy 0.1606\n",
      "Epoch 8 Batch 3072 Loss 3.1818 Accuracy 0.1606\n",
      "Epoch 8 Batch 3136 Loss 3.1772 Accuracy 0.1606\n",
      "Epoch 8 Loss 3.1772 Accuracy 0.1606\n",
      "Time taken for 1 epoch: 36.62730407714844 secs\n",
      "\n",
      "Epoch 9 Batch 64 Loss 2.9528 Accuracy 0.1611\n",
      "Epoch 9 Batch 128 Loss 2.9332 Accuracy 0.1626\n",
      "Epoch 9 Batch 192 Loss 2.9255 Accuracy 0.1636\n",
      "Epoch 9 Batch 256 Loss 2.9175 Accuracy 0.1632\n",
      "Epoch 9 Batch 320 Loss 2.9110 Accuracy 0.1633\n",
      "Epoch 9 Batch 384 Loss 2.9056 Accuracy 0.1629\n",
      "Epoch 9 Batch 448 Loss 2.9022 Accuracy 0.1630\n",
      "Epoch 9 Batch 512 Loss 2.8990 Accuracy 0.1633\n",
      "Epoch 9 Batch 576 Loss 2.8956 Accuracy 0.1632\n",
      "Epoch 9 Batch 640 Loss 2.8931 Accuracy 0.1631\n",
      "Epoch 9 Batch 704 Loss 2.8883 Accuracy 0.1637\n",
      "Epoch 9 Batch 768 Loss 2.8827 Accuracy 0.1636\n",
      "Epoch 9 Batch 832 Loss 2.8804 Accuracy 0.1637\n",
      "Epoch 9 Batch 896 Loss 2.8758 Accuracy 0.1638\n",
      "Epoch 9 Batch 960 Loss 2.8732 Accuracy 0.1640\n",
      "Epoch 9 Batch 1024 Loss 2.8701 Accuracy 0.1639\n",
      "Epoch 9 Batch 1088 Loss 2.8655 Accuracy 0.1640\n",
      "Epoch 9 Batch 1152 Loss 2.8613 Accuracy 0.1643\n",
      "Epoch 9 Batch 1216 Loss 2.8575 Accuracy 0.1643\n",
      "Epoch 9 Batch 1280 Loss 2.8527 Accuracy 0.1641\n",
      "Epoch 9 Batch 1344 Loss 2.8477 Accuracy 0.1645\n",
      "Epoch 9 Batch 1408 Loss 2.8429 Accuracy 0.1647\n",
      "Epoch 9 Batch 1472 Loss 2.8385 Accuracy 0.1647\n",
      "Epoch 9 Batch 1536 Loss 2.8338 Accuracy 0.1644\n",
      "Epoch 9 Batch 1600 Loss 2.8295 Accuracy 0.1644\n",
      "Epoch 9 Batch 1664 Loss 2.8262 Accuracy 0.1643\n",
      "Epoch 9 Batch 1728 Loss 2.8212 Accuracy 0.1645\n",
      "Epoch 9 Batch 1792 Loss 2.8162 Accuracy 0.1647\n",
      "Epoch 9 Batch 1856 Loss 2.8115 Accuracy 0.1646\n",
      "Epoch 9 Batch 1920 Loss 2.8070 Accuracy 0.1647\n",
      "Epoch 9 Batch 1984 Loss 2.8031 Accuracy 0.1649\n",
      "Epoch 9 Batch 2048 Loss 2.7997 Accuracy 0.1650\n",
      "Epoch 9 Batch 2112 Loss 2.7953 Accuracy 0.1652\n",
      "Epoch 9 Batch 2176 Loss 2.7906 Accuracy 0.1652\n",
      "Epoch 9 Batch 2240 Loss 2.7866 Accuracy 0.1653\n",
      "Epoch 9 Batch 2304 Loss 2.7834 Accuracy 0.1652\n",
      "Epoch 9 Batch 2368 Loss 2.7789 Accuracy 0.1652\n",
      "Epoch 9 Batch 2432 Loss 2.7755 Accuracy 0.1654\n",
      "Epoch 9 Batch 2496 Loss 2.7713 Accuracy 0.1655\n",
      "Epoch 9 Batch 2560 Loss 2.7676 Accuracy 0.1656\n",
      "Epoch 9 Batch 2624 Loss 2.7637 Accuracy 0.1656\n",
      "Epoch 9 Batch 2688 Loss 2.7592 Accuracy 0.1660\n",
      "Epoch 9 Batch 2752 Loss 2.7559 Accuracy 0.1659\n",
      "Epoch 9 Batch 2816 Loss 2.7529 Accuracy 0.1661\n",
      "Epoch 9 Batch 2880 Loss 2.7492 Accuracy 0.1660\n",
      "Epoch 9 Batch 2944 Loss 2.7455 Accuracy 0.1661\n",
      "Epoch 9 Batch 3008 Loss 2.7409 Accuracy 0.1664\n",
      "Epoch 9 Batch 3072 Loss 2.7373 Accuracy 0.1664\n",
      "Epoch 9 Batch 3136 Loss 2.7336 Accuracy 0.1664\n",
      "Epoch 9 Loss 2.7336 Accuracy 0.1664\n",
      "Time taken for 1 epoch: 37.15247321128845 secs\n",
      "\n",
      "Epoch 10 Batch 64 Loss 2.5292 Accuracy 0.1668\n",
      "Epoch 10 Batch 128 Loss 2.5434 Accuracy 0.1671\n",
      "Epoch 10 Batch 192 Loss 2.5390 Accuracy 0.1709\n",
      "Epoch 10 Batch 256 Loss 2.5390 Accuracy 0.1713\n",
      "Epoch 10 Batch 320 Loss 2.5328 Accuracy 0.1722\n",
      "Epoch 10 Batch 384 Loss 2.5326 Accuracy 0.1718\n",
      "Epoch 10 Batch 448 Loss 2.5281 Accuracy 0.1716\n",
      "Epoch 10 Batch 512 Loss 2.5240 Accuracy 0.1723\n",
      "Epoch 10 Batch 576 Loss 2.5196 Accuracy 0.1716\n",
      "Epoch 10 Batch 640 Loss 2.5157 Accuracy 0.1725\n",
      "Epoch 10 Batch 704 Loss 2.5142 Accuracy 0.1729\n",
      "Epoch 10 Batch 768 Loss 2.5089 Accuracy 0.1731\n",
      "Epoch 10 Batch 832 Loss 2.5081 Accuracy 0.1733\n",
      "Epoch 10 Batch 896 Loss 2.5077 Accuracy 0.1739\n",
      "Epoch 10 Batch 960 Loss 2.5051 Accuracy 0.1743\n",
      "Epoch 10 Batch 1024 Loss 2.5004 Accuracy 0.1748\n",
      "Epoch 10 Batch 1088 Loss 2.4981 Accuracy 0.1747\n",
      "Epoch 10 Batch 1152 Loss 2.4956 Accuracy 0.1748\n",
      "Epoch 10 Batch 1216 Loss 2.4921 Accuracy 0.1747\n",
      "Epoch 10 Batch 1280 Loss 2.4911 Accuracy 0.1749\n",
      "Epoch 10 Batch 1344 Loss 2.4854 Accuracy 0.1756\n",
      "Epoch 10 Batch 1408 Loss 2.4825 Accuracy 0.1763\n",
      "Epoch 10 Batch 1472 Loss 2.4790 Accuracy 0.1770\n",
      "Epoch 10 Batch 1536 Loss 2.4775 Accuracy 0.1773\n",
      "Epoch 10 Batch 1600 Loss 2.4739 Accuracy 0.1773\n",
      "Epoch 10 Batch 1664 Loss 2.4704 Accuracy 0.1777\n",
      "Epoch 10 Batch 1728 Loss 2.4676 Accuracy 0.1779\n",
      "Epoch 10 Batch 1792 Loss 2.4653 Accuracy 0.1780\n",
      "Epoch 10 Batch 1856 Loss 2.4642 Accuracy 0.1781\n",
      "Epoch 10 Batch 1920 Loss 2.4616 Accuracy 0.1781\n",
      "Epoch 10 Batch 1984 Loss 2.4585 Accuracy 0.1784\n",
      "Epoch 10 Batch 2048 Loss 2.4552 Accuracy 0.1788\n",
      "Epoch 10 Batch 2112 Loss 2.4511 Accuracy 0.1791\n",
      "Epoch 10 Batch 2176 Loss 2.4490 Accuracy 0.1793\n",
      "Epoch 10 Batch 2240 Loss 2.4462 Accuracy 0.1798\n",
      "Epoch 10 Batch 2304 Loss 2.4450 Accuracy 0.1798\n",
      "Epoch 10 Batch 2368 Loss 2.4427 Accuracy 0.1798\n",
      "Epoch 10 Batch 2432 Loss 2.4393 Accuracy 0.1802\n",
      "Epoch 10 Batch 2496 Loss 2.4363 Accuracy 0.1806\n",
      "Epoch 10 Batch 2560 Loss 2.4329 Accuracy 0.1809\n",
      "Epoch 10 Batch 2624 Loss 2.4302 Accuracy 0.1810\n",
      "Epoch 10 Batch 2688 Loss 2.4268 Accuracy 0.1812\n",
      "Epoch 10 Batch 2752 Loss 2.4239 Accuracy 0.1815\n",
      "Epoch 10 Batch 2816 Loss 2.4212 Accuracy 0.1816\n",
      "Epoch 10 Batch 2880 Loss 2.4192 Accuracy 0.1817\n",
      "Epoch 10 Batch 2944 Loss 2.4164 Accuracy 0.1819\n",
      "Epoch 10 Batch 3008 Loss 2.4143 Accuracy 0.1821\n",
      "Epoch 10 Batch 3072 Loss 2.4112 Accuracy 0.1822\n",
      "Epoch 10 Batch 3136 Loss 2.4087 Accuracy 0.1824\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
      "Epoch 10 Loss 2.4087 Accuracy 0.1824\n",
      "Time taken for 1 epoch: 38.26667284965515 secs\n",
      "\n",
      "Epoch 11 Batch 64 Loss 2.2857 Accuracy 0.1924\n",
      "Epoch 11 Batch 128 Loss 2.2767 Accuracy 0.1933\n",
      "Epoch 11 Batch 192 Loss 2.2659 Accuracy 0.1944\n",
      "Epoch 11 Batch 256 Loss 2.2662 Accuracy 0.1930\n",
      "Epoch 11 Batch 320 Loss 2.2617 Accuracy 0.1919\n",
      "Epoch 11 Batch 384 Loss 2.2639 Accuracy 0.1912\n",
      "Epoch 11 Batch 448 Loss 2.2608 Accuracy 0.1902\n",
      "Epoch 11 Batch 512 Loss 2.2579 Accuracy 0.1909\n",
      "Epoch 11 Batch 576 Loss 2.2541 Accuracy 0.1916\n",
      "Epoch 11 Batch 640 Loss 2.2500 Accuracy 0.1910\n",
      "Epoch 11 Batch 704 Loss 2.2500 Accuracy 0.1904\n",
      "Epoch 11 Batch 768 Loss 2.2485 Accuracy 0.1903\n",
      "Epoch 11 Batch 832 Loss 2.2476 Accuracy 0.1910\n",
      "Epoch 11 Batch 896 Loss 2.2454 Accuracy 0.1919\n",
      "Epoch 11 Batch 960 Loss 2.2442 Accuracy 0.1926\n",
      "Epoch 11 Batch 1024 Loss 2.2413 Accuracy 0.1930\n",
      "Epoch 11 Batch 1088 Loss 2.2404 Accuracy 0.1925\n",
      "Epoch 11 Batch 1152 Loss 2.2377 Accuracy 0.1929\n",
      "Epoch 11 Batch 1216 Loss 2.2358 Accuracy 0.1932\n",
      "Epoch 11 Batch 1280 Loss 2.2335 Accuracy 0.1939\n",
      "Epoch 11 Batch 1344 Loss 2.2309 Accuracy 0.1939\n",
      "Epoch 11 Batch 1408 Loss 2.2288 Accuracy 0.1943\n",
      "Epoch 11 Batch 1472 Loss 2.2281 Accuracy 0.1940\n",
      "Epoch 11 Batch 1536 Loss 2.2256 Accuracy 0.1943\n",
      "Epoch 11 Batch 1600 Loss 2.2231 Accuracy 0.1948\n",
      "Epoch 11 Batch 1664 Loss 2.2227 Accuracy 0.1950\n",
      "Epoch 11 Batch 1728 Loss 2.2188 Accuracy 0.1951\n",
      "Epoch 11 Batch 1792 Loss 2.2164 Accuracy 0.1951\n",
      "Epoch 11 Batch 1856 Loss 2.2141 Accuracy 0.1953\n",
      "Epoch 11 Batch 1920 Loss 2.2107 Accuracy 0.1954\n",
      "Epoch 11 Batch 1984 Loss 2.2089 Accuracy 0.1953\n",
      "Epoch 11 Batch 2048 Loss 2.2076 Accuracy 0.1952\n",
      "Epoch 11 Batch 2112 Loss 2.2059 Accuracy 0.1951\n",
      "Epoch 11 Batch 2176 Loss 2.2027 Accuracy 0.1952\n",
      "Epoch 11 Batch 2240 Loss 2.2004 Accuracy 0.1956\n",
      "Epoch 11 Batch 2304 Loss 2.1968 Accuracy 0.1958\n",
      "Epoch 11 Batch 2368 Loss 2.1944 Accuracy 0.1960\n",
      "Epoch 11 Batch 2432 Loss 2.1932 Accuracy 0.1959\n",
      "Epoch 11 Batch 2496 Loss 2.1907 Accuracy 0.1962\n",
      "Epoch 11 Batch 2560 Loss 2.1887 Accuracy 0.1963\n",
      "Epoch 11 Batch 2624 Loss 2.1864 Accuracy 0.1964\n",
      "Epoch 11 Batch 2688 Loss 2.1840 Accuracy 0.1965\n",
      "Epoch 11 Batch 2752 Loss 2.1818 Accuracy 0.1967\n",
      "Epoch 11 Batch 2816 Loss 2.1791 Accuracy 0.1970\n",
      "Epoch 11 Batch 2880 Loss 2.1764 Accuracy 0.1972\n",
      "Epoch 11 Batch 2944 Loss 2.1745 Accuracy 0.1971\n",
      "Epoch 11 Batch 3008 Loss 2.1716 Accuracy 0.1972\n",
      "Epoch 11 Batch 3072 Loss 2.1690 Accuracy 0.1974\n",
      "Epoch 11 Batch 3136 Loss 2.1670 Accuracy 0.1976\n",
      "Epoch 11 Loss 2.1670 Accuracy 0.1976\n",
      "Time taken for 1 epoch: 39.143845081329346 secs\n",
      "\n",
      "Epoch 12 Batch 64 Loss 2.0485 Accuracy 0.1960\n",
      "Epoch 12 Batch 128 Loss 2.0568 Accuracy 0.2026\n",
      "Epoch 12 Batch 192 Loss 2.0593 Accuracy 0.2020\n",
      "Epoch 12 Batch 256 Loss 2.0542 Accuracy 0.2038\n",
      "Epoch 12 Batch 320 Loss 2.0620 Accuracy 0.2031\n",
      "Epoch 12 Batch 384 Loss 2.0572 Accuracy 0.2025\n",
      "Epoch 12 Batch 448 Loss 2.0498 Accuracy 0.2028\n",
      "Epoch 12 Batch 512 Loss 2.0451 Accuracy 0.2036\n",
      "Epoch 12 Batch 576 Loss 2.0432 Accuracy 0.2036\n",
      "Epoch 12 Batch 640 Loss 2.0392 Accuracy 0.2038\n",
      "Epoch 12 Batch 704 Loss 2.0375 Accuracy 0.2035\n",
      "Epoch 12 Batch 768 Loss 2.0372 Accuracy 0.2045\n",
      "Epoch 12 Batch 832 Loss 2.0375 Accuracy 0.2044\n",
      "Epoch 12 Batch 896 Loss 2.0359 Accuracy 0.2044\n",
      "Epoch 12 Batch 960 Loss 2.0321 Accuracy 0.2047\n",
      "Epoch 12 Batch 1024 Loss 2.0307 Accuracy 0.2048\n",
      "Epoch 12 Batch 1088 Loss 2.0262 Accuracy 0.2054\n",
      "Epoch 12 Batch 1152 Loss 2.0233 Accuracy 0.2060\n",
      "Epoch 12 Batch 1216 Loss 2.0214 Accuracy 0.2059\n",
      "Epoch 12 Batch 1280 Loss 2.0207 Accuracy 0.2055\n",
      "Epoch 12 Batch 1344 Loss 2.0190 Accuracy 0.2055\n",
      "Epoch 12 Batch 1408 Loss 2.0167 Accuracy 0.2053\n",
      "Epoch 12 Batch 1472 Loss 2.0131 Accuracy 0.2053\n",
      "Epoch 12 Batch 1536 Loss 2.0124 Accuracy 0.2057\n",
      "Epoch 12 Batch 1600 Loss 2.0114 Accuracy 0.2057\n",
      "Epoch 12 Batch 1664 Loss 2.0101 Accuracy 0.2054\n",
      "Epoch 12 Batch 1728 Loss 2.0098 Accuracy 0.2057\n",
      "Epoch 12 Batch 1792 Loss 2.0068 Accuracy 0.2061\n",
      "Epoch 12 Batch 1856 Loss 2.0047 Accuracy 0.2059\n",
      "Epoch 12 Batch 1920 Loss 2.0024 Accuracy 0.2060\n",
      "Epoch 12 Batch 1984 Loss 2.0017 Accuracy 0.2060\n",
      "Epoch 12 Batch 2048 Loss 1.9999 Accuracy 0.2061\n",
      "Epoch 12 Batch 2112 Loss 1.9985 Accuracy 0.2061\n",
      "Epoch 12 Batch 2176 Loss 1.9972 Accuracy 0.2062\n",
      "Epoch 12 Batch 2240 Loss 1.9950 Accuracy 0.2063\n",
      "Epoch 12 Batch 2304 Loss 1.9940 Accuracy 0.2064\n",
      "Epoch 12 Batch 2368 Loss 1.9922 Accuracy 0.2064\n",
      "Epoch 12 Batch 2432 Loss 1.9902 Accuracy 0.2063\n",
      "Epoch 12 Batch 2496 Loss 1.9882 Accuracy 0.2064\n",
      "Epoch 12 Batch 2560 Loss 1.9870 Accuracy 0.2065\n",
      "Epoch 12 Batch 2624 Loss 1.9851 Accuracy 0.2066\n",
      "Epoch 12 Batch 2688 Loss 1.9835 Accuracy 0.2066\n",
      "Epoch 12 Batch 2752 Loss 1.9818 Accuracy 0.2066\n",
      "Epoch 12 Batch 2816 Loss 1.9795 Accuracy 0.2067\n",
      "Epoch 12 Batch 2880 Loss 1.9773 Accuracy 0.2066\n",
      "Epoch 12 Batch 2944 Loss 1.9756 Accuracy 0.2068\n",
      "Epoch 12 Batch 3008 Loss 1.9736 Accuracy 0.2068\n",
      "Epoch 12 Batch 3072 Loss 1.9719 Accuracy 0.2068\n",
      "Epoch 12 Batch 3136 Loss 1.9708 Accuracy 0.2068\n",
      "Epoch 12 Loss 1.9708 Accuracy 0.2068\n",
      "Time taken for 1 epoch: 37.60092520713806 secs\n",
      "\n",
      "Epoch 13 Batch 64 Loss 1.8819 Accuracy 0.2150\n",
      "Epoch 13 Batch 128 Loss 1.8847 Accuracy 0.2145\n",
      "Epoch 13 Batch 192 Loss 1.8751 Accuracy 0.2120\n",
      "Epoch 13 Batch 256 Loss 1.8651 Accuracy 0.2125\n",
      "Epoch 13 Batch 320 Loss 1.8616 Accuracy 0.2106\n",
      "Epoch 13 Batch 384 Loss 1.8687 Accuracy 0.2101\n",
      "Epoch 13 Batch 448 Loss 1.8698 Accuracy 0.2099\n",
      "Epoch 13 Batch 512 Loss 1.8644 Accuracy 0.2112\n",
      "Epoch 13 Batch 576 Loss 1.8605 Accuracy 0.2119\n",
      "Epoch 13 Batch 640 Loss 1.8575 Accuracy 0.2128\n",
      "Epoch 13 Batch 704 Loss 1.8597 Accuracy 0.2121\n",
      "Epoch 13 Batch 768 Loss 1.8589 Accuracy 0.2129\n",
      "Epoch 13 Batch 832 Loss 1.8589 Accuracy 0.2122\n",
      "Epoch 13 Batch 896 Loss 1.8539 Accuracy 0.2133\n",
      "Epoch 13 Batch 960 Loss 1.8541 Accuracy 0.2129\n",
      "Epoch 13 Batch 1024 Loss 1.8540 Accuracy 0.2136\n",
      "Epoch 13 Batch 1088 Loss 1.8514 Accuracy 0.2132\n",
      "Epoch 13 Batch 1152 Loss 1.8502 Accuracy 0.2128\n",
      "Epoch 13 Batch 1216 Loss 1.8479 Accuracy 0.2125\n",
      "Epoch 13 Batch 1280 Loss 1.8454 Accuracy 0.2131\n",
      "Epoch 13 Batch 1344 Loss 1.8450 Accuracy 0.2132\n",
      "Epoch 13 Batch 1408 Loss 1.8426 Accuracy 0.2136\n",
      "Epoch 13 Batch 1472 Loss 1.8413 Accuracy 0.2132\n",
      "Epoch 13 Batch 1536 Loss 1.8411 Accuracy 0.2129\n",
      "Epoch 13 Batch 1600 Loss 1.8391 Accuracy 0.2133\n",
      "Epoch 13 Batch 1664 Loss 1.8373 Accuracy 0.2131\n",
      "Epoch 13 Batch 1728 Loss 1.8368 Accuracy 0.2131\n",
      "Epoch 13 Batch 1792 Loss 1.8360 Accuracy 0.2128\n",
      "Epoch 13 Batch 1856 Loss 1.8346 Accuracy 0.2129\n",
      "Epoch 13 Batch 1920 Loss 1.8335 Accuracy 0.2129\n",
      "Epoch 13 Batch 1984 Loss 1.8316 Accuracy 0.2129\n",
      "Epoch 13 Batch 2048 Loss 1.8306 Accuracy 0.2128\n",
      "Epoch 13 Batch 2112 Loss 1.8285 Accuracy 0.2129\n",
      "Epoch 13 Batch 2176 Loss 1.8267 Accuracy 0.2130\n",
      "Epoch 13 Batch 2240 Loss 1.8252 Accuracy 0.2134\n",
      "Epoch 13 Batch 2304 Loss 1.8240 Accuracy 0.2134\n",
      "Epoch 13 Batch 2368 Loss 1.8236 Accuracy 0.2136\n",
      "Epoch 13 Batch 2432 Loss 1.8228 Accuracy 0.2136\n",
      "Epoch 13 Batch 2496 Loss 1.8215 Accuracy 0.2137\n",
      "Epoch 13 Batch 2560 Loss 1.8210 Accuracy 0.2138\n",
      "Epoch 13 Batch 2624 Loss 1.8182 Accuracy 0.2139\n",
      "Epoch 13 Batch 2688 Loss 1.8171 Accuracy 0.2142\n",
      "Epoch 13 Batch 2752 Loss 1.8160 Accuracy 0.2142\n",
      "Epoch 13 Batch 2816 Loss 1.8153 Accuracy 0.2142\n",
      "Epoch 13 Batch 2880 Loss 1.8131 Accuracy 0.2143\n",
      "Epoch 13 Batch 2944 Loss 1.8109 Accuracy 0.2145\n",
      "Epoch 13 Batch 3008 Loss 1.8097 Accuracy 0.2144\n",
      "Epoch 13 Batch 3072 Loss 1.8087 Accuracy 0.2145\n",
      "Epoch 13 Batch 3136 Loss 1.8074 Accuracy 0.2145\n",
      "Epoch 13 Loss 1.8074 Accuracy 0.2145\n",
      "Time taken for 1 epoch: 38.95561099052429 secs\n",
      "\n",
      "Epoch 14 Batch 64 Loss 1.7713 Accuracy 0.2141\n",
      "Epoch 14 Batch 128 Loss 1.7587 Accuracy 0.2116\n",
      "Epoch 14 Batch 192 Loss 1.7452 Accuracy 0.2134\n",
      "Epoch 14 Batch 256 Loss 1.7401 Accuracy 0.2121\n",
      "Epoch 14 Batch 320 Loss 1.7313 Accuracy 0.2127\n",
      "Epoch 14 Batch 384 Loss 1.7287 Accuracy 0.2142\n",
      "Epoch 14 Batch 448 Loss 1.7257 Accuracy 0.2151\n",
      "Epoch 14 Batch 512 Loss 1.7266 Accuracy 0.2162\n",
      "Epoch 14 Batch 576 Loss 1.7244 Accuracy 0.2177\n",
      "Epoch 14 Batch 640 Loss 1.7211 Accuracy 0.2187\n",
      "Epoch 14 Batch 704 Loss 1.7199 Accuracy 0.2186\n",
      "Epoch 14 Batch 768 Loss 1.7208 Accuracy 0.2183\n",
      "Epoch 14 Batch 832 Loss 1.7171 Accuracy 0.2188\n",
      "Epoch 14 Batch 896 Loss 1.7146 Accuracy 0.2187\n",
      "Epoch 14 Batch 960 Loss 1.7164 Accuracy 0.2188\n",
      "Epoch 14 Batch 1024 Loss 1.7149 Accuracy 0.2191\n",
      "Epoch 14 Batch 1088 Loss 1.7136 Accuracy 0.2197\n",
      "Epoch 14 Batch 1152 Loss 1.7125 Accuracy 0.2196\n",
      "Epoch 14 Batch 1216 Loss 1.7101 Accuracy 0.2198\n",
      "Epoch 14 Batch 1280 Loss 1.7076 Accuracy 0.2200\n",
      "Epoch 14 Batch 1344 Loss 1.7047 Accuracy 0.2202\n",
      "Epoch 14 Batch 1408 Loss 1.7030 Accuracy 0.2201\n",
      "Epoch 14 Batch 1472 Loss 1.7019 Accuracy 0.2197\n",
      "Epoch 14 Batch 1536 Loss 1.7000 Accuracy 0.2197\n",
      "Epoch 14 Batch 1600 Loss 1.6972 Accuracy 0.2202\n",
      "Epoch 14 Batch 1664 Loss 1.6964 Accuracy 0.2201\n",
      "Epoch 14 Batch 1728 Loss 1.6972 Accuracy 0.2202\n",
      "Epoch 14 Batch 1792 Loss 1.6949 Accuracy 0.2205\n",
      "Epoch 14 Batch 1856 Loss 1.6936 Accuracy 0.2205\n",
      "Epoch 14 Batch 1920 Loss 1.6928 Accuracy 0.2204\n",
      "Epoch 14 Batch 1984 Loss 1.6908 Accuracy 0.2206\n",
      "Epoch 14 Batch 2048 Loss 1.6896 Accuracy 0.2205\n",
      "Epoch 14 Batch 2112 Loss 1.6881 Accuracy 0.2204\n",
      "Epoch 14 Batch 2176 Loss 1.6878 Accuracy 0.2201\n",
      "Epoch 14 Batch 2240 Loss 1.6868 Accuracy 0.2202\n",
      "Epoch 14 Batch 2304 Loss 1.6850 Accuracy 0.2203\n",
      "Epoch 14 Batch 2368 Loss 1.6851 Accuracy 0.2204\n",
      "Epoch 14 Batch 2432 Loss 1.6841 Accuracy 0.2205\n",
      "Epoch 14 Batch 2496 Loss 1.6831 Accuracy 0.2209\n",
      "Epoch 14 Batch 2560 Loss 1.6820 Accuracy 0.2212\n",
      "Epoch 14 Batch 2624 Loss 1.6815 Accuracy 0.2213\n",
      "Epoch 14 Batch 2688 Loss 1.6806 Accuracy 0.2214\n",
      "Epoch 14 Batch 2752 Loss 1.6794 Accuracy 0.2216\n",
      "Epoch 14 Batch 2816 Loss 1.6777 Accuracy 0.2216\n",
      "Epoch 14 Batch 2880 Loss 1.6767 Accuracy 0.2218\n",
      "Epoch 14 Batch 2944 Loss 1.6752 Accuracy 0.2219\n",
      "Epoch 14 Batch 3008 Loss 1.6741 Accuracy 0.2219\n",
      "Epoch 14 Batch 3072 Loss 1.6737 Accuracy 0.2219\n",
      "Epoch 14 Batch 3136 Loss 1.6728 Accuracy 0.2219\n",
      "Epoch 14 Loss 1.6728 Accuracy 0.2219\n",
      "Time taken for 1 epoch: 37.957716941833496 secs\n",
      "\n",
      "Epoch 15 Batch 64 Loss 1.6151 Accuracy 0.2282\n",
      "Epoch 15 Batch 128 Loss 1.6184 Accuracy 0.2308\n",
      "Epoch 15 Batch 192 Loss 1.6199 Accuracy 0.2314\n",
      "Epoch 15 Batch 256 Loss 1.6142 Accuracy 0.2311\n",
      "Epoch 15 Batch 320 Loss 1.6166 Accuracy 0.2286\n",
      "Epoch 15 Batch 384 Loss 1.6176 Accuracy 0.2271\n",
      "Epoch 15 Batch 448 Loss 1.6106 Accuracy 0.2266\n",
      "Epoch 15 Batch 512 Loss 1.6096 Accuracy 0.2268\n",
      "Epoch 15 Batch 576 Loss 1.6091 Accuracy 0.2267\n",
      "Epoch 15 Batch 640 Loss 1.6082 Accuracy 0.2262\n",
      "Epoch 15 Batch 704 Loss 1.6062 Accuracy 0.2258\n",
      "Epoch 15 Batch 768 Loss 1.6061 Accuracy 0.2256\n",
      "Epoch 15 Batch 832 Loss 1.6027 Accuracy 0.2263\n",
      "Epoch 15 Batch 896 Loss 1.5972 Accuracy 0.2269\n",
      "Epoch 15 Batch 960 Loss 1.5974 Accuracy 0.2271\n",
      "Epoch 15 Batch 1024 Loss 1.5997 Accuracy 0.2272\n",
      "Epoch 15 Batch 1088 Loss 1.6001 Accuracy 0.2270\n",
      "Epoch 15 Batch 1152 Loss 1.5960 Accuracy 0.2272\n",
      "Epoch 15 Batch 1216 Loss 1.5984 Accuracy 0.2270\n",
      "Epoch 15 Batch 1280 Loss 1.5967 Accuracy 0.2274\n",
      "Epoch 15 Batch 1344 Loss 1.5957 Accuracy 0.2271\n",
      "Epoch 15 Batch 1408 Loss 1.5956 Accuracy 0.2268\n",
      "Epoch 15 Batch 1472 Loss 1.5952 Accuracy 0.2267\n",
      "Epoch 15 Batch 1536 Loss 1.5928 Accuracy 0.2266\n",
      "Epoch 15 Batch 1600 Loss 1.5938 Accuracy 0.2263\n",
      "Epoch 15 Batch 1664 Loss 1.5945 Accuracy 0.2263\n",
      "Epoch 15 Batch 1728 Loss 1.5938 Accuracy 0.2264\n",
      "Epoch 15 Batch 1792 Loss 1.5930 Accuracy 0.2261\n",
      "Epoch 15 Batch 1856 Loss 1.5921 Accuracy 0.2265\n",
      "Epoch 15 Batch 1920 Loss 1.5909 Accuracy 0.2266\n",
      "Epoch 15 Batch 1984 Loss 1.5899 Accuracy 0.2267\n",
      "Epoch 15 Batch 2048 Loss 1.5894 Accuracy 0.2268\n",
      "Epoch 15 Batch 2112 Loss 1.5874 Accuracy 0.2270\n",
      "Epoch 15 Batch 2176 Loss 1.5878 Accuracy 0.2269\n",
      "Epoch 15 Batch 2240 Loss 1.5861 Accuracy 0.2272\n",
      "Epoch 15 Batch 2304 Loss 1.5843 Accuracy 0.2273\n",
      "Epoch 15 Batch 2368 Loss 1.5838 Accuracy 0.2274\n",
      "Epoch 15 Batch 2432 Loss 1.5829 Accuracy 0.2273\n",
      "Epoch 15 Batch 2496 Loss 1.5819 Accuracy 0.2274\n",
      "Epoch 15 Batch 2560 Loss 1.5800 Accuracy 0.2274\n",
      "Epoch 15 Batch 2624 Loss 1.5788 Accuracy 0.2276\n",
      "Epoch 15 Batch 2688 Loss 1.5775 Accuracy 0.2276\n",
      "Epoch 15 Batch 2752 Loss 1.5763 Accuracy 0.2278\n",
      "Epoch 15 Batch 2816 Loss 1.5749 Accuracy 0.2280\n",
      "Epoch 15 Batch 2880 Loss 1.5746 Accuracy 0.2281\n",
      "Epoch 15 Batch 2944 Loss 1.5733 Accuracy 0.2283\n",
      "Epoch 15 Batch 3008 Loss 1.5719 Accuracy 0.2283\n",
      "Epoch 15 Batch 3072 Loss 1.5712 Accuracy 0.2283\n",
      "Epoch 15 Batch 3136 Loss 1.5697 Accuracy 0.2284\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
      "Epoch 15 Loss 1.5697 Accuracy 0.2284\n",
      "Time taken for 1 epoch: 38.7521390914917 secs\n",
      "\n",
      "Epoch 16 Batch 64 Loss 1.5482 Accuracy 0.2431\n",
      "Epoch 16 Batch 128 Loss 1.5385 Accuracy 0.2371\n",
      "Epoch 16 Batch 192 Loss 1.5364 Accuracy 0.2331\n",
      "Epoch 16 Batch 256 Loss 1.5357 Accuracy 0.2325\n",
      "Epoch 16 Batch 320 Loss 1.5265 Accuracy 0.2322\n",
      "Epoch 16 Batch 384 Loss 1.5277 Accuracy 0.2307\n",
      "Epoch 16 Batch 448 Loss 1.5230 Accuracy 0.2310\n",
      "Epoch 16 Batch 512 Loss 1.5223 Accuracy 0.2322\n",
      "Epoch 16 Batch 576 Loss 1.5197 Accuracy 0.2328\n",
      "Epoch 16 Batch 640 Loss 1.5129 Accuracy 0.2332\n",
      "Epoch 16 Batch 704 Loss 1.5125 Accuracy 0.2322\n",
      "Epoch 16 Batch 768 Loss 1.5108 Accuracy 0.2320\n",
      "Epoch 16 Batch 832 Loss 1.5137 Accuracy 0.2317\n",
      "Epoch 16 Batch 896 Loss 1.5097 Accuracy 0.2315\n",
      "Epoch 16 Batch 960 Loss 1.5096 Accuracy 0.2316\n",
      "Epoch 16 Batch 1024 Loss 1.5109 Accuracy 0.2319\n",
      "Epoch 16 Batch 1088 Loss 1.5102 Accuracy 0.2322\n",
      "Epoch 16 Batch 1152 Loss 1.5105 Accuracy 0.2322\n",
      "Epoch 16 Batch 1216 Loss 1.5077 Accuracy 0.2325\n",
      "Epoch 16 Batch 1280 Loss 1.5062 Accuracy 0.2324\n",
      "Epoch 16 Batch 1344 Loss 1.5060 Accuracy 0.2322\n",
      "Epoch 16 Batch 1408 Loss 1.5041 Accuracy 0.2322\n",
      "Epoch 16 Batch 1472 Loss 1.5050 Accuracy 0.2322\n",
      "Epoch 16 Batch 1536 Loss 1.5052 Accuracy 0.2321\n",
      "Epoch 16 Batch 1600 Loss 1.5034 Accuracy 0.2322\n",
      "Epoch 16 Batch 1664 Loss 1.5031 Accuracy 0.2325\n",
      "Epoch 16 Batch 1728 Loss 1.5014 Accuracy 0.2327\n",
      "Epoch 16 Batch 1792 Loss 1.4998 Accuracy 0.2326\n",
      "Epoch 16 Batch 1856 Loss 1.4989 Accuracy 0.2323\n",
      "Epoch 16 Batch 1920 Loss 1.4975 Accuracy 0.2323\n",
      "Epoch 16 Batch 1984 Loss 1.4970 Accuracy 0.2326\n",
      "Epoch 16 Batch 2048 Loss 1.4952 Accuracy 0.2326\n",
      "Epoch 16 Batch 2112 Loss 1.4950 Accuracy 0.2324\n",
      "Epoch 16 Batch 2176 Loss 1.4948 Accuracy 0.2326\n",
      "Epoch 16 Batch 2240 Loss 1.4940 Accuracy 0.2326\n",
      "Epoch 16 Batch 2304 Loss 1.4944 Accuracy 0.2325\n",
      "Epoch 16 Batch 2368 Loss 1.4938 Accuracy 0.2326\n",
      "Epoch 16 Batch 2432 Loss 1.4934 Accuracy 0.2326\n",
      "Epoch 16 Batch 2496 Loss 1.4927 Accuracy 0.2326\n",
      "Epoch 16 Batch 2560 Loss 1.4925 Accuracy 0.2327\n",
      "Epoch 16 Batch 2624 Loss 1.4901 Accuracy 0.2328\n",
      "Epoch 16 Batch 2688 Loss 1.4905 Accuracy 0.2327\n",
      "Epoch 16 Batch 2752 Loss 1.4893 Accuracy 0.2327\n",
      "Epoch 16 Batch 2816 Loss 1.4885 Accuracy 0.2329\n",
      "Epoch 16 Batch 2880 Loss 1.4889 Accuracy 0.2329\n",
      "Epoch 16 Batch 2944 Loss 1.4874 Accuracy 0.2333\n",
      "Epoch 16 Batch 3008 Loss 1.4870 Accuracy 0.2333\n",
      "Epoch 16 Batch 3072 Loss 1.4875 Accuracy 0.2332\n",
      "Epoch 16 Batch 3136 Loss 1.4879 Accuracy 0.2332\n",
      "Epoch 16 Loss 1.4879 Accuracy 0.2332\n",
      "Time taken for 1 epoch: 37.21838021278381 secs\n",
      "\n",
      "Epoch 17 Batch 64 Loss 1.4371 Accuracy 0.2409\n",
      "Epoch 17 Batch 128 Loss 1.4384 Accuracy 0.2353\n",
      "Epoch 17 Batch 192 Loss 1.4541 Accuracy 0.2331\n",
      "Epoch 17 Batch 256 Loss 1.4588 Accuracy 0.2344\n",
      "Epoch 17 Batch 320 Loss 1.4639 Accuracy 0.2354\n",
      "Epoch 17 Batch 384 Loss 1.4590 Accuracy 0.2385\n",
      "Epoch 17 Batch 448 Loss 1.4593 Accuracy 0.2384\n",
      "Epoch 17 Batch 512 Loss 1.4499 Accuracy 0.2388\n",
      "Epoch 17 Batch 576 Loss 1.4453 Accuracy 0.2388\n",
      "Epoch 17 Batch 640 Loss 1.4425 Accuracy 0.2389\n",
      "Epoch 17 Batch 704 Loss 1.4365 Accuracy 0.2384\n",
      "Epoch 17 Batch 768 Loss 1.4372 Accuracy 0.2382\n",
      "Epoch 17 Batch 832 Loss 1.4384 Accuracy 0.2381\n",
      "Epoch 17 Batch 896 Loss 1.4365 Accuracy 0.2384\n",
      "Epoch 17 Batch 960 Loss 1.4376 Accuracy 0.2379\n",
      "Epoch 17 Batch 1024 Loss 1.4381 Accuracy 0.2376\n",
      "Epoch 17 Batch 1088 Loss 1.4407 Accuracy 0.2369\n",
      "Epoch 17 Batch 1152 Loss 1.4376 Accuracy 0.2374\n",
      "Epoch 17 Batch 1216 Loss 1.4394 Accuracy 0.2370\n",
      "Epoch 17 Batch 1280 Loss 1.4413 Accuracy 0.2371\n",
      "Epoch 17 Batch 1344 Loss 1.4404 Accuracy 0.2371\n",
      "Epoch 17 Batch 1408 Loss 1.4380 Accuracy 0.2369\n",
      "Epoch 17 Batch 1472 Loss 1.4371 Accuracy 0.2371\n",
      "Epoch 17 Batch 1536 Loss 1.4376 Accuracy 0.2369\n",
      "Epoch 17 Batch 1600 Loss 1.4369 Accuracy 0.2370\n",
      "Epoch 17 Batch 1664 Loss 1.4362 Accuracy 0.2373\n",
      "Epoch 17 Batch 1728 Loss 1.4345 Accuracy 0.2371\n",
      "Epoch 17 Batch 1792 Loss 1.4345 Accuracy 0.2371\n",
      "Epoch 17 Batch 1856 Loss 1.4324 Accuracy 0.2375\n",
      "Epoch 17 Batch 1920 Loss 1.4325 Accuracy 0.2375\n",
      "Epoch 17 Batch 1984 Loss 1.4320 Accuracy 0.2377\n",
      "Epoch 17 Batch 2048 Loss 1.4304 Accuracy 0.2378\n",
      "Epoch 17 Batch 2112 Loss 1.4310 Accuracy 0.2376\n",
      "Epoch 17 Batch 2176 Loss 1.4296 Accuracy 0.2379\n",
      "Epoch 17 Batch 2240 Loss 1.4286 Accuracy 0.2378\n",
      "Epoch 17 Batch 2304 Loss 1.4286 Accuracy 0.2376\n",
      "Epoch 17 Batch 2368 Loss 1.4270 Accuracy 0.2377\n",
      "Epoch 17 Batch 2432 Loss 1.4263 Accuracy 0.2379\n",
      "Epoch 17 Batch 2496 Loss 1.4250 Accuracy 0.2378\n",
      "Epoch 17 Batch 2560 Loss 1.4245 Accuracy 0.2377\n",
      "Epoch 17 Batch 2624 Loss 1.4239 Accuracy 0.2380\n",
      "Epoch 17 Batch 2688 Loss 1.4237 Accuracy 0.2378\n",
      "Epoch 17 Batch 2752 Loss 1.4232 Accuracy 0.2381\n",
      "Epoch 17 Batch 2816 Loss 1.4230 Accuracy 0.2380\n",
      "Epoch 17 Batch 2880 Loss 1.4229 Accuracy 0.2379\n",
      "Epoch 17 Batch 2944 Loss 1.4229 Accuracy 0.2379\n",
      "Epoch 17 Batch 3008 Loss 1.4221 Accuracy 0.2378\n",
      "Epoch 17 Batch 3072 Loss 1.4215 Accuracy 0.2378\n",
      "Epoch 17 Batch 3136 Loss 1.4211 Accuracy 0.2377\n",
      "Epoch 17 Loss 1.4211 Accuracy 0.2377\n",
      "Time taken for 1 epoch: 37.131170988082886 secs\n",
      "\n",
      "Epoch 18 Batch 64 Loss 1.3823 Accuracy 0.2382\n",
      "Epoch 18 Batch 128 Loss 1.3759 Accuracy 0.2390\n",
      "Epoch 18 Batch 192 Loss 1.3796 Accuracy 0.2391\n",
      "Epoch 18 Batch 256 Loss 1.3907 Accuracy 0.2384\n",
      "Epoch 18 Batch 320 Loss 1.3939 Accuracy 0.2401\n",
      "Epoch 18 Batch 384 Loss 1.3915 Accuracy 0.2393\n",
      "Epoch 18 Batch 448 Loss 1.3915 Accuracy 0.2391\n",
      "Epoch 18 Batch 512 Loss 1.3892 Accuracy 0.2400\n",
      "Epoch 18 Batch 576 Loss 1.3830 Accuracy 0.2414\n",
      "Epoch 18 Batch 640 Loss 1.3816 Accuracy 0.2418\n",
      "Epoch 18 Batch 704 Loss 1.3837 Accuracy 0.2413\n",
      "Epoch 18 Batch 768 Loss 1.3819 Accuracy 0.2409\n",
      "Epoch 18 Batch 832 Loss 1.3794 Accuracy 0.2412\n",
      "Epoch 18 Batch 896 Loss 1.3780 Accuracy 0.2409\n",
      "Epoch 18 Batch 960 Loss 1.3778 Accuracy 0.2408\n",
      "Epoch 18 Batch 1024 Loss 1.3753 Accuracy 0.2411\n",
      "Epoch 18 Batch 1088 Loss 1.3728 Accuracy 0.2412\n",
      "Epoch 18 Batch 1152 Loss 1.3737 Accuracy 0.2406\n",
      "Epoch 18 Batch 1216 Loss 1.3752 Accuracy 0.2407\n",
      "Epoch 18 Batch 1280 Loss 1.3738 Accuracy 0.2410\n",
      "Epoch 18 Batch 1344 Loss 1.3731 Accuracy 0.2409\n",
      "Epoch 18 Batch 1408 Loss 1.3730 Accuracy 0.2413\n",
      "Epoch 18 Batch 1472 Loss 1.3752 Accuracy 0.2411\n",
      "Epoch 18 Batch 1536 Loss 1.3758 Accuracy 0.2411\n",
      "Epoch 18 Batch 1600 Loss 1.3757 Accuracy 0.2414\n",
      "Epoch 18 Batch 1664 Loss 1.3758 Accuracy 0.2418\n",
      "Epoch 18 Batch 1728 Loss 1.3766 Accuracy 0.2418\n",
      "Epoch 18 Batch 1792 Loss 1.3752 Accuracy 0.2416\n",
      "Epoch 18 Batch 1856 Loss 1.3745 Accuracy 0.2422\n",
      "Epoch 18 Batch 1920 Loss 1.3746 Accuracy 0.2422\n",
      "Epoch 18 Batch 1984 Loss 1.3734 Accuracy 0.2423\n",
      "Epoch 18 Batch 2048 Loss 1.3734 Accuracy 0.2421\n",
      "Epoch 18 Batch 2112 Loss 1.3749 Accuracy 0.2421\n",
      "Epoch 18 Batch 2176 Loss 1.3734 Accuracy 0.2422\n",
      "Epoch 18 Batch 2240 Loss 1.3731 Accuracy 0.2421\n",
      "Epoch 18 Batch 2304 Loss 1.3716 Accuracy 0.2423\n",
      "Epoch 18 Batch 2368 Loss 1.3721 Accuracy 0.2424\n",
      "Epoch 18 Batch 2432 Loss 1.3721 Accuracy 0.2424\n",
      "Epoch 18 Batch 2496 Loss 1.3716 Accuracy 0.2424\n",
      "Epoch 18 Batch 2560 Loss 1.3708 Accuracy 0.2424\n",
      "Epoch 18 Batch 2624 Loss 1.3701 Accuracy 0.2423\n",
      "Epoch 18 Batch 2688 Loss 1.3705 Accuracy 0.2422\n",
      "Epoch 18 Batch 2752 Loss 1.3703 Accuracy 0.2424\n",
      "Epoch 18 Batch 2816 Loss 1.3698 Accuracy 0.2423\n",
      "Epoch 18 Batch 2880 Loss 1.3689 Accuracy 0.2425\n",
      "Epoch 18 Batch 2944 Loss 1.3686 Accuracy 0.2423\n",
      "Epoch 18 Batch 3008 Loss 1.3671 Accuracy 0.2425\n",
      "Epoch 18 Batch 3072 Loss 1.3670 Accuracy 0.2425\n",
      "Epoch 18 Batch 3136 Loss 1.3661 Accuracy 0.2425\n",
      "Epoch 18 Loss 1.3661 Accuracy 0.2425\n",
      "Time taken for 1 epoch: 38.85397291183472 secs\n",
      "\n",
      "Epoch 19 Batch 64 Loss 1.3697 Accuracy 0.2322\n",
      "Epoch 19 Batch 128 Loss 1.3643 Accuracy 0.2384\n",
      "Epoch 19 Batch 192 Loss 1.3641 Accuracy 0.2386\n",
      "Epoch 19 Batch 256 Loss 1.3531 Accuracy 0.2405\n",
      "Epoch 19 Batch 320 Loss 1.3499 Accuracy 0.2414\n",
      "Epoch 19 Batch 384 Loss 1.3415 Accuracy 0.2426\n",
      "Epoch 19 Batch 448 Loss 1.3416 Accuracy 0.2429\n",
      "Epoch 19 Batch 512 Loss 1.3378 Accuracy 0.2425\n",
      "Epoch 19 Batch 576 Loss 1.3449 Accuracy 0.2411\n",
      "Epoch 19 Batch 640 Loss 1.3418 Accuracy 0.2415\n",
      "Epoch 19 Batch 704 Loss 1.3444 Accuracy 0.2423\n",
      "Epoch 19 Batch 768 Loss 1.3502 Accuracy 0.2418\n",
      "Epoch 19 Batch 832 Loss 1.3492 Accuracy 0.2419\n",
      "Epoch 19 Batch 896 Loss 1.3490 Accuracy 0.2422\n",
      "Epoch 19 Batch 960 Loss 1.3452 Accuracy 0.2429\n",
      "Epoch 19 Batch 1024 Loss 1.3418 Accuracy 0.2431\n",
      "Epoch 19 Batch 1088 Loss 1.3417 Accuracy 0.2435\n",
      "Epoch 19 Batch 1152 Loss 1.3391 Accuracy 0.2439\n",
      "Epoch 19 Batch 1216 Loss 1.3383 Accuracy 0.2440\n",
      "Epoch 19 Batch 1280 Loss 1.3407 Accuracy 0.2446\n",
      "Epoch 19 Batch 1344 Loss 1.3392 Accuracy 0.2446\n",
      "Epoch 19 Batch 1408 Loss 1.3393 Accuracy 0.2445\n",
      "Epoch 19 Batch 1472 Loss 1.3376 Accuracy 0.2448\n",
      "Epoch 19 Batch 1536 Loss 1.3357 Accuracy 0.2446\n",
      "Epoch 19 Batch 1600 Loss 1.3356 Accuracy 0.2450\n",
      "Epoch 19 Batch 1664 Loss 1.3341 Accuracy 0.2456\n",
      "Epoch 19 Batch 1728 Loss 1.3325 Accuracy 0.2456\n",
      "Epoch 19 Batch 1792 Loss 1.3309 Accuracy 0.2460\n",
      "Epoch 19 Batch 1856 Loss 1.3292 Accuracy 0.2460\n",
      "Epoch 19 Batch 1920 Loss 1.3290 Accuracy 0.2460\n",
      "Epoch 19 Batch 1984 Loss 1.3271 Accuracy 0.2462\n",
      "Epoch 19 Batch 2048 Loss 1.3257 Accuracy 0.2463\n",
      "Epoch 19 Batch 2112 Loss 1.3240 Accuracy 0.2464\n",
      "Epoch 19 Batch 2176 Loss 1.3241 Accuracy 0.2465\n",
      "Epoch 19 Batch 2240 Loss 1.3213 Accuracy 0.2467\n",
      "Epoch 19 Batch 2304 Loss 1.3214 Accuracy 0.2465\n",
      "Epoch 19 Batch 2368 Loss 1.3194 Accuracy 0.2464\n",
      "Epoch 19 Batch 2432 Loss 1.3191 Accuracy 0.2465\n",
      "Epoch 19 Batch 2496 Loss 1.3170 Accuracy 0.2467\n",
      "Epoch 19 Batch 2560 Loss 1.3175 Accuracy 0.2467\n",
      "Epoch 19 Batch 2624 Loss 1.3173 Accuracy 0.2466\n",
      "Epoch 19 Batch 2688 Loss 1.3174 Accuracy 0.2466\n",
      "Epoch 19 Batch 2752 Loss 1.3185 Accuracy 0.2464\n",
      "Epoch 19 Batch 2816 Loss 1.3171 Accuracy 0.2466\n",
      "Epoch 19 Batch 2880 Loss 1.3176 Accuracy 0.2469\n",
      "Epoch 19 Batch 2944 Loss 1.3161 Accuracy 0.2469\n",
      "Epoch 19 Batch 3008 Loss 1.3166 Accuracy 0.2470\n",
      "Epoch 19 Batch 3072 Loss 1.3171 Accuracy 0.2469\n",
      "Epoch 19 Batch 3136 Loss 1.3167 Accuracy 0.2468\n",
      "Epoch 19 Loss 1.3167 Accuracy 0.2468\n",
      "Time taken for 1 epoch: 36.99584102630615 secs\n",
      "\n",
      "Epoch 20 Batch 64 Loss 1.3518 Accuracy 0.2441\n",
      "Epoch 20 Batch 128 Loss 1.3456 Accuracy 0.2476\n",
      "Epoch 20 Batch 192 Loss 1.3349 Accuracy 0.2469\n",
      "Epoch 20 Batch 256 Loss 1.3174 Accuracy 0.2474\n",
      "Epoch 20 Batch 320 Loss 1.3159 Accuracy 0.2482\n",
      "Epoch 20 Batch 384 Loss 1.3036 Accuracy 0.2493\n",
      "Epoch 20 Batch 448 Loss 1.3027 Accuracy 0.2503\n",
      "Epoch 20 Batch 512 Loss 1.3028 Accuracy 0.2504\n",
      "Epoch 20 Batch 576 Loss 1.3026 Accuracy 0.2500\n",
      "Epoch 20 Batch 640 Loss 1.3046 Accuracy 0.2486\n",
      "Epoch 20 Batch 704 Loss 1.2982 Accuracy 0.2496\n",
      "Epoch 20 Batch 768 Loss 1.3056 Accuracy 0.2476\n",
      "Epoch 20 Batch 832 Loss 1.3056 Accuracy 0.2480\n",
      "Epoch 20 Batch 896 Loss 1.3042 Accuracy 0.2482\n",
      "Epoch 20 Batch 960 Loss 1.3034 Accuracy 0.2485\n",
      "Epoch 20 Batch 1024 Loss 1.3038 Accuracy 0.2485\n",
      "Epoch 20 Batch 1088 Loss 1.3038 Accuracy 0.2479\n",
      "Epoch 20 Batch 1152 Loss 1.3005 Accuracy 0.2486\n",
      "Epoch 20 Batch 1216 Loss 1.2997 Accuracy 0.2487\n",
      "Epoch 20 Batch 1280 Loss 1.2994 Accuracy 0.2491\n",
      "Epoch 20 Batch 1344 Loss 1.2976 Accuracy 0.2491\n",
      "Epoch 20 Batch 1408 Loss 1.2970 Accuracy 0.2495\n",
      "Epoch 20 Batch 1472 Loss 1.2953 Accuracy 0.2501\n",
      "Epoch 20 Batch 1536 Loss 1.2927 Accuracy 0.2504\n",
      "Epoch 20 Batch 1600 Loss 1.2923 Accuracy 0.2504\n",
      "Epoch 20 Batch 1664 Loss 1.2927 Accuracy 0.2504\n",
      "Epoch 20 Batch 1728 Loss 1.2908 Accuracy 0.2506\n",
      "Epoch 20 Batch 1792 Loss 1.2911 Accuracy 0.2506\n",
      "Epoch 20 Batch 1856 Loss 1.2892 Accuracy 0.2510\n",
      "Epoch 20 Batch 1920 Loss 1.2888 Accuracy 0.2509\n",
      "Epoch 20 Batch 1984 Loss 1.2891 Accuracy 0.2507\n",
      "Epoch 20 Batch 2048 Loss 1.2911 Accuracy 0.2506\n",
      "Epoch 20 Batch 2112 Loss 1.2905 Accuracy 0.2504\n",
      "Epoch 20 Batch 2176 Loss 1.2895 Accuracy 0.2503\n",
      "Epoch 20 Batch 2240 Loss 1.2878 Accuracy 0.2510\n",
      "Epoch 20 Batch 2304 Loss 1.2883 Accuracy 0.2508\n",
      "Epoch 20 Batch 2368 Loss 1.2870 Accuracy 0.2505\n",
      "Epoch 20 Batch 2432 Loss 1.2865 Accuracy 0.2506\n",
      "Epoch 20 Batch 2496 Loss 1.2859 Accuracy 0.2507\n",
      "Epoch 20 Batch 2560 Loss 1.2841 Accuracy 0.2505\n",
      "Epoch 20 Batch 2624 Loss 1.2829 Accuracy 0.2504\n",
      "Epoch 20 Batch 2688 Loss 1.2836 Accuracy 0.2505\n",
      "Epoch 20 Batch 2752 Loss 1.2829 Accuracy 0.2504\n",
      "Epoch 20 Batch 2816 Loss 1.2823 Accuracy 0.2506\n",
      "Epoch 20 Batch 2880 Loss 1.2814 Accuracy 0.2506\n",
      "Epoch 20 Batch 2944 Loss 1.2808 Accuracy 0.2507\n",
      "Epoch 20 Batch 3008 Loss 1.2800 Accuracy 0.2507\n",
      "Epoch 20 Batch 3072 Loss 1.2797 Accuracy 0.2506\n",
      "Epoch 20 Batch 3136 Loss 1.2798 Accuracy 0.2505\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
      "Epoch 20 Loss 1.2798 Accuracy 0.2505\n",
      "Time taken for 1 epoch: 37.209216833114624 secs\n",
      "\n",
      "Epoch 21 Batch 64 Loss 1.1901 Accuracy 0.2613\n",
      "Epoch 21 Batch 128 Loss 1.2201 Accuracy 0.2619\n",
      "Epoch 21 Batch 192 Loss 1.2339 Accuracy 0.2588\n",
      "Epoch 21 Batch 256 Loss 1.2360 Accuracy 0.2544\n",
      "Epoch 21 Batch 320 Loss 1.2449 Accuracy 0.2521\n",
      "Epoch 21 Batch 384 Loss 1.2500 Accuracy 0.2515\n",
      "Epoch 21 Batch 448 Loss 1.2468 Accuracy 0.2521\n",
      "Epoch 21 Batch 512 Loss 1.2498 Accuracy 0.2521\n",
      "Epoch 21 Batch 576 Loss 1.2527 Accuracy 0.2520\n",
      "Epoch 21 Batch 640 Loss 1.2509 Accuracy 0.2528\n",
      "Epoch 21 Batch 704 Loss 1.2506 Accuracy 0.2526\n",
      "Epoch 21 Batch 768 Loss 1.2504 Accuracy 0.2529\n",
      "Epoch 21 Batch 832 Loss 1.2459 Accuracy 0.2535\n",
      "Epoch 21 Batch 896 Loss 1.2442 Accuracy 0.2542\n",
      "Epoch 21 Batch 960 Loss 1.2446 Accuracy 0.2542\n",
      "Epoch 21 Batch 1024 Loss 1.2446 Accuracy 0.2543\n",
      "Epoch 21 Batch 1088 Loss 1.2462 Accuracy 0.2538\n",
      "Epoch 21 Batch 1152 Loss 1.2481 Accuracy 0.2537\n",
      "Epoch 21 Batch 1216 Loss 1.2460 Accuracy 0.2541\n",
      "Epoch 21 Batch 1280 Loss 1.2443 Accuracy 0.2540\n",
      "Epoch 21 Batch 1344 Loss 1.2466 Accuracy 0.2537\n",
      "Epoch 21 Batch 1408 Loss 1.2475 Accuracy 0.2533\n",
      "Epoch 21 Batch 1472 Loss 1.2483 Accuracy 0.2528\n",
      "Epoch 21 Batch 1536 Loss 1.2483 Accuracy 0.2532\n",
      "Epoch 21 Batch 1600 Loss 1.2486 Accuracy 0.2530\n",
      "Epoch 21 Batch 1664 Loss 1.2495 Accuracy 0.2529\n",
      "Epoch 21 Batch 1728 Loss 1.2468 Accuracy 0.2537\n",
      "Epoch 21 Batch 1792 Loss 1.2474 Accuracy 0.2535\n",
      "Epoch 21 Batch 1856 Loss 1.2469 Accuracy 0.2537\n",
      "Epoch 21 Batch 1920 Loss 1.2460 Accuracy 0.2537\n",
      "Epoch 21 Batch 1984 Loss 1.2449 Accuracy 0.2538\n",
      "Epoch 21 Batch 2048 Loss 1.2445 Accuracy 0.2539\n",
      "Epoch 21 Batch 2112 Loss 1.2447 Accuracy 0.2538\n",
      "Epoch 21 Batch 2176 Loss 1.2439 Accuracy 0.2539\n",
      "Epoch 21 Batch 2240 Loss 1.2436 Accuracy 0.2543\n",
      "Epoch 21 Batch 2304 Loss 1.2430 Accuracy 0.2543\n",
      "Epoch 21 Batch 2368 Loss 1.2429 Accuracy 0.2544\n",
      "Epoch 21 Batch 2432 Loss 1.2421 Accuracy 0.2545\n",
      "Epoch 21 Batch 2496 Loss 1.2422 Accuracy 0.2546\n",
      "Epoch 21 Batch 2560 Loss 1.2420 Accuracy 0.2548\n",
      "Epoch 21 Batch 2624 Loss 1.2432 Accuracy 0.2547\n",
      "Epoch 21 Batch 2688 Loss 1.2437 Accuracy 0.2543\n",
      "Epoch 21 Batch 2752 Loss 1.2430 Accuracy 0.2541\n",
      "Epoch 21 Batch 2816 Loss 1.2420 Accuracy 0.2542\n",
      "Epoch 21 Batch 2880 Loss 1.2414 Accuracy 0.2546\n",
      "Epoch 21 Batch 2944 Loss 1.2416 Accuracy 0.2547\n",
      "Epoch 21 Batch 3008 Loss 1.2422 Accuracy 0.2547\n",
      "Epoch 21 Batch 3072 Loss 1.2420 Accuracy 0.2547\n",
      "Epoch 21 Batch 3136 Loss 1.2412 Accuracy 0.2548\n",
      "Epoch 21 Loss 1.2412 Accuracy 0.2548\n",
      "Time taken for 1 epoch: 38.93172311782837 secs\n",
      "\n",
      "Epoch 22 Batch 64 Loss 1.2330 Accuracy 0.2610\n",
      "Epoch 22 Batch 128 Loss 1.1983 Accuracy 0.2638\n",
      "Epoch 22 Batch 192 Loss 1.2078 Accuracy 0.2626\n",
      "Epoch 22 Batch 256 Loss 1.2065 Accuracy 0.2623\n",
      "Epoch 22 Batch 320 Loss 1.2100 Accuracy 0.2589\n",
      "Epoch 22 Batch 384 Loss 1.2139 Accuracy 0.2586\n",
      "Epoch 22 Batch 448 Loss 1.2129 Accuracy 0.2598\n",
      "Epoch 22 Batch 512 Loss 1.2131 Accuracy 0.2593\n",
      "Epoch 22 Batch 576 Loss 1.2130 Accuracy 0.2605\n",
      "Epoch 22 Batch 640 Loss 1.2191 Accuracy 0.2602\n",
      "Epoch 22 Batch 704 Loss 1.2173 Accuracy 0.2597\n",
      "Epoch 22 Batch 768 Loss 1.2192 Accuracy 0.2599\n",
      "Epoch 22 Batch 832 Loss 1.2201 Accuracy 0.2600\n",
      "Epoch 22 Batch 896 Loss 1.2215 Accuracy 0.2591\n",
      "Epoch 22 Batch 960 Loss 1.2231 Accuracy 0.2587\n",
      "Epoch 22 Batch 1024 Loss 1.2212 Accuracy 0.2586\n",
      "Epoch 22 Batch 1088 Loss 1.2196 Accuracy 0.2584\n",
      "Epoch 22 Batch 1152 Loss 1.2194 Accuracy 0.2588\n",
      "Epoch 22 Batch 1216 Loss 1.2196 Accuracy 0.2588\n",
      "Epoch 22 Batch 1280 Loss 1.2180 Accuracy 0.2588\n",
      "Epoch 22 Batch 1344 Loss 1.2159 Accuracy 0.2587\n",
      "Epoch 22 Batch 1408 Loss 1.2160 Accuracy 0.2584\n",
      "Epoch 22 Batch 1472 Loss 1.2144 Accuracy 0.2586\n",
      "Epoch 22 Batch 1536 Loss 1.2120 Accuracy 0.2589\n",
      "Epoch 22 Batch 1600 Loss 1.2137 Accuracy 0.2586\n",
      "Epoch 22 Batch 1664 Loss 1.2130 Accuracy 0.2585\n",
      "Epoch 22 Batch 1728 Loss 1.2133 Accuracy 0.2582\n",
      "Epoch 22 Batch 1792 Loss 1.2127 Accuracy 0.2586\n",
      "Epoch 22 Batch 1856 Loss 1.2132 Accuracy 0.2585\n",
      "Epoch 22 Batch 1920 Loss 1.2123 Accuracy 0.2584\n",
      "Epoch 22 Batch 1984 Loss 1.2132 Accuracy 0.2580\n",
      "Epoch 22 Batch 2048 Loss 1.2140 Accuracy 0.2581\n",
      "Epoch 22 Batch 2112 Loss 1.2146 Accuracy 0.2580\n",
      "Epoch 22 Batch 2176 Loss 1.2137 Accuracy 0.2580\n",
      "Epoch 22 Batch 2240 Loss 1.2128 Accuracy 0.2580\n",
      "Epoch 22 Batch 2304 Loss 1.2118 Accuracy 0.2582\n",
      "Epoch 22 Batch 2368 Loss 1.2120 Accuracy 0.2584\n",
      "Epoch 22 Batch 2432 Loss 1.2103 Accuracy 0.2586\n",
      "Epoch 22 Batch 2496 Loss 1.2096 Accuracy 0.2587\n",
      "Epoch 22 Batch 2560 Loss 1.2091 Accuracy 0.2587\n",
      "Epoch 22 Batch 2624 Loss 1.2098 Accuracy 0.2589\n",
      "Epoch 22 Batch 2688 Loss 1.2082 Accuracy 0.2590\n",
      "Epoch 22 Batch 2752 Loss 1.2072 Accuracy 0.2591\n",
      "Epoch 22 Batch 2816 Loss 1.2070 Accuracy 0.2591\n",
      "Epoch 22 Batch 2880 Loss 1.2075 Accuracy 0.2591\n",
      "Epoch 22 Batch 2944 Loss 1.2081 Accuracy 0.2588\n",
      "Epoch 22 Batch 3008 Loss 1.2071 Accuracy 0.2588\n",
      "Epoch 22 Batch 3072 Loss 1.2064 Accuracy 0.2591\n",
      "Epoch 22 Batch 3136 Loss 1.2039 Accuracy 0.2595\n",
      "Epoch 22 Loss 1.2039 Accuracy 0.2595\n",
      "Time taken for 1 epoch: 37.06944799423218 secs\n",
      "\n",
      "Epoch 23 Batch 64 Loss 1.1837 Accuracy 0.2673\n",
      "Epoch 23 Batch 128 Loss 1.1822 Accuracy 0.2654\n",
      "Epoch 23 Batch 192 Loss 1.1854 Accuracy 0.2649\n",
      "Epoch 23 Batch 256 Loss 1.1802 Accuracy 0.2621\n",
      "Epoch 23 Batch 320 Loss 1.1792 Accuracy 0.2604\n",
      "Epoch 23 Batch 384 Loss 1.1744 Accuracy 0.2623\n",
      "Epoch 23 Batch 448 Loss 1.1744 Accuracy 0.2628\n",
      "Epoch 23 Batch 512 Loss 1.1762 Accuracy 0.2622\n",
      "Epoch 23 Batch 576 Loss 1.1750 Accuracy 0.2631\n",
      "Epoch 23 Batch 640 Loss 1.1748 Accuracy 0.2629\n",
      "Epoch 23 Batch 704 Loss 1.1736 Accuracy 0.2640\n",
      "Epoch 23 Batch 768 Loss 1.1727 Accuracy 0.2638\n",
      "Epoch 23 Batch 832 Loss 1.1760 Accuracy 0.2638\n",
      "Epoch 23 Batch 896 Loss 1.1803 Accuracy 0.2632\n",
      "Epoch 23 Batch 960 Loss 1.1806 Accuracy 0.2626\n",
      "Epoch 23 Batch 1024 Loss 1.1791 Accuracy 0.2627\n",
      "Epoch 23 Batch 1088 Loss 1.1783 Accuracy 0.2635\n",
      "Epoch 23 Batch 1152 Loss 1.1771 Accuracy 0.2639\n",
      "Epoch 23 Batch 1216 Loss 1.1788 Accuracy 0.2631\n",
      "Epoch 23 Batch 1280 Loss 1.1796 Accuracy 0.2628\n",
      "Epoch 23 Batch 1344 Loss 1.1816 Accuracy 0.2630\n",
      "Epoch 23 Batch 1408 Loss 1.1813 Accuracy 0.2625\n",
      "Epoch 23 Batch 1472 Loss 1.1798 Accuracy 0.2628\n",
      "Epoch 23 Batch 1536 Loss 1.1790 Accuracy 0.2624\n",
      "Epoch 23 Batch 1600 Loss 1.1782 Accuracy 0.2631\n",
      "Epoch 23 Batch 1664 Loss 1.1768 Accuracy 0.2632\n",
      "Epoch 23 Batch 1728 Loss 1.1765 Accuracy 0.2637\n",
      "Epoch 23 Batch 1792 Loss 1.1773 Accuracy 0.2635\n",
      "Epoch 23 Batch 1856 Loss 1.1748 Accuracy 0.2636\n",
      "Epoch 23 Batch 1920 Loss 1.1754 Accuracy 0.2632\n",
      "Epoch 23 Batch 1984 Loss 1.1743 Accuracy 0.2633\n",
      "Epoch 23 Batch 2048 Loss 1.1732 Accuracy 0.2636\n",
      "Epoch 23 Batch 2112 Loss 1.1726 Accuracy 0.2634\n",
      "Epoch 23 Batch 2176 Loss 1.1730 Accuracy 0.2630\n",
      "Epoch 23 Batch 2240 Loss 1.1712 Accuracy 0.2628\n",
      "Epoch 23 Batch 2304 Loss 1.1704 Accuracy 0.2630\n",
      "Epoch 23 Batch 2368 Loss 1.1718 Accuracy 0.2627\n",
      "Epoch 23 Batch 2432 Loss 1.1709 Accuracy 0.2626\n",
      "Epoch 23 Batch 2496 Loss 1.1715 Accuracy 0.2624\n",
      "Epoch 23 Batch 2560 Loss 1.1715 Accuracy 0.2625\n",
      "Epoch 23 Batch 2624 Loss 1.1719 Accuracy 0.2626\n",
      "Epoch 23 Batch 2688 Loss 1.1716 Accuracy 0.2626\n",
      "Epoch 23 Batch 2752 Loss 1.1710 Accuracy 0.2626\n",
      "Epoch 23 Batch 2816 Loss 1.1705 Accuracy 0.2626\n",
      "Epoch 23 Batch 2880 Loss 1.1689 Accuracy 0.2628\n",
      "Epoch 23 Batch 2944 Loss 1.1695 Accuracy 0.2631\n",
      "Epoch 23 Batch 3008 Loss 1.1695 Accuracy 0.2632\n",
      "Epoch 23 Batch 3072 Loss 1.1692 Accuracy 0.2634\n",
      "Epoch 23 Batch 3136 Loss 1.1684 Accuracy 0.2636\n",
      "Epoch 23 Loss 1.1684 Accuracy 0.2636\n",
      "Time taken for 1 epoch: 40.020400047302246 secs\n",
      "\n",
      "Epoch 24 Batch 64 Loss 1.0831 Accuracy 0.2704\n",
      "Epoch 24 Batch 128 Loss 1.1106 Accuracy 0.2715\n",
      "Epoch 24 Batch 192 Loss 1.1257 Accuracy 0.2694\n",
      "Epoch 24 Batch 256 Loss 1.1392 Accuracy 0.2700\n",
      "Epoch 24 Batch 320 Loss 1.1450 Accuracy 0.2694\n",
      "Epoch 24 Batch 384 Loss 1.1437 Accuracy 0.2690\n",
      "Epoch 24 Batch 448 Loss 1.1539 Accuracy 0.2685\n",
      "Epoch 24 Batch 512 Loss 1.1491 Accuracy 0.2690\n",
      "Epoch 24 Batch 576 Loss 1.1421 Accuracy 0.2702\n",
      "Epoch 24 Batch 640 Loss 1.1453 Accuracy 0.2691\n",
      "Epoch 24 Batch 704 Loss 1.1454 Accuracy 0.2681\n",
      "Epoch 24 Batch 768 Loss 1.1473 Accuracy 0.2684\n",
      "Epoch 24 Batch 832 Loss 1.1526 Accuracy 0.2674\n",
      "Epoch 24 Batch 896 Loss 1.1535 Accuracy 0.2674\n",
      "Epoch 24 Batch 960 Loss 1.1513 Accuracy 0.2678\n",
      "Epoch 24 Batch 1024 Loss 1.1489 Accuracy 0.2682\n",
      "Epoch 24 Batch 1088 Loss 1.1497 Accuracy 0.2674\n",
      "Epoch 24 Batch 1152 Loss 1.1510 Accuracy 0.2675\n",
      "Epoch 24 Batch 1216 Loss 1.1506 Accuracy 0.2677\n",
      "Epoch 24 Batch 1280 Loss 1.1495 Accuracy 0.2681\n",
      "Epoch 24 Batch 1344 Loss 1.1487 Accuracy 0.2682\n",
      "Epoch 24 Batch 1408 Loss 1.1493 Accuracy 0.2682\n",
      "Epoch 24 Batch 1472 Loss 1.1509 Accuracy 0.2679\n",
      "Epoch 24 Batch 1536 Loss 1.1519 Accuracy 0.2679\n",
      "Epoch 24 Batch 1600 Loss 1.1514 Accuracy 0.2678\n",
      "Epoch 24 Batch 1664 Loss 1.1516 Accuracy 0.2679\n",
      "Epoch 24 Batch 1728 Loss 1.1491 Accuracy 0.2679\n",
      "Epoch 24 Batch 1792 Loss 1.1464 Accuracy 0.2682\n",
      "Epoch 24 Batch 1856 Loss 1.1449 Accuracy 0.2685\n",
      "Epoch 24 Batch 1920 Loss 1.1447 Accuracy 0.2681\n",
      "Epoch 24 Batch 1984 Loss 1.1449 Accuracy 0.2680\n",
      "Epoch 24 Batch 2048 Loss 1.1446 Accuracy 0.2678\n",
      "Epoch 24 Batch 2112 Loss 1.1455 Accuracy 0.2678\n",
      "Epoch 24 Batch 2176 Loss 1.1445 Accuracy 0.2680\n",
      "Epoch 24 Batch 2240 Loss 1.1436 Accuracy 0.2679\n",
      "Epoch 24 Batch 2304 Loss 1.1435 Accuracy 0.2682\n",
      "Epoch 24 Batch 2368 Loss 1.1439 Accuracy 0.2685\n",
      "Epoch 24 Batch 2432 Loss 1.1435 Accuracy 0.2684\n",
      "Epoch 24 Batch 2496 Loss 1.1437 Accuracy 0.2680\n",
      "Epoch 24 Batch 2560 Loss 1.1430 Accuracy 0.2676\n",
      "Epoch 24 Batch 2624 Loss 1.1413 Accuracy 0.2676\n",
      "Epoch 24 Batch 2688 Loss 1.1422 Accuracy 0.2675\n",
      "Epoch 24 Batch 2752 Loss 1.1416 Accuracy 0.2676\n",
      "Epoch 24 Batch 2816 Loss 1.1416 Accuracy 0.2677\n",
      "Epoch 24 Batch 2880 Loss 1.1417 Accuracy 0.2676\n",
      "Epoch 24 Batch 2944 Loss 1.1408 Accuracy 0.2675\n",
      "Epoch 24 Batch 3008 Loss 1.1399 Accuracy 0.2674\n",
      "Epoch 24 Batch 3072 Loss 1.1393 Accuracy 0.2675\n",
      "Epoch 24 Batch 3136 Loss 1.1385 Accuracy 0.2677\n",
      "Epoch 24 Loss 1.1385 Accuracy 0.2677\n",
      "Time taken for 1 epoch: 39.02183985710144 secs\n",
      "\n",
      "Epoch 25 Batch 64 Loss 1.1121 Accuracy 0.2675\n",
      "Epoch 25 Batch 128 Loss 1.1205 Accuracy 0.2639\n",
      "Epoch 25 Batch 192 Loss 1.1091 Accuracy 0.2663\n",
      "Epoch 25 Batch 256 Loss 1.1157 Accuracy 0.2683\n",
      "Epoch 25 Batch 320 Loss 1.1106 Accuracy 0.2708\n",
      "Epoch 25 Batch 384 Loss 1.1148 Accuracy 0.2714\n",
      "Epoch 25 Batch 448 Loss 1.1132 Accuracy 0.2711\n",
      "Epoch 25 Batch 512 Loss 1.1104 Accuracy 0.2723\n",
      "Epoch 25 Batch 576 Loss 1.1124 Accuracy 0.2711\n",
      "Epoch 25 Batch 640 Loss 1.1151 Accuracy 0.2706\n",
      "Epoch 25 Batch 704 Loss 1.1182 Accuracy 0.2697\n",
      "Epoch 25 Batch 768 Loss 1.1190 Accuracy 0.2695\n",
      "Epoch 25 Batch 832 Loss 1.1221 Accuracy 0.2696\n",
      "Epoch 25 Batch 896 Loss 1.1202 Accuracy 0.2691\n",
      "Epoch 25 Batch 960 Loss 1.1225 Accuracy 0.2692\n",
      "Epoch 25 Batch 1024 Loss 1.1191 Accuracy 0.2692\n",
      "Epoch 25 Batch 1088 Loss 1.1194 Accuracy 0.2692\n",
      "Epoch 25 Batch 1152 Loss 1.1176 Accuracy 0.2691\n",
      "Epoch 25 Batch 1216 Loss 1.1171 Accuracy 0.2694\n",
      "Epoch 25 Batch 1280 Loss 1.1165 Accuracy 0.2693\n",
      "Epoch 25 Batch 1344 Loss 1.1160 Accuracy 0.2697\n",
      "Epoch 25 Batch 1408 Loss 1.1167 Accuracy 0.2700\n",
      "Epoch 25 Batch 1472 Loss 1.1183 Accuracy 0.2703\n",
      "Epoch 25 Batch 1536 Loss 1.1168 Accuracy 0.2704\n",
      "Epoch 25 Batch 1600 Loss 1.1162 Accuracy 0.2704\n",
      "Epoch 25 Batch 1664 Loss 1.1149 Accuracy 0.2703\n",
      "Epoch 25 Batch 1728 Loss 1.1139 Accuracy 0.2706\n",
      "Epoch 25 Batch 1792 Loss 1.1126 Accuracy 0.2704\n",
      "Epoch 25 Batch 1856 Loss 1.1117 Accuracy 0.2705\n",
      "Epoch 25 Batch 1920 Loss 1.1114 Accuracy 0.2705\n",
      "Epoch 25 Batch 1984 Loss 1.1121 Accuracy 0.2703\n",
      "Epoch 25 Batch 2048 Loss 1.1107 Accuracy 0.2702\n",
      "Epoch 25 Batch 2112 Loss 1.1111 Accuracy 0.2698\n",
      "Epoch 25 Batch 2176 Loss 1.1118 Accuracy 0.2701\n",
      "Epoch 25 Batch 2240 Loss 1.1115 Accuracy 0.2703\n",
      "Epoch 25 Batch 2304 Loss 1.1095 Accuracy 0.2705\n",
      "Epoch 25 Batch 2368 Loss 1.1099 Accuracy 0.2703\n",
      "Epoch 25 Batch 2432 Loss 1.1088 Accuracy 0.2703\n",
      "Epoch 25 Batch 2496 Loss 1.1075 Accuracy 0.2704\n",
      "Epoch 25 Batch 2560 Loss 1.1075 Accuracy 0.2704\n",
      "Epoch 25 Batch 2624 Loss 1.1076 Accuracy 0.2704\n",
      "Epoch 25 Batch 2688 Loss 1.1085 Accuracy 0.2702\n",
      "Epoch 25 Batch 2752 Loss 1.1077 Accuracy 0.2705\n",
      "Epoch 25 Batch 2816 Loss 1.1077 Accuracy 0.2709\n",
      "Epoch 25 Batch 2880 Loss 1.1089 Accuracy 0.2707\n",
      "Epoch 25 Batch 2944 Loss 1.1080 Accuracy 0.2707\n",
      "Epoch 25 Batch 3008 Loss 1.1079 Accuracy 0.2708\n",
      "Epoch 25 Batch 3072 Loss 1.1079 Accuracy 0.2709\n",
      "Epoch 25 Batch 3136 Loss 1.1079 Accuracy 0.2708\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
      "Epoch 25 Loss 1.1079 Accuracy 0.2708\n",
      "Time taken for 1 epoch: 36.86268615722656 secs\n",
      "\n",
      "Epoch 26 Batch 64 Loss 1.0943 Accuracy 0.2711\n",
      "Epoch 26 Batch 128 Loss 1.0881 Accuracy 0.2756\n",
      "Epoch 26 Batch 192 Loss 1.0828 Accuracy 0.2784\n",
      "Epoch 26 Batch 256 Loss 1.0932 Accuracy 0.2748\n",
      "Epoch 26 Batch 320 Loss 1.0974 Accuracy 0.2734\n",
      "Epoch 26 Batch 384 Loss 1.0919 Accuracy 0.2735\n",
      "Epoch 26 Batch 448 Loss 1.0924 Accuracy 0.2729\n",
      "Epoch 26 Batch 512 Loss 1.0900 Accuracy 0.2740\n",
      "Epoch 26 Batch 576 Loss 1.0927 Accuracy 0.2724\n",
      "Epoch 26 Batch 640 Loss 1.0879 Accuracy 0.2725\n",
      "Epoch 26 Batch 704 Loss 1.0898 Accuracy 0.2735\n",
      "Epoch 26 Batch 768 Loss 1.0923 Accuracy 0.2738\n",
      "Epoch 26 Batch 832 Loss 1.0923 Accuracy 0.2739\n",
      "Epoch 26 Batch 896 Loss 1.0948 Accuracy 0.2733\n",
      "Epoch 26 Batch 960 Loss 1.0935 Accuracy 0.2732\n",
      "Epoch 26 Batch 1024 Loss 1.0940 Accuracy 0.2734\n",
      "Epoch 26 Batch 1088 Loss 1.0968 Accuracy 0.2731\n",
      "Epoch 26 Batch 1152 Loss 1.0935 Accuracy 0.2731\n",
      "Epoch 26 Batch 1216 Loss 1.0905 Accuracy 0.2737\n",
      "Epoch 26 Batch 1280 Loss 1.0873 Accuracy 0.2740\n",
      "Epoch 26 Batch 1344 Loss 1.0876 Accuracy 0.2739\n",
      "Epoch 26 Batch 1408 Loss 1.0869 Accuracy 0.2739\n",
      "Epoch 26 Batch 1472 Loss 1.0872 Accuracy 0.2744\n",
      "Epoch 26 Batch 1536 Loss 1.0868 Accuracy 0.2742\n",
      "Epoch 26 Batch 1600 Loss 1.0846 Accuracy 0.2740\n",
      "Epoch 26 Batch 1664 Loss 1.0841 Accuracy 0.2741\n",
      "Epoch 26 Batch 1728 Loss 1.0832 Accuracy 0.2738\n",
      "Epoch 26 Batch 1792 Loss 1.0833 Accuracy 0.2739\n",
      "Epoch 26 Batch 1856 Loss 1.0836 Accuracy 0.2738\n",
      "Epoch 26 Batch 1920 Loss 1.0821 Accuracy 0.2740\n",
      "Epoch 26 Batch 1984 Loss 1.0828 Accuracy 0.2740\n",
      "Epoch 26 Batch 2048 Loss 1.0836 Accuracy 0.2741\n",
      "Epoch 26 Batch 2112 Loss 1.0829 Accuracy 0.2742\n",
      "Epoch 26 Batch 2176 Loss 1.0841 Accuracy 0.2746\n",
      "Epoch 26 Batch 2240 Loss 1.0830 Accuracy 0.2749\n",
      "Epoch 26 Batch 2304 Loss 1.0832 Accuracy 0.2746\n",
      "Epoch 26 Batch 2368 Loss 1.0851 Accuracy 0.2743\n",
      "Epoch 26 Batch 2432 Loss 1.0853 Accuracy 0.2746\n",
      "Epoch 26 Batch 2496 Loss 1.0843 Accuracy 0.2750\n",
      "Epoch 26 Batch 2560 Loss 1.0838 Accuracy 0.2748\n",
      "Epoch 26 Batch 2624 Loss 1.0844 Accuracy 0.2747\n",
      "Epoch 26 Batch 2688 Loss 1.0851 Accuracy 0.2751\n",
      "Epoch 26 Batch 2752 Loss 1.0852 Accuracy 0.2751\n",
      "Epoch 26 Batch 2816 Loss 1.0851 Accuracy 0.2748\n",
      "Epoch 26 Batch 2880 Loss 1.0835 Accuracy 0.2748\n",
      "Epoch 26 Batch 2944 Loss 1.0833 Accuracy 0.2746\n",
      "Epoch 26 Batch 3008 Loss 1.0833 Accuracy 0.2744\n",
      "Epoch 26 Batch 3072 Loss 1.0833 Accuracy 0.2745\n",
      "Epoch 26 Batch 3136 Loss 1.0832 Accuracy 0.2744\n",
      "Epoch 26 Loss 1.0832 Accuracy 0.2744\n",
      "Time taken for 1 epoch: 38.432491064071655 secs\n",
      "\n",
      "Epoch 27 Batch 64 Loss 1.0906 Accuracy 0.2782\n",
      "Epoch 27 Batch 128 Loss 1.0685 Accuracy 0.2806\n",
      "Epoch 27 Batch 192 Loss 1.0777 Accuracy 0.2780\n",
      "Epoch 27 Batch 256 Loss 1.0682 Accuracy 0.2777\n",
      "Epoch 27 Batch 320 Loss 1.0709 Accuracy 0.2774\n",
      "Epoch 27 Batch 384 Loss 1.0651 Accuracy 0.2768\n",
      "Epoch 27 Batch 448 Loss 1.0668 Accuracy 0.2772\n",
      "Epoch 27 Batch 512 Loss 1.0663 Accuracy 0.2767\n",
      "Epoch 27 Batch 576 Loss 1.0645 Accuracy 0.2769\n",
      "Epoch 27 Batch 640 Loss 1.0635 Accuracy 0.2775\n",
      "Epoch 27 Batch 704 Loss 1.0594 Accuracy 0.2789\n",
      "Epoch 27 Batch 768 Loss 1.0692 Accuracy 0.2781\n",
      "Epoch 27 Batch 832 Loss 1.0682 Accuracy 0.2777\n",
      "Epoch 27 Batch 896 Loss 1.0692 Accuracy 0.2777\n",
      "Epoch 27 Batch 960 Loss 1.0654 Accuracy 0.2783\n",
      "Epoch 27 Batch 1024 Loss 1.0666 Accuracy 0.2788\n",
      "Epoch 27 Batch 1088 Loss 1.0665 Accuracy 0.2787\n",
      "Epoch 27 Batch 1152 Loss 1.0686 Accuracy 0.2784\n",
      "Epoch 27 Batch 1216 Loss 1.0667 Accuracy 0.2781\n",
      "Epoch 27 Batch 1280 Loss 1.0679 Accuracy 0.2780\n",
      "Epoch 27 Batch 1344 Loss 1.0687 Accuracy 0.2776\n",
      "Epoch 27 Batch 1408 Loss 1.0664 Accuracy 0.2779\n",
      "Epoch 27 Batch 1472 Loss 1.0671 Accuracy 0.2776\n",
      "Epoch 27 Batch 1536 Loss 1.0663 Accuracy 0.2784\n",
      "Epoch 27 Batch 1600 Loss 1.0654 Accuracy 0.2783\n",
      "Epoch 27 Batch 1664 Loss 1.0645 Accuracy 0.2782\n",
      "Epoch 27 Batch 1728 Loss 1.0640 Accuracy 0.2781\n",
      "Epoch 27 Batch 1792 Loss 1.0637 Accuracy 0.2781\n",
      "Epoch 27 Batch 1856 Loss 1.0641 Accuracy 0.2780\n",
      "Epoch 27 Batch 1920 Loss 1.0626 Accuracy 0.2781\n",
      "Epoch 27 Batch 1984 Loss 1.0621 Accuracy 0.2779\n",
      "Epoch 27 Batch 2048 Loss 1.0618 Accuracy 0.2779\n",
      "Epoch 27 Batch 2112 Loss 1.0615 Accuracy 0.2779\n",
      "Epoch 27 Batch 2176 Loss 1.0598 Accuracy 0.2776\n",
      "Epoch 27 Batch 2240 Loss 1.0586 Accuracy 0.2779\n",
      "Epoch 27 Batch 2304 Loss 1.0584 Accuracy 0.2779\n",
      "Epoch 27 Batch 2368 Loss 1.0578 Accuracy 0.2782\n",
      "Epoch 27 Batch 2432 Loss 1.0565 Accuracy 0.2783\n",
      "Epoch 27 Batch 2496 Loss 1.0561 Accuracy 0.2784\n",
      "Epoch 27 Batch 2560 Loss 1.0558 Accuracy 0.2783\n",
      "Epoch 27 Batch 2624 Loss 1.0563 Accuracy 0.2781\n",
      "Epoch 27 Batch 2688 Loss 1.0561 Accuracy 0.2782\n",
      "Epoch 27 Batch 2752 Loss 1.0553 Accuracy 0.2782\n",
      "Epoch 27 Batch 2816 Loss 1.0552 Accuracy 0.2785\n",
      "Epoch 27 Batch 2880 Loss 1.0556 Accuracy 0.2785\n",
      "Epoch 27 Batch 2944 Loss 1.0550 Accuracy 0.2784\n",
      "Epoch 27 Batch 3008 Loss 1.0545 Accuracy 0.2783\n",
      "Epoch 27 Batch 3072 Loss 1.0550 Accuracy 0.2782\n",
      "Epoch 27 Batch 3136 Loss 1.0550 Accuracy 0.2781\n",
      "Epoch 27 Loss 1.0550 Accuracy 0.2781\n",
      "Time taken for 1 epoch: 36.882137060165405 secs\n",
      "\n",
      "Epoch 28 Batch 64 Loss 1.0132 Accuracy 0.2834\n",
      "Epoch 28 Batch 128 Loss 1.0075 Accuracy 0.2859\n",
      "Epoch 28 Batch 192 Loss 1.0286 Accuracy 0.2830\n",
      "Epoch 28 Batch 256 Loss 1.0255 Accuracy 0.2852\n",
      "Epoch 28 Batch 320 Loss 1.0279 Accuracy 0.2836\n",
      "Epoch 28 Batch 384 Loss 1.0284 Accuracy 0.2829\n",
      "Epoch 28 Batch 448 Loss 1.0332 Accuracy 0.2822\n",
      "Epoch 28 Batch 512 Loss 1.0344 Accuracy 0.2815\n",
      "Epoch 28 Batch 576 Loss 1.0343 Accuracy 0.2809\n",
      "Epoch 28 Batch 640 Loss 1.0379 Accuracy 0.2793\n",
      "Epoch 28 Batch 704 Loss 1.0390 Accuracy 0.2778\n",
      "Epoch 28 Batch 768 Loss 1.0389 Accuracy 0.2776\n",
      "Epoch 28 Batch 832 Loss 1.0417 Accuracy 0.2780\n",
      "Epoch 28 Batch 896 Loss 1.0428 Accuracy 0.2779\n",
      "Epoch 28 Batch 960 Loss 1.0431 Accuracy 0.2784\n",
      "Epoch 28 Batch 1024 Loss 1.0369 Accuracy 0.2792\n",
      "Epoch 28 Batch 1088 Loss 1.0361 Accuracy 0.2792\n",
      "Epoch 28 Batch 1152 Loss 1.0367 Accuracy 0.2794\n",
      "Epoch 28 Batch 1216 Loss 1.0368 Accuracy 0.2795\n",
      "Epoch 28 Batch 1280 Loss 1.0361 Accuracy 0.2797\n",
      "Epoch 28 Batch 1344 Loss 1.0351 Accuracy 0.2799\n",
      "Epoch 28 Batch 1408 Loss 1.0329 Accuracy 0.2802\n",
      "Epoch 28 Batch 1472 Loss 1.0324 Accuracy 0.2806\n",
      "Epoch 28 Batch 1536 Loss 1.0326 Accuracy 0.2804\n",
      "Epoch 28 Batch 1600 Loss 1.0318 Accuracy 0.2807\n",
      "Epoch 28 Batch 1664 Loss 1.0313 Accuracy 0.2813\n",
      "Epoch 28 Batch 1728 Loss 1.0308 Accuracy 0.2810\n",
      "Epoch 28 Batch 1792 Loss 1.0336 Accuracy 0.2810\n",
      "Epoch 28 Batch 1856 Loss 1.0329 Accuracy 0.2810\n",
      "Epoch 28 Batch 1920 Loss 1.0316 Accuracy 0.2809\n",
      "Epoch 28 Batch 1984 Loss 1.0312 Accuracy 0.2809\n",
      "Epoch 28 Batch 2048 Loss 1.0303 Accuracy 0.2811\n",
      "Epoch 28 Batch 2112 Loss 1.0303 Accuracy 0.2811\n",
      "Epoch 28 Batch 2176 Loss 1.0293 Accuracy 0.2813\n",
      "Epoch 28 Batch 2240 Loss 1.0284 Accuracy 0.2816\n",
      "Epoch 28 Batch 2304 Loss 1.0279 Accuracy 0.2820\n",
      "Epoch 28 Batch 2368 Loss 1.0268 Accuracy 0.2822\n",
      "Epoch 28 Batch 2432 Loss 1.0272 Accuracy 0.2820\n",
      "Epoch 28 Batch 2496 Loss 1.0274 Accuracy 0.2819\n",
      "Epoch 28 Batch 2560 Loss 1.0269 Accuracy 0.2818\n",
      "Epoch 28 Batch 2624 Loss 1.0273 Accuracy 0.2819\n",
      "Epoch 28 Batch 2688 Loss 1.0259 Accuracy 0.2819\n",
      "Epoch 28 Batch 2752 Loss 1.0262 Accuracy 0.2819\n",
      "Epoch 28 Batch 2816 Loss 1.0253 Accuracy 0.2820\n",
      "Epoch 28 Batch 2880 Loss 1.0263 Accuracy 0.2818\n",
      "Epoch 28 Batch 2944 Loss 1.0260 Accuracy 0.2821\n",
      "Epoch 28 Batch 3008 Loss 1.0250 Accuracy 0.2823\n",
      "Epoch 28 Batch 3072 Loss 1.0241 Accuracy 0.2823\n",
      "Epoch 28 Batch 3136 Loss 1.0243 Accuracy 0.2824\n",
      "Epoch 28 Loss 1.0243 Accuracy 0.2824\n",
      "Time taken for 1 epoch: 37.596805810928345 secs\n",
      "\n",
      "Epoch 29 Batch 64 Loss 0.9972 Accuracy 0.2811\n",
      "Epoch 29 Batch 128 Loss 1.0003 Accuracy 0.2861\n",
      "Epoch 29 Batch 192 Loss 0.9975 Accuracy 0.2847\n",
      "Epoch 29 Batch 256 Loss 0.9977 Accuracy 0.2867\n",
      "Epoch 29 Batch 320 Loss 1.0124 Accuracy 0.2853\n",
      "Epoch 29 Batch 384 Loss 1.0090 Accuracy 0.2857\n",
      "Epoch 29 Batch 448 Loss 1.0085 Accuracy 0.2856\n",
      "Epoch 29 Batch 512 Loss 1.0125 Accuracy 0.2873\n",
      "Epoch 29 Batch 576 Loss 1.0133 Accuracy 0.2863\n",
      "Epoch 29 Batch 640 Loss 1.0134 Accuracy 0.2853\n",
      "Epoch 29 Batch 704 Loss 1.0112 Accuracy 0.2859\n",
      "Epoch 29 Batch 768 Loss 1.0113 Accuracy 0.2863\n",
      "Epoch 29 Batch 832 Loss 1.0129 Accuracy 0.2868\n",
      "Epoch 29 Batch 896 Loss 1.0139 Accuracy 0.2861\n",
      "Epoch 29 Batch 960 Loss 1.0142 Accuracy 0.2859\n",
      "Epoch 29 Batch 1024 Loss 1.0151 Accuracy 0.2855\n",
      "Epoch 29 Batch 1088 Loss 1.0170 Accuracy 0.2860\n",
      "Epoch 29 Batch 1152 Loss 1.0179 Accuracy 0.2860\n",
      "Epoch 29 Batch 1216 Loss 1.0171 Accuracy 0.2855\n",
      "Epoch 29 Batch 1280 Loss 1.0155 Accuracy 0.2854\n",
      "Epoch 29 Batch 1344 Loss 1.0124 Accuracy 0.2851\n",
      "Epoch 29 Batch 1408 Loss 1.0123 Accuracy 0.2849\n",
      "Epoch 29 Batch 1472 Loss 1.0117 Accuracy 0.2851\n",
      "Epoch 29 Batch 1536 Loss 1.0101 Accuracy 0.2850\n",
      "Epoch 29 Batch 1600 Loss 1.0111 Accuracy 0.2846\n",
      "Epoch 29 Batch 1664 Loss 1.0095 Accuracy 0.2848\n",
      "Epoch 29 Batch 1728 Loss 1.0091 Accuracy 0.2844\n",
      "Epoch 29 Batch 1792 Loss 1.0097 Accuracy 0.2843\n",
      "Epoch 29 Batch 1856 Loss 1.0097 Accuracy 0.2839\n",
      "Epoch 29 Batch 1920 Loss 1.0116 Accuracy 0.2839\n",
      "Epoch 29 Batch 1984 Loss 1.0108 Accuracy 0.2833\n",
      "Epoch 29 Batch 2048 Loss 1.0099 Accuracy 0.2835\n",
      "Epoch 29 Batch 2112 Loss 1.0089 Accuracy 0.2838\n",
      "Epoch 29 Batch 2176 Loss 1.0096 Accuracy 0.2841\n",
      "Epoch 29 Batch 2240 Loss 1.0070 Accuracy 0.2845\n",
      "Epoch 29 Batch 2304 Loss 1.0059 Accuracy 0.2844\n",
      "Epoch 29 Batch 2368 Loss 1.0059 Accuracy 0.2848\n",
      "Epoch 29 Batch 2432 Loss 1.0048 Accuracy 0.2852\n",
      "Epoch 29 Batch 2496 Loss 1.0046 Accuracy 0.2854\n",
      "Epoch 29 Batch 2560 Loss 1.0039 Accuracy 0.2853\n",
      "Epoch 29 Batch 2624 Loss 1.0037 Accuracy 0.2854\n",
      "Epoch 29 Batch 2688 Loss 1.0032 Accuracy 0.2855\n",
      "Epoch 29 Batch 2752 Loss 1.0030 Accuracy 0.2855\n",
      "Epoch 29 Batch 2816 Loss 1.0021 Accuracy 0.2853\n",
      "Epoch 29 Batch 2880 Loss 1.0022 Accuracy 0.2854\n",
      "Epoch 29 Batch 2944 Loss 1.0016 Accuracy 0.2855\n",
      "Epoch 29 Batch 3008 Loss 1.0010 Accuracy 0.2854\n",
      "Epoch 29 Batch 3072 Loss 1.0006 Accuracy 0.2855\n",
      "Epoch 29 Batch 3136 Loss 1.0008 Accuracy 0.2856\n",
      "Epoch 29 Loss 1.0008 Accuracy 0.2856\n",
      "Time taken for 1 epoch: 37.43198609352112 secs\n",
      "\n",
      "Epoch 30 Batch 64 Loss 1.0255 Accuracy 0.2759\n",
      "Epoch 30 Batch 128 Loss 0.9852 Accuracy 0.2878\n",
      "Epoch 30 Batch 192 Loss 0.9722 Accuracy 0.2839\n",
      "Epoch 30 Batch 256 Loss 0.9629 Accuracy 0.2908\n",
      "Epoch 30 Batch 320 Loss 0.9651 Accuracy 0.2895\n",
      "Epoch 30 Batch 384 Loss 0.9726 Accuracy 0.2903\n",
      "Epoch 30 Batch 448 Loss 0.9707 Accuracy 0.2900\n",
      "Epoch 30 Batch 512 Loss 0.9748 Accuracy 0.2895\n",
      "Epoch 30 Batch 576 Loss 0.9740 Accuracy 0.2897\n",
      "Epoch 30 Batch 640 Loss 0.9697 Accuracy 0.2898\n",
      "Epoch 30 Batch 704 Loss 0.9700 Accuracy 0.2899\n",
      "Epoch 30 Batch 768 Loss 0.9685 Accuracy 0.2892\n",
      "Epoch 30 Batch 832 Loss 0.9653 Accuracy 0.2894\n",
      "Epoch 30 Batch 896 Loss 0.9688 Accuracy 0.2891\n",
      "Epoch 30 Batch 960 Loss 0.9692 Accuracy 0.2884\n",
      "Epoch 30 Batch 1024 Loss 0.9730 Accuracy 0.2878\n",
      "Epoch 30 Batch 1088 Loss 0.9747 Accuracy 0.2878\n",
      "Epoch 30 Batch 1152 Loss 0.9758 Accuracy 0.2879\n",
      "Epoch 30 Batch 1216 Loss 0.9789 Accuracy 0.2879\n",
      "Epoch 30 Batch 1280 Loss 0.9846 Accuracy 0.2874\n",
      "Epoch 30 Batch 1344 Loss 0.9836 Accuracy 0.2874\n",
      "Epoch 30 Batch 1408 Loss 0.9827 Accuracy 0.2877\n",
      "Epoch 30 Batch 1472 Loss 0.9831 Accuracy 0.2874\n",
      "Epoch 30 Batch 1536 Loss 0.9837 Accuracy 0.2870\n",
      "Epoch 30 Batch 1600 Loss 0.9825 Accuracy 0.2870\n",
      "Epoch 30 Batch 1664 Loss 0.9819 Accuracy 0.2866\n",
      "Epoch 30 Batch 1728 Loss 0.9823 Accuracy 0.2872\n",
      "Epoch 30 Batch 1792 Loss 0.9841 Accuracy 0.2869\n",
      "Epoch 30 Batch 1856 Loss 0.9843 Accuracy 0.2872\n",
      "Epoch 30 Batch 1920 Loss 0.9831 Accuracy 0.2867\n",
      "Epoch 30 Batch 1984 Loss 0.9831 Accuracy 0.2871\n",
      "Epoch 30 Batch 2048 Loss 0.9816 Accuracy 0.2870\n",
      "Epoch 30 Batch 2112 Loss 0.9813 Accuracy 0.2872\n",
      "Epoch 30 Batch 2176 Loss 0.9818 Accuracy 0.2874\n",
      "Epoch 30 Batch 2240 Loss 0.9817 Accuracy 0.2872\n",
      "Epoch 30 Batch 2304 Loss 0.9804 Accuracy 0.2874\n",
      "Epoch 30 Batch 2368 Loss 0.9811 Accuracy 0.2872\n",
      "Epoch 30 Batch 2432 Loss 0.9800 Accuracy 0.2872\n",
      "Epoch 30 Batch 2496 Loss 0.9790 Accuracy 0.2873\n",
      "Epoch 30 Batch 2560 Loss 0.9776 Accuracy 0.2875\n",
      "Epoch 30 Batch 2624 Loss 0.9791 Accuracy 0.2875\n",
      "Epoch 30 Batch 2688 Loss 0.9792 Accuracy 0.2877\n",
      "Epoch 30 Batch 2752 Loss 0.9803 Accuracy 0.2877\n",
      "Epoch 30 Batch 2816 Loss 0.9800 Accuracy 0.2878\n",
      "Epoch 30 Batch 2880 Loss 0.9799 Accuracy 0.2878\n",
      "Epoch 30 Batch 2944 Loss 0.9782 Accuracy 0.2879\n",
      "Epoch 30 Batch 3008 Loss 0.9792 Accuracy 0.2881\n",
      "Epoch 30 Batch 3072 Loss 0.9786 Accuracy 0.2882\n",
      "Epoch 30 Batch 3136 Loss 0.9772 Accuracy 0.2885\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
      "Epoch 30 Loss 0.9772 Accuracy 0.2885\n",
      "Time taken for 1 epoch: 37.222354888916016 secs\n",
      "\n",
      "Epoch 31 Batch 64 Loss 0.9729 Accuracy 0.2745\n",
      "Epoch 31 Batch 128 Loss 0.9628 Accuracy 0.2817\n",
      "Epoch 31 Batch 192 Loss 0.9445 Accuracy 0.2891\n",
      "Epoch 31 Batch 256 Loss 0.9526 Accuracy 0.2897\n",
      "Epoch 31 Batch 320 Loss 0.9538 Accuracy 0.2872\n",
      "Epoch 31 Batch 384 Loss 0.9524 Accuracy 0.2887\n",
      "Epoch 31 Batch 448 Loss 0.9618 Accuracy 0.2898\n",
      "Epoch 31 Batch 512 Loss 0.9716 Accuracy 0.2886\n",
      "Epoch 31 Batch 576 Loss 0.9636 Accuracy 0.2896\n",
      "Epoch 31 Batch 640 Loss 0.9600 Accuracy 0.2901\n",
      "Epoch 31 Batch 704 Loss 0.9578 Accuracy 0.2905\n",
      "Epoch 31 Batch 768 Loss 0.9570 Accuracy 0.2912\n",
      "Epoch 31 Batch 832 Loss 0.9566 Accuracy 0.2916\n",
      "Epoch 31 Batch 896 Loss 0.9566 Accuracy 0.2921\n",
      "Epoch 31 Batch 960 Loss 0.9586 Accuracy 0.2923\n",
      "Epoch 31 Batch 1024 Loss 0.9588 Accuracy 0.2921\n",
      "Epoch 31 Batch 1088 Loss 0.9564 Accuracy 0.2921\n",
      "Epoch 31 Batch 1152 Loss 0.9552 Accuracy 0.2915\n",
      "Epoch 31 Batch 1216 Loss 0.9562 Accuracy 0.2914\n",
      "Epoch 31 Batch 1280 Loss 0.9554 Accuracy 0.2916\n",
      "Epoch 31 Batch 1344 Loss 0.9546 Accuracy 0.2916\n",
      "Epoch 31 Batch 1408 Loss 0.9579 Accuracy 0.2916\n",
      "Epoch 31 Batch 1472 Loss 0.9593 Accuracy 0.2919\n",
      "Epoch 31 Batch 1536 Loss 0.9589 Accuracy 0.2918\n",
      "Epoch 31 Batch 1600 Loss 0.9594 Accuracy 0.2914\n",
      "Epoch 31 Batch 1664 Loss 0.9580 Accuracy 0.2910\n",
      "Epoch 31 Batch 1728 Loss 0.9556 Accuracy 0.2913\n",
      "Epoch 31 Batch 1792 Loss 0.9555 Accuracy 0.2912\n",
      "Epoch 31 Batch 1856 Loss 0.9559 Accuracy 0.2908\n",
      "Epoch 31 Batch 1920 Loss 0.9558 Accuracy 0.2914\n",
      "Epoch 31 Batch 1984 Loss 0.9543 Accuracy 0.2915\n",
      "Epoch 31 Batch 2048 Loss 0.9530 Accuracy 0.2915\n",
      "Epoch 31 Batch 2112 Loss 0.9523 Accuracy 0.2919\n",
      "Epoch 31 Batch 2176 Loss 0.9522 Accuracy 0.2918\n",
      "Epoch 31 Batch 2240 Loss 0.9533 Accuracy 0.2918\n",
      "Epoch 31 Batch 2304 Loss 0.9531 Accuracy 0.2919\n",
      "Epoch 31 Batch 2368 Loss 0.9522 Accuracy 0.2920\n",
      "Epoch 31 Batch 2432 Loss 0.9518 Accuracy 0.2919\n",
      "Epoch 31 Batch 2496 Loss 0.9510 Accuracy 0.2920\n",
      "Epoch 31 Batch 2560 Loss 0.9519 Accuracy 0.2918\n",
      "Epoch 31 Batch 2624 Loss 0.9527 Accuracy 0.2918\n",
      "Epoch 31 Batch 2688 Loss 0.9517 Accuracy 0.2920\n",
      "Epoch 31 Batch 2752 Loss 0.9526 Accuracy 0.2921\n",
      "Epoch 31 Batch 2816 Loss 0.9515 Accuracy 0.2924\n",
      "Epoch 31 Batch 2880 Loss 0.9519 Accuracy 0.2926\n",
      "Epoch 31 Batch 2944 Loss 0.9526 Accuracy 0.2925\n",
      "Epoch 31 Batch 3008 Loss 0.9523 Accuracy 0.2921\n",
      "Epoch 31 Batch 3072 Loss 0.9526 Accuracy 0.2923\n",
      "Epoch 31 Batch 3136 Loss 0.9522 Accuracy 0.2927\n",
      "Epoch 31 Loss 0.9522 Accuracy 0.2927\n",
      "Time taken for 1 epoch: 36.296852111816406 secs\n",
      "\n",
      "Epoch 32 Batch 64 Loss 0.9459 Accuracy 0.2889\n",
      "Epoch 32 Batch 128 Loss 0.9477 Accuracy 0.2909\n",
      "Epoch 32 Batch 192 Loss 0.9410 Accuracy 0.2916\n",
      "Epoch 32 Batch 256 Loss 0.9521 Accuracy 0.2908\n",
      "Epoch 32 Batch 320 Loss 0.9509 Accuracy 0.2914\n",
      "Epoch 32 Batch 384 Loss 0.9425 Accuracy 0.2924\n",
      "Epoch 32 Batch 448 Loss 0.9299 Accuracy 0.2930\n",
      "Epoch 32 Batch 512 Loss 0.9308 Accuracy 0.2925\n",
      "Epoch 32 Batch 576 Loss 0.9285 Accuracy 0.2926\n",
      "Epoch 32 Batch 640 Loss 0.9257 Accuracy 0.2926\n",
      "Epoch 32 Batch 704 Loss 0.9255 Accuracy 0.2942\n",
      "Epoch 32 Batch 768 Loss 0.9265 Accuracy 0.2944\n",
      "Epoch 32 Batch 832 Loss 0.9290 Accuracy 0.2946\n",
      "Epoch 32 Batch 896 Loss 0.9284 Accuracy 0.2940\n",
      "Epoch 32 Batch 960 Loss 0.9309 Accuracy 0.2945\n",
      "Epoch 32 Batch 1024 Loss 0.9346 Accuracy 0.2948\n",
      "Epoch 32 Batch 1088 Loss 0.9364 Accuracy 0.2945\n",
      "Epoch 32 Batch 1152 Loss 0.9360 Accuracy 0.2944\n",
      "Epoch 32 Batch 1216 Loss 0.9346 Accuracy 0.2947\n",
      "Epoch 32 Batch 1280 Loss 0.9365 Accuracy 0.2949\n",
      "Epoch 32 Batch 1344 Loss 0.9349 Accuracy 0.2950\n",
      "Epoch 32 Batch 1408 Loss 0.9353 Accuracy 0.2949\n",
      "Epoch 32 Batch 1472 Loss 0.9352 Accuracy 0.2946\n",
      "Epoch 32 Batch 1536 Loss 0.9345 Accuracy 0.2947\n",
      "Epoch 32 Batch 1600 Loss 0.9336 Accuracy 0.2949\n",
      "Epoch 32 Batch 1664 Loss 0.9333 Accuracy 0.2946\n",
      "Epoch 32 Batch 1728 Loss 0.9341 Accuracy 0.2943\n",
      "Epoch 32 Batch 1792 Loss 0.9337 Accuracy 0.2946\n",
      "Epoch 32 Batch 1856 Loss 0.9326 Accuracy 0.2946\n",
      "Epoch 32 Batch 1920 Loss 0.9333 Accuracy 0.2948\n",
      "Epoch 32 Batch 1984 Loss 0.9332 Accuracy 0.2948\n",
      "Epoch 32 Batch 2048 Loss 0.9335 Accuracy 0.2946\n",
      "Epoch 32 Batch 2112 Loss 0.9332 Accuracy 0.2946\n",
      "Epoch 32 Batch 2176 Loss 0.9336 Accuracy 0.2947\n",
      "Epoch 32 Batch 2240 Loss 0.9331 Accuracy 0.2948\n",
      "Epoch 32 Batch 2304 Loss 0.9334 Accuracy 0.2948\n",
      "Epoch 32 Batch 2368 Loss 0.9317 Accuracy 0.2948\n",
      "Epoch 32 Batch 2432 Loss 0.9313 Accuracy 0.2950\n",
      "Epoch 32 Batch 2496 Loss 0.9310 Accuracy 0.2950\n",
      "Epoch 32 Batch 2560 Loss 0.9309 Accuracy 0.2951\n",
      "Epoch 32 Batch 2624 Loss 0.9298 Accuracy 0.2951\n",
      "Epoch 32 Batch 2688 Loss 0.9282 Accuracy 0.2952\n",
      "Epoch 32 Batch 2752 Loss 0.9279 Accuracy 0.2953\n",
      "Epoch 32 Batch 2816 Loss 0.9286 Accuracy 0.2955\n",
      "Epoch 32 Batch 2880 Loss 0.9281 Accuracy 0.2955\n",
      "Epoch 32 Batch 2944 Loss 0.9270 Accuracy 0.2956\n",
      "Epoch 32 Batch 3008 Loss 0.9272 Accuracy 0.2958\n",
      "Epoch 32 Batch 3072 Loss 0.9262 Accuracy 0.2959\n",
      "Epoch 32 Batch 3136 Loss 0.9259 Accuracy 0.2960\n",
      "Epoch 32 Loss 0.9259 Accuracy 0.2960\n",
      "Time taken for 1 epoch: 39.071433305740356 secs\n",
      "\n",
      "Epoch 33 Batch 64 Loss 0.9339 Accuracy 0.2967\n",
      "Epoch 33 Batch 128 Loss 0.9035 Accuracy 0.2986\n",
      "Epoch 33 Batch 192 Loss 0.9185 Accuracy 0.3005\n",
      "Epoch 33 Batch 256 Loss 0.9331 Accuracy 0.2956\n",
      "Epoch 33 Batch 320 Loss 0.9363 Accuracy 0.2940\n",
      "Epoch 33 Batch 384 Loss 0.9442 Accuracy 0.2934\n",
      "Epoch 33 Batch 448 Loss 0.9360 Accuracy 0.2940\n",
      "Epoch 33 Batch 512 Loss 0.9311 Accuracy 0.2948\n",
      "Epoch 33 Batch 576 Loss 0.9280 Accuracy 0.2960\n",
      "Epoch 33 Batch 640 Loss 0.9262 Accuracy 0.2955\n",
      "Epoch 33 Batch 704 Loss 0.9221 Accuracy 0.2958\n",
      "Epoch 33 Batch 768 Loss 0.9231 Accuracy 0.2961\n",
      "Epoch 33 Batch 832 Loss 0.9258 Accuracy 0.2956\n",
      "Epoch 33 Batch 896 Loss 0.9248 Accuracy 0.2959\n",
      "Epoch 33 Batch 960 Loss 0.9280 Accuracy 0.2957\n",
      "Epoch 33 Batch 1024 Loss 0.9245 Accuracy 0.2965\n",
      "Epoch 33 Batch 1088 Loss 0.9215 Accuracy 0.2968\n",
      "Epoch 33 Batch 1152 Loss 0.9234 Accuracy 0.2967\n",
      "Epoch 33 Batch 1216 Loss 0.9214 Accuracy 0.2974\n",
      "Epoch 33 Batch 1280 Loss 0.9194 Accuracy 0.2979\n",
      "Epoch 33 Batch 1344 Loss 0.9177 Accuracy 0.2983\n",
      "Epoch 33 Batch 1408 Loss 0.9150 Accuracy 0.2984\n",
      "Epoch 33 Batch 1472 Loss 0.9164 Accuracy 0.2983\n",
      "Epoch 33 Batch 1536 Loss 0.9154 Accuracy 0.2987\n",
      "Epoch 33 Batch 1600 Loss 0.9143 Accuracy 0.2990\n",
      "Epoch 33 Batch 1664 Loss 0.9132 Accuracy 0.2989\n",
      "Epoch 33 Batch 1728 Loss 0.9143 Accuracy 0.2985\n",
      "Epoch 33 Batch 1792 Loss 0.9135 Accuracy 0.2985\n",
      "Epoch 33 Batch 1856 Loss 0.9133 Accuracy 0.2985\n",
      "Epoch 33 Batch 1920 Loss 0.9140 Accuracy 0.2982\n",
      "Epoch 33 Batch 1984 Loss 0.9136 Accuracy 0.2982\n",
      "Epoch 33 Batch 2048 Loss 0.9132 Accuracy 0.2982\n",
      "Epoch 33 Batch 2112 Loss 0.9126 Accuracy 0.2980\n",
      "Epoch 33 Batch 2176 Loss 0.9115 Accuracy 0.2982\n",
      "Epoch 33 Batch 2240 Loss 0.9103 Accuracy 0.2986\n",
      "Epoch 33 Batch 2304 Loss 0.9098 Accuracy 0.2985\n",
      "Epoch 33 Batch 2368 Loss 0.9092 Accuracy 0.2987\n",
      "Epoch 33 Batch 2432 Loss 0.9092 Accuracy 0.2985\n",
      "Epoch 33 Batch 2496 Loss 0.9098 Accuracy 0.2982\n",
      "Epoch 33 Batch 2560 Loss 0.9090 Accuracy 0.2981\n",
      "Epoch 33 Batch 2624 Loss 0.9079 Accuracy 0.2985\n",
      "Epoch 33 Batch 2688 Loss 0.9064 Accuracy 0.2989\n",
      "Epoch 33 Batch 2752 Loss 0.9070 Accuracy 0.2987\n",
      "Epoch 33 Batch 2816 Loss 0.9070 Accuracy 0.2988\n",
      "Epoch 33 Batch 2880 Loss 0.9066 Accuracy 0.2990\n",
      "Epoch 33 Batch 2944 Loss 0.9074 Accuracy 0.2988\n",
      "Epoch 33 Batch 3008 Loss 0.9065 Accuracy 0.2989\n",
      "Epoch 33 Batch 3072 Loss 0.9063 Accuracy 0.2988\n",
      "Epoch 33 Batch 3136 Loss 0.9071 Accuracy 0.2986\n",
      "Epoch 33 Loss 0.9071 Accuracy 0.2986\n",
      "Time taken for 1 epoch: 38.83112978935242 secs\n",
      "\n",
      "Epoch 34 Batch 64 Loss 0.8796 Accuracy 0.3107\n",
      "Epoch 34 Batch 128 Loss 0.8671 Accuracy 0.3046\n",
      "Epoch 34 Batch 192 Loss 0.8812 Accuracy 0.3051\n",
      "Epoch 34 Batch 256 Loss 0.8831 Accuracy 0.3030\n",
      "Epoch 34 Batch 320 Loss 0.8928 Accuracy 0.3016\n",
      "Epoch 34 Batch 384 Loss 0.8862 Accuracy 0.3034\n",
      "Epoch 34 Batch 448 Loss 0.9008 Accuracy 0.3017\n",
      "Epoch 34 Batch 512 Loss 0.8990 Accuracy 0.3017\n",
      "Epoch 34 Batch 576 Loss 0.8936 Accuracy 0.3022\n",
      "Epoch 34 Batch 640 Loss 0.8936 Accuracy 0.3030\n",
      "Epoch 34 Batch 704 Loss 0.8906 Accuracy 0.3032\n",
      "Epoch 34 Batch 768 Loss 0.8903 Accuracy 0.3029\n",
      "Epoch 34 Batch 832 Loss 0.8919 Accuracy 0.3022\n",
      "Epoch 34 Batch 896 Loss 0.8911 Accuracy 0.3020\n",
      "Epoch 34 Batch 960 Loss 0.8883 Accuracy 0.3023\n",
      "Epoch 34 Batch 1024 Loss 0.8899 Accuracy 0.3034\n",
      "Epoch 34 Batch 1088 Loss 0.8914 Accuracy 0.3030\n",
      "Epoch 34 Batch 1152 Loss 0.8881 Accuracy 0.3038\n",
      "Epoch 34 Batch 1216 Loss 0.8880 Accuracy 0.3034\n",
      "Epoch 34 Batch 1280 Loss 0.8876 Accuracy 0.3039\n",
      "Epoch 34 Batch 1344 Loss 0.8869 Accuracy 0.3043\n",
      "Epoch 34 Batch 1408 Loss 0.8849 Accuracy 0.3045\n",
      "Epoch 34 Batch 1472 Loss 0.8837 Accuracy 0.3049\n",
      "Epoch 34 Batch 1536 Loss 0.8834 Accuracy 0.3043\n",
      "Epoch 34 Batch 1600 Loss 0.8837 Accuracy 0.3043\n",
      "Epoch 34 Batch 1664 Loss 0.8824 Accuracy 0.3040\n",
      "Epoch 34 Batch 1728 Loss 0.8826 Accuracy 0.3039\n",
      "Epoch 34 Batch 1792 Loss 0.8840 Accuracy 0.3032\n",
      "Epoch 34 Batch 1856 Loss 0.8827 Accuracy 0.3036\n",
      "Epoch 34 Batch 1920 Loss 0.8838 Accuracy 0.3033\n",
      "Epoch 34 Batch 1984 Loss 0.8844 Accuracy 0.3032\n",
      "Epoch 34 Batch 2048 Loss 0.8833 Accuracy 0.3030\n",
      "Epoch 34 Batch 2112 Loss 0.8836 Accuracy 0.3028\n",
      "Epoch 34 Batch 2176 Loss 0.8828 Accuracy 0.3027\n",
      "Epoch 34 Batch 2240 Loss 0.8821 Accuracy 0.3029\n",
      "Epoch 34 Batch 2304 Loss 0.8818 Accuracy 0.3029\n",
      "Epoch 34 Batch 2368 Loss 0.8811 Accuracy 0.3028\n",
      "Epoch 34 Batch 2432 Loss 0.8803 Accuracy 0.3028\n",
      "Epoch 34 Batch 2496 Loss 0.8804 Accuracy 0.3029\n",
      "Epoch 34 Batch 2560 Loss 0.8802 Accuracy 0.3030\n",
      "Epoch 34 Batch 2624 Loss 0.8802 Accuracy 0.3026\n",
      "Epoch 34 Batch 2688 Loss 0.8801 Accuracy 0.3027\n",
      "Epoch 34 Batch 2752 Loss 0.8809 Accuracy 0.3027\n",
      "Epoch 34 Batch 2816 Loss 0.8815 Accuracy 0.3025\n",
      "Epoch 34 Batch 2880 Loss 0.8820 Accuracy 0.3024\n",
      "Epoch 34 Batch 2944 Loss 0.8811 Accuracy 0.3025\n",
      "Epoch 34 Batch 3008 Loss 0.8825 Accuracy 0.3024\n",
      "Epoch 34 Batch 3072 Loss 0.8822 Accuracy 0.3026\n",
      "Epoch 34 Batch 3136 Loss 0.8815 Accuracy 0.3027\n",
      "Epoch 34 Loss 0.8815 Accuracy 0.3027\n",
      "Time taken for 1 epoch: 38.45915603637695 secs\n",
      "\n",
      "Epoch 35 Batch 64 Loss 0.8304 Accuracy 0.3026\n",
      "Epoch 35 Batch 128 Loss 0.8472 Accuracy 0.3139\n",
      "Epoch 35 Batch 192 Loss 0.8604 Accuracy 0.3080\n",
      "Epoch 35 Batch 256 Loss 0.8682 Accuracy 0.3040\n",
      "Epoch 35 Batch 320 Loss 0.8791 Accuracy 0.3031\n",
      "Epoch 35 Batch 384 Loss 0.8843 Accuracy 0.3027\n",
      "Epoch 35 Batch 448 Loss 0.8869 Accuracy 0.3022\n",
      "Epoch 35 Batch 512 Loss 0.8818 Accuracy 0.3032\n",
      "Epoch 35 Batch 576 Loss 0.8774 Accuracy 0.3038\n",
      "Epoch 35 Batch 640 Loss 0.8757 Accuracy 0.3039\n",
      "Epoch 35 Batch 704 Loss 0.8757 Accuracy 0.3037\n",
      "Epoch 35 Batch 768 Loss 0.8784 Accuracy 0.3040\n",
      "Epoch 35 Batch 832 Loss 0.8755 Accuracy 0.3039\n",
      "Epoch 35 Batch 896 Loss 0.8735 Accuracy 0.3046\n",
      "Epoch 35 Batch 960 Loss 0.8713 Accuracy 0.3044\n",
      "Epoch 35 Batch 1024 Loss 0.8722 Accuracy 0.3041\n",
      "Epoch 35 Batch 1088 Loss 0.8710 Accuracy 0.3052\n",
      "Epoch 35 Batch 1152 Loss 0.8713 Accuracy 0.3050\n",
      "Epoch 35 Batch 1216 Loss 0.8708 Accuracy 0.3053\n",
      "Epoch 35 Batch 1280 Loss 0.8700 Accuracy 0.3052\n",
      "Epoch 35 Batch 1344 Loss 0.8740 Accuracy 0.3047\n",
      "Epoch 35 Batch 1408 Loss 0.8717 Accuracy 0.3046\n",
      "Epoch 35 Batch 1472 Loss 0.8715 Accuracy 0.3046\n",
      "Epoch 35 Batch 1536 Loss 0.8717 Accuracy 0.3044\n",
      "Epoch 35 Batch 1600 Loss 0.8727 Accuracy 0.3043\n",
      "Epoch 35 Batch 1664 Loss 0.8738 Accuracy 0.3039\n",
      "Epoch 35 Batch 1728 Loss 0.8734 Accuracy 0.3037\n",
      "Epoch 35 Batch 1792 Loss 0.8743 Accuracy 0.3040\n",
      "Epoch 35 Batch 1856 Loss 0.8724 Accuracy 0.3039\n",
      "Epoch 35 Batch 1920 Loss 0.8727 Accuracy 0.3043\n",
      "Epoch 35 Batch 1984 Loss 0.8734 Accuracy 0.3041\n",
      "Epoch 35 Batch 2048 Loss 0.8741 Accuracy 0.3041\n",
      "Epoch 35 Batch 2112 Loss 0.8733 Accuracy 0.3045\n",
      "Epoch 35 Batch 2176 Loss 0.8738 Accuracy 0.3044\n",
      "Epoch 35 Batch 2240 Loss 0.8731 Accuracy 0.3044\n",
      "Epoch 35 Batch 2304 Loss 0.8730 Accuracy 0.3043\n",
      "Epoch 35 Batch 2368 Loss 0.8730 Accuracy 0.3044\n",
      "Epoch 35 Batch 2432 Loss 0.8730 Accuracy 0.3044\n",
      "Epoch 35 Batch 2496 Loss 0.8726 Accuracy 0.3046\n",
      "Epoch 35 Batch 2560 Loss 0.8710 Accuracy 0.3048\n",
      "Epoch 35 Batch 2624 Loss 0.8706 Accuracy 0.3048\n",
      "Epoch 35 Batch 2688 Loss 0.8697 Accuracy 0.3046\n",
      "Epoch 35 Batch 2752 Loss 0.8681 Accuracy 0.3047\n",
      "Epoch 35 Batch 2816 Loss 0.8671 Accuracy 0.3049\n",
      "Epoch 35 Batch 2880 Loss 0.8678 Accuracy 0.3047\n",
      "Epoch 35 Batch 2944 Loss 0.8667 Accuracy 0.3048\n",
      "Epoch 35 Batch 3008 Loss 0.8668 Accuracy 0.3046\n",
      "Epoch 35 Batch 3072 Loss 0.8672 Accuracy 0.3046\n",
      "Epoch 35 Batch 3136 Loss 0.8673 Accuracy 0.3045\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
      "Epoch 35 Loss 0.8673 Accuracy 0.3045\n",
      "Time taken for 1 epoch: 36.80699276924133 secs\n",
      "\n",
      "Epoch 36 Batch 64 Loss 0.8133 Accuracy 0.3057\n",
      "Epoch 36 Batch 128 Loss 0.8455 Accuracy 0.3017\n",
      "Epoch 36 Batch 192 Loss 0.8450 Accuracy 0.3041\n",
      "Epoch 36 Batch 256 Loss 0.8456 Accuracy 0.3027\n",
      "Epoch 36 Batch 320 Loss 0.8402 Accuracy 0.3064\n",
      "Epoch 36 Batch 384 Loss 0.8414 Accuracy 0.3053\n",
      "Epoch 36 Batch 448 Loss 0.8513 Accuracy 0.3051\n",
      "Epoch 36 Batch 512 Loss 0.8548 Accuracy 0.3067\n",
      "Epoch 36 Batch 576 Loss 0.8538 Accuracy 0.3067\n",
      "Epoch 36 Batch 640 Loss 0.8515 Accuracy 0.3057\n",
      "Epoch 36 Batch 704 Loss 0.8465 Accuracy 0.3068\n",
      "Epoch 36 Batch 768 Loss 0.8479 Accuracy 0.3061\n",
      "Epoch 36 Batch 832 Loss 0.8478 Accuracy 0.3066\n",
      "Epoch 36 Batch 896 Loss 0.8467 Accuracy 0.3059\n",
      "Epoch 36 Batch 960 Loss 0.8464 Accuracy 0.3053\n",
      "Epoch 36 Batch 1024 Loss 0.8444 Accuracy 0.3070\n",
      "Epoch 36 Batch 1088 Loss 0.8413 Accuracy 0.3076\n",
      "Epoch 36 Batch 1152 Loss 0.8362 Accuracy 0.3081\n",
      "Epoch 36 Batch 1216 Loss 0.8359 Accuracy 0.3075\n",
      "Epoch 36 Batch 1280 Loss 0.8407 Accuracy 0.3070\n",
      "Epoch 36 Batch 1344 Loss 0.8422 Accuracy 0.3075\n",
      "Epoch 36 Batch 1408 Loss 0.8416 Accuracy 0.3077\n",
      "Epoch 36 Batch 1472 Loss 0.8397 Accuracy 0.3085\n",
      "Epoch 36 Batch 1536 Loss 0.8400 Accuracy 0.3087\n",
      "Epoch 36 Batch 1600 Loss 0.8422 Accuracy 0.3090\n",
      "Epoch 36 Batch 1664 Loss 0.8432 Accuracy 0.3086\n",
      "Epoch 36 Batch 1728 Loss 0.8443 Accuracy 0.3091\n",
      "Epoch 36 Batch 1792 Loss 0.8441 Accuracy 0.3091\n",
      "Epoch 36 Batch 1856 Loss 0.8454 Accuracy 0.3087\n",
      "Epoch 36 Batch 1920 Loss 0.8468 Accuracy 0.3085\n",
      "Epoch 36 Batch 1984 Loss 0.8475 Accuracy 0.3086\n",
      "Epoch 36 Batch 2048 Loss 0.8472 Accuracy 0.3085\n",
      "Epoch 36 Batch 2112 Loss 0.8484 Accuracy 0.3080\n",
      "Epoch 36 Batch 2176 Loss 0.8490 Accuracy 0.3080\n",
      "Epoch 36 Batch 2240 Loss 0.8491 Accuracy 0.3076\n",
      "Epoch 36 Batch 2304 Loss 0.8495 Accuracy 0.3079\n",
      "Epoch 36 Batch 2368 Loss 0.8492 Accuracy 0.3079\n",
      "Epoch 36 Batch 2432 Loss 0.8492 Accuracy 0.3080\n",
      "Epoch 36 Batch 2496 Loss 0.8480 Accuracy 0.3079\n",
      "Epoch 36 Batch 2560 Loss 0.8473 Accuracy 0.3082\n",
      "Epoch 36 Batch 2624 Loss 0.8473 Accuracy 0.3084\n",
      "Epoch 36 Batch 2688 Loss 0.8465 Accuracy 0.3081\n",
      "Epoch 36 Batch 2752 Loss 0.8459 Accuracy 0.3081\n",
      "Epoch 36 Batch 2816 Loss 0.8456 Accuracy 0.3079\n",
      "Epoch 36 Batch 2880 Loss 0.8449 Accuracy 0.3077\n",
      "Epoch 36 Batch 2944 Loss 0.8449 Accuracy 0.3080\n",
      "Epoch 36 Batch 3008 Loss 0.8454 Accuracy 0.3081\n",
      "Epoch 36 Batch 3072 Loss 0.8442 Accuracy 0.3082\n",
      "Epoch 36 Batch 3136 Loss 0.8441 Accuracy 0.3080\n",
      "Epoch 36 Loss 0.8441 Accuracy 0.3080\n",
      "Time taken for 1 epoch: 37.88949990272522 secs\n",
      "\n",
      "Epoch 37 Batch 64 Loss 0.8529 Accuracy 0.3089\n",
      "Epoch 37 Batch 128 Loss 0.8394 Accuracy 0.3154\n",
      "Epoch 37 Batch 192 Loss 0.8486 Accuracy 0.3139\n",
      "Epoch 37 Batch 256 Loss 0.8460 Accuracy 0.3126\n",
      "Epoch 37 Batch 320 Loss 0.8384 Accuracy 0.3101\n",
      "Epoch 37 Batch 384 Loss 0.8452 Accuracy 0.3066\n",
      "Epoch 37 Batch 448 Loss 0.8466 Accuracy 0.3051\n",
      "Epoch 37 Batch 512 Loss 0.8436 Accuracy 0.3060\n",
      "Epoch 37 Batch 576 Loss 0.8489 Accuracy 0.3055\n",
      "Epoch 37 Batch 640 Loss 0.8443 Accuracy 0.3049\n",
      "Epoch 37 Batch 704 Loss 0.8408 Accuracy 0.3058\n",
      "Epoch 37 Batch 768 Loss 0.8388 Accuracy 0.3054\n",
      "Epoch 37 Batch 832 Loss 0.8387 Accuracy 0.3061\n",
      "Epoch 37 Batch 896 Loss 0.8399 Accuracy 0.3057\n",
      "Epoch 37 Batch 960 Loss 0.8376 Accuracy 0.3061\n",
      "Epoch 37 Batch 1024 Loss 0.8401 Accuracy 0.3058\n",
      "Epoch 37 Batch 1088 Loss 0.8382 Accuracy 0.3060\n",
      "Epoch 37 Batch 1152 Loss 0.8384 Accuracy 0.3062\n",
      "Epoch 37 Batch 1216 Loss 0.8381 Accuracy 0.3062\n",
      "Epoch 37 Batch 1280 Loss 0.8384 Accuracy 0.3067\n",
      "Epoch 37 Batch 1344 Loss 0.8345 Accuracy 0.3069\n",
      "Epoch 37 Batch 1408 Loss 0.8355 Accuracy 0.3071\n",
      "Epoch 37 Batch 1472 Loss 0.8371 Accuracy 0.3071\n",
      "Epoch 37 Batch 1536 Loss 0.8380 Accuracy 0.3070\n",
      "Epoch 37 Batch 1600 Loss 0.8366 Accuracy 0.3072\n",
      "Epoch 37 Batch 1664 Loss 0.8340 Accuracy 0.3078\n",
      "Epoch 37 Batch 1728 Loss 0.8331 Accuracy 0.3080\n",
      "Epoch 37 Batch 1792 Loss 0.8320 Accuracy 0.3083\n",
      "Epoch 37 Batch 1856 Loss 0.8313 Accuracy 0.3085\n",
      "Epoch 37 Batch 1920 Loss 0.8318 Accuracy 0.3085\n",
      "Epoch 37 Batch 1984 Loss 0.8300 Accuracy 0.3090\n",
      "Epoch 37 Batch 2048 Loss 0.8292 Accuracy 0.3093\n",
      "Epoch 37 Batch 2112 Loss 0.8288 Accuracy 0.3095\n",
      "Epoch 37 Batch 2176 Loss 0.8301 Accuracy 0.3094\n",
      "Epoch 37 Batch 2240 Loss 0.8299 Accuracy 0.3097\n",
      "Epoch 37 Batch 2304 Loss 0.8279 Accuracy 0.3099\n",
      "Epoch 37 Batch 2368 Loss 0.8284 Accuracy 0.3098\n",
      "Epoch 37 Batch 2432 Loss 0.8280 Accuracy 0.3098\n",
      "Epoch 37 Batch 2496 Loss 0.8277 Accuracy 0.3099\n",
      "Epoch 37 Batch 2560 Loss 0.8275 Accuracy 0.3099\n",
      "Epoch 37 Batch 2624 Loss 0.8266 Accuracy 0.3102\n",
      "Epoch 37 Batch 2688 Loss 0.8265 Accuracy 0.3101\n",
      "Epoch 37 Batch 2752 Loss 0.8264 Accuracy 0.3100\n",
      "Epoch 37 Batch 2816 Loss 0.8263 Accuracy 0.3098\n",
      "Epoch 37 Batch 2880 Loss 0.8261 Accuracy 0.3098\n",
      "Epoch 37 Batch 2944 Loss 0.8252 Accuracy 0.3100\n",
      "Epoch 37 Batch 3008 Loss 0.8243 Accuracy 0.3101\n",
      "Epoch 37 Batch 3072 Loss 0.8246 Accuracy 0.3100\n",
      "Epoch 37 Batch 3136 Loss 0.8257 Accuracy 0.3098\n",
      "Epoch 37 Loss 0.8257 Accuracy 0.3098\n",
      "Time taken for 1 epoch: 36.94664192199707 secs\n",
      "\n",
      "Epoch 38 Batch 64 Loss 0.8325 Accuracy 0.2976\n",
      "Epoch 38 Batch 128 Loss 0.8129 Accuracy 0.3066\n",
      "Epoch 38 Batch 192 Loss 0.8079 Accuracy 0.3103\n",
      "Epoch 38 Batch 256 Loss 0.8024 Accuracy 0.3133\n",
      "Epoch 38 Batch 320 Loss 0.7977 Accuracy 0.3133\n",
      "Epoch 38 Batch 384 Loss 0.8052 Accuracy 0.3125\n",
      "Epoch 38 Batch 448 Loss 0.8082 Accuracy 0.3139\n",
      "Epoch 38 Batch 512 Loss 0.8082 Accuracy 0.3142\n",
      "Epoch 38 Batch 576 Loss 0.8094 Accuracy 0.3127\n",
      "Epoch 38 Batch 640 Loss 0.8111 Accuracy 0.3135\n",
      "Epoch 38 Batch 704 Loss 0.8085 Accuracy 0.3125\n",
      "Epoch 38 Batch 768 Loss 0.8054 Accuracy 0.3131\n",
      "Epoch 38 Batch 832 Loss 0.8042 Accuracy 0.3121\n",
      "Epoch 38 Batch 896 Loss 0.8054 Accuracy 0.3121\n",
      "Epoch 38 Batch 960 Loss 0.8080 Accuracy 0.3114\n",
      "Epoch 38 Batch 1024 Loss 0.8079 Accuracy 0.3110\n",
      "Epoch 38 Batch 1088 Loss 0.8084 Accuracy 0.3106\n",
      "Epoch 38 Batch 1152 Loss 0.8104 Accuracy 0.3109\n",
      "Epoch 38 Batch 1216 Loss 0.8106 Accuracy 0.3109\n",
      "Epoch 38 Batch 1280 Loss 0.8092 Accuracy 0.3116\n",
      "Epoch 38 Batch 1344 Loss 0.8094 Accuracy 0.3115\n",
      "Epoch 38 Batch 1408 Loss 0.8107 Accuracy 0.3119\n",
      "Epoch 38 Batch 1472 Loss 0.8132 Accuracy 0.3113\n",
      "Epoch 38 Batch 1536 Loss 0.8143 Accuracy 0.3114\n",
      "Epoch 38 Batch 1600 Loss 0.8135 Accuracy 0.3114\n",
      "Epoch 38 Batch 1664 Loss 0.8123 Accuracy 0.3113\n",
      "Epoch 38 Batch 1728 Loss 0.8112 Accuracy 0.3115\n",
      "Epoch 38 Batch 1792 Loss 0.8100 Accuracy 0.3119\n",
      "Epoch 38 Batch 1856 Loss 0.8104 Accuracy 0.3119\n",
      "Epoch 38 Batch 1920 Loss 0.8095 Accuracy 0.3117\n",
      "Epoch 38 Batch 1984 Loss 0.8082 Accuracy 0.3118\n",
      "Epoch 38 Batch 2048 Loss 0.8084 Accuracy 0.3115\n",
      "Epoch 38 Batch 2112 Loss 0.8086 Accuracy 0.3114\n",
      "Epoch 38 Batch 2176 Loss 0.8080 Accuracy 0.3115\n",
      "Epoch 38 Batch 2240 Loss 0.8088 Accuracy 0.3117\n",
      "Epoch 38 Batch 2304 Loss 0.8079 Accuracy 0.3118\n",
      "Epoch 38 Batch 2368 Loss 0.8062 Accuracy 0.3123\n",
      "Epoch 38 Batch 2432 Loss 0.8062 Accuracy 0.3128\n",
      "Epoch 38 Batch 2496 Loss 0.8070 Accuracy 0.3128\n",
      "Epoch 38 Batch 2560 Loss 0.8067 Accuracy 0.3128\n",
      "Epoch 38 Batch 2624 Loss 0.8079 Accuracy 0.3126\n",
      "Epoch 38 Batch 2688 Loss 0.8080 Accuracy 0.3127\n",
      "Epoch 38 Batch 2752 Loss 0.8080 Accuracy 0.3126\n",
      "Epoch 38 Batch 2816 Loss 0.8064 Accuracy 0.3125\n",
      "Epoch 38 Batch 2880 Loss 0.8058 Accuracy 0.3127\n",
      "Epoch 38 Batch 2944 Loss 0.8055 Accuracy 0.3131\n",
      "Epoch 38 Batch 3008 Loss 0.8051 Accuracy 0.3131\n",
      "Epoch 38 Batch 3072 Loss 0.8060 Accuracy 0.3130\n",
      "Epoch 38 Batch 3136 Loss 0.8063 Accuracy 0.3129\n",
      "Epoch 38 Loss 0.8063 Accuracy 0.3129\n",
      "Time taken for 1 epoch: 36.44168496131897 secs\n",
      "\n",
      "Epoch 39 Batch 64 Loss 0.8614 Accuracy 0.3079\n",
      "Epoch 39 Batch 128 Loss 0.8136 Accuracy 0.3140\n",
      "Epoch 39 Batch 192 Loss 0.8045 Accuracy 0.3139\n",
      "Epoch 39 Batch 256 Loss 0.8125 Accuracy 0.3116\n",
      "Epoch 39 Batch 320 Loss 0.7985 Accuracy 0.3137\n",
      "Epoch 39 Batch 384 Loss 0.8073 Accuracy 0.3116\n",
      "Epoch 39 Batch 448 Loss 0.8055 Accuracy 0.3115\n",
      "Epoch 39 Batch 512 Loss 0.8067 Accuracy 0.3113\n",
      "Epoch 39 Batch 576 Loss 0.7991 Accuracy 0.3138\n",
      "Epoch 39 Batch 640 Loss 0.8009 Accuracy 0.3134\n",
      "Epoch 39 Batch 704 Loss 0.8028 Accuracy 0.3126\n",
      "Epoch 39 Batch 768 Loss 0.8010 Accuracy 0.3144\n",
      "Epoch 39 Batch 832 Loss 0.8004 Accuracy 0.3139\n",
      "Epoch 39 Batch 896 Loss 0.7968 Accuracy 0.3144\n",
      "Epoch 39 Batch 960 Loss 0.7949 Accuracy 0.3137\n",
      "Epoch 39 Batch 1024 Loss 0.7953 Accuracy 0.3130\n",
      "Epoch 39 Batch 1088 Loss 0.8001 Accuracy 0.3127\n",
      "Epoch 39 Batch 1152 Loss 0.7990 Accuracy 0.3124\n",
      "Epoch 39 Batch 1216 Loss 0.7993 Accuracy 0.3128\n",
      "Epoch 39 Batch 1280 Loss 0.7963 Accuracy 0.3132\n",
      "Epoch 39 Batch 1344 Loss 0.7962 Accuracy 0.3127\n",
      "Epoch 39 Batch 1408 Loss 0.7960 Accuracy 0.3126\n",
      "Epoch 39 Batch 1472 Loss 0.7963 Accuracy 0.3127\n",
      "Epoch 39 Batch 1536 Loss 0.7982 Accuracy 0.3126\n",
      "Epoch 39 Batch 1600 Loss 0.7976 Accuracy 0.3131\n",
      "Epoch 39 Batch 1664 Loss 0.7975 Accuracy 0.3136\n",
      "Epoch 39 Batch 1728 Loss 0.7992 Accuracy 0.3132\n",
      "Epoch 39 Batch 1792 Loss 0.8007 Accuracy 0.3130\n",
      "Epoch 39 Batch 1856 Loss 0.8006 Accuracy 0.3128\n",
      "Epoch 39 Batch 1920 Loss 0.8020 Accuracy 0.3129\n",
      "Epoch 39 Batch 1984 Loss 0.8003 Accuracy 0.3130\n",
      "Epoch 39 Batch 2048 Loss 0.7984 Accuracy 0.3129\n",
      "Epoch 39 Batch 2112 Loss 0.7975 Accuracy 0.3132\n",
      "Epoch 39 Batch 2176 Loss 0.7968 Accuracy 0.3138\n",
      "Epoch 39 Batch 2240 Loss 0.7959 Accuracy 0.3140\n",
      "Epoch 39 Batch 2304 Loss 0.7964 Accuracy 0.3139\n",
      "Epoch 39 Batch 2368 Loss 0.7952 Accuracy 0.3141\n",
      "Epoch 39 Batch 2432 Loss 0.7933 Accuracy 0.3147\n",
      "Epoch 39 Batch 2496 Loss 0.7929 Accuracy 0.3149\n",
      "Epoch 39 Batch 2560 Loss 0.7923 Accuracy 0.3148\n",
      "Epoch 39 Batch 2624 Loss 0.7923 Accuracy 0.3144\n",
      "Epoch 39 Batch 2688 Loss 0.7931 Accuracy 0.3145\n",
      "Epoch 39 Batch 2752 Loss 0.7935 Accuracy 0.3144\n",
      "Epoch 39 Batch 2816 Loss 0.7940 Accuracy 0.3144\n",
      "Epoch 39 Batch 2880 Loss 0.7942 Accuracy 0.3146\n",
      "Epoch 39 Batch 2944 Loss 0.7952 Accuracy 0.3146\n",
      "Epoch 39 Batch 3008 Loss 0.7939 Accuracy 0.3145\n",
      "Epoch 39 Batch 3072 Loss 0.7930 Accuracy 0.3147\n",
      "Epoch 39 Batch 3136 Loss 0.7924 Accuracy 0.3145\n",
      "Epoch 39 Loss 0.7924 Accuracy 0.3145\n",
      "Time taken for 1 epoch: 37.29347801208496 secs\n",
      "\n",
      "Epoch 40 Batch 64 Loss 0.7448 Accuracy 0.3265\n",
      "Epoch 40 Batch 128 Loss 0.7872 Accuracy 0.3300\n",
      "Epoch 40 Batch 192 Loss 0.7941 Accuracy 0.3277\n",
      "Epoch 40 Batch 256 Loss 0.7863 Accuracy 0.3239\n",
      "Epoch 40 Batch 320 Loss 0.7783 Accuracy 0.3215\n",
      "Epoch 40 Batch 384 Loss 0.7849 Accuracy 0.3193\n",
      "Epoch 40 Batch 448 Loss 0.7843 Accuracy 0.3179\n",
      "Epoch 40 Batch 512 Loss 0.7812 Accuracy 0.3178\n",
      "Epoch 40 Batch 576 Loss 0.7739 Accuracy 0.3185\n",
      "Epoch 40 Batch 640 Loss 0.7765 Accuracy 0.3191\n",
      "Epoch 40 Batch 704 Loss 0.7712 Accuracy 0.3191\n",
      "Epoch 40 Batch 768 Loss 0.7687 Accuracy 0.3187\n",
      "Epoch 40 Batch 832 Loss 0.7715 Accuracy 0.3183\n",
      "Epoch 40 Batch 896 Loss 0.7724 Accuracy 0.3187\n",
      "Epoch 40 Batch 960 Loss 0.7730 Accuracy 0.3196\n",
      "Epoch 40 Batch 1024 Loss 0.7779 Accuracy 0.3186\n",
      "Epoch 40 Batch 1088 Loss 0.7768 Accuracy 0.3183\n",
      "Epoch 40 Batch 1152 Loss 0.7751 Accuracy 0.3177\n",
      "Epoch 40 Batch 1216 Loss 0.7753 Accuracy 0.3175\n",
      "Epoch 40 Batch 1280 Loss 0.7773 Accuracy 0.3170\n",
      "Epoch 40 Batch 1344 Loss 0.7742 Accuracy 0.3170\n",
      "Epoch 40 Batch 1408 Loss 0.7734 Accuracy 0.3169\n",
      "Epoch 40 Batch 1472 Loss 0.7706 Accuracy 0.3169\n",
      "Epoch 40 Batch 1536 Loss 0.7715 Accuracy 0.3173\n",
      "Epoch 40 Batch 1600 Loss 0.7702 Accuracy 0.3179\n",
      "Epoch 40 Batch 1664 Loss 0.7712 Accuracy 0.3177\n",
      "Epoch 40 Batch 1728 Loss 0.7730 Accuracy 0.3175\n",
      "Epoch 40 Batch 1792 Loss 0.7744 Accuracy 0.3175\n",
      "Epoch 40 Batch 1856 Loss 0.7757 Accuracy 0.3174\n",
      "Epoch 40 Batch 1920 Loss 0.7741 Accuracy 0.3172\n",
      "Epoch 40 Batch 1984 Loss 0.7755 Accuracy 0.3170\n",
      "Epoch 40 Batch 2048 Loss 0.7759 Accuracy 0.3170\n",
      "Epoch 40 Batch 2112 Loss 0.7770 Accuracy 0.3172\n",
      "Epoch 40 Batch 2176 Loss 0.7773 Accuracy 0.3169\n",
      "Epoch 40 Batch 2240 Loss 0.7758 Accuracy 0.3171\n",
      "Epoch 40 Batch 2304 Loss 0.7747 Accuracy 0.3167\n",
      "Epoch 40 Batch 2368 Loss 0.7745 Accuracy 0.3171\n",
      "Epoch 40 Batch 2432 Loss 0.7745 Accuracy 0.3168\n",
      "Epoch 40 Batch 2496 Loss 0.7734 Accuracy 0.3170\n",
      "Epoch 40 Batch 2560 Loss 0.7745 Accuracy 0.3170\n",
      "Epoch 40 Batch 2624 Loss 0.7747 Accuracy 0.3169\n",
      "Epoch 40 Batch 2688 Loss 0.7746 Accuracy 0.3168\n",
      "Epoch 40 Batch 2752 Loss 0.7740 Accuracy 0.3168\n",
      "Epoch 40 Batch 2816 Loss 0.7750 Accuracy 0.3167\n",
      "Epoch 40 Batch 2880 Loss 0.7754 Accuracy 0.3168\n",
      "Epoch 40 Batch 2944 Loss 0.7747 Accuracy 0.3168\n",
      "Epoch 40 Batch 3008 Loss 0.7752 Accuracy 0.3169\n",
      "Epoch 40 Batch 3072 Loss 0.7750 Accuracy 0.3169\n",
      "Epoch 40 Batch 3136 Loss 0.7745 Accuracy 0.3170\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
      "Epoch 40 Loss 0.7745 Accuracy 0.3170\n",
      "Time taken for 1 epoch: 38.48408007621765 secs\n",
      "\n",
      "Epoch 41 Batch 64 Loss 0.8285 Accuracy 0.3110\n",
      "Epoch 41 Batch 128 Loss 0.8002 Accuracy 0.3207\n",
      "Epoch 41 Batch 192 Loss 0.7977 Accuracy 0.3188\n",
      "Epoch 41 Batch 256 Loss 0.7860 Accuracy 0.3191\n",
      "Epoch 41 Batch 320 Loss 0.7806 Accuracy 0.3195\n",
      "Epoch 41 Batch 384 Loss 0.7758 Accuracy 0.3167\n",
      "Epoch 41 Batch 448 Loss 0.7682 Accuracy 0.3187\n",
      "Epoch 41 Batch 512 Loss 0.7736 Accuracy 0.3192\n",
      "Epoch 41 Batch 576 Loss 0.7728 Accuracy 0.3187\n",
      "Epoch 41 Batch 640 Loss 0.7732 Accuracy 0.3178\n",
      "Epoch 41 Batch 704 Loss 0.7691 Accuracy 0.3177\n",
      "Epoch 41 Batch 768 Loss 0.7651 Accuracy 0.3190\n",
      "Epoch 41 Batch 832 Loss 0.7678 Accuracy 0.3190\n",
      "Epoch 41 Batch 896 Loss 0.7669 Accuracy 0.3186\n",
      "Epoch 41 Batch 960 Loss 0.7652 Accuracy 0.3187\n",
      "Epoch 41 Batch 1024 Loss 0.7641 Accuracy 0.3185\n",
      "Epoch 41 Batch 1088 Loss 0.7639 Accuracy 0.3184\n",
      "Epoch 41 Batch 1152 Loss 0.7594 Accuracy 0.3188\n",
      "Epoch 41 Batch 1216 Loss 0.7575 Accuracy 0.3198\n",
      "Epoch 41 Batch 1280 Loss 0.7567 Accuracy 0.3197\n",
      "Epoch 41 Batch 1344 Loss 0.7551 Accuracy 0.3194\n",
      "Epoch 41 Batch 1408 Loss 0.7560 Accuracy 0.3192\n",
      "Epoch 41 Batch 1472 Loss 0.7550 Accuracy 0.3192\n",
      "Epoch 41 Batch 1536 Loss 0.7556 Accuracy 0.3191\n",
      "Epoch 41 Batch 1600 Loss 0.7548 Accuracy 0.3196\n",
      "Epoch 41 Batch 1664 Loss 0.7552 Accuracy 0.3195\n",
      "Epoch 41 Batch 1728 Loss 0.7555 Accuracy 0.3192\n",
      "Epoch 41 Batch 1792 Loss 0.7554 Accuracy 0.3195\n",
      "Epoch 41 Batch 1856 Loss 0.7548 Accuracy 0.3196\n",
      "Epoch 41 Batch 1920 Loss 0.7542 Accuracy 0.3194\n",
      "Epoch 41 Batch 1984 Loss 0.7540 Accuracy 0.3196\n",
      "Epoch 41 Batch 2048 Loss 0.7540 Accuracy 0.3194\n",
      "Epoch 41 Batch 2112 Loss 0.7553 Accuracy 0.3195\n",
      "Epoch 41 Batch 2176 Loss 0.7542 Accuracy 0.3196\n",
      "Epoch 41 Batch 2240 Loss 0.7532 Accuracy 0.3198\n",
      "Epoch 41 Batch 2304 Loss 0.7539 Accuracy 0.3198\n",
      "Epoch 41 Batch 2368 Loss 0.7536 Accuracy 0.3199\n",
      "Epoch 41 Batch 2432 Loss 0.7526 Accuracy 0.3199\n",
      "Epoch 41 Batch 2496 Loss 0.7518 Accuracy 0.3200\n",
      "Epoch 41 Batch 2560 Loss 0.7517 Accuracy 0.3202\n",
      "Epoch 41 Batch 2624 Loss 0.7534 Accuracy 0.3198\n",
      "Epoch 41 Batch 2688 Loss 0.7528 Accuracy 0.3199\n",
      "Epoch 41 Batch 2752 Loss 0.7529 Accuracy 0.3204\n",
      "Epoch 41 Batch 2816 Loss 0.7543 Accuracy 0.3204\n",
      "Epoch 41 Batch 2880 Loss 0.7545 Accuracy 0.3201\n",
      "Epoch 41 Batch 2944 Loss 0.7541 Accuracy 0.3202\n",
      "Epoch 41 Batch 3008 Loss 0.7528 Accuracy 0.3204\n",
      "Epoch 41 Batch 3072 Loss 0.7526 Accuracy 0.3203\n",
      "Epoch 41 Batch 3136 Loss 0.7525 Accuracy 0.3205\n",
      "Epoch 41 Loss 0.7525 Accuracy 0.3205\n",
      "Time taken for 1 epoch: 37.14859890937805 secs\n",
      "\n",
      "Epoch 42 Batch 64 Loss 0.7287 Accuracy 0.3198\n",
      "Epoch 42 Batch 128 Loss 0.7424 Accuracy 0.3202\n",
      "Epoch 42 Batch 192 Loss 0.7503 Accuracy 0.3219\n",
      "Epoch 42 Batch 256 Loss 0.7458 Accuracy 0.3218\n",
      "Epoch 42 Batch 320 Loss 0.7539 Accuracy 0.3188\n",
      "Epoch 42 Batch 384 Loss 0.7546 Accuracy 0.3181\n",
      "Epoch 42 Batch 448 Loss 0.7477 Accuracy 0.3202\n",
      "Epoch 42 Batch 512 Loss 0.7466 Accuracy 0.3219\n",
      "Epoch 42 Batch 576 Loss 0.7458 Accuracy 0.3216\n",
      "Epoch 42 Batch 640 Loss 0.7555 Accuracy 0.3213\n",
      "Epoch 42 Batch 704 Loss 0.7532 Accuracy 0.3214\n",
      "Epoch 42 Batch 768 Loss 0.7524 Accuracy 0.3211\n",
      "Epoch 42 Batch 832 Loss 0.7536 Accuracy 0.3210\n",
      "Epoch 42 Batch 896 Loss 0.7535 Accuracy 0.3197\n",
      "Epoch 42 Batch 960 Loss 0.7521 Accuracy 0.3187\n",
      "Epoch 42 Batch 1024 Loss 0.7545 Accuracy 0.3177\n",
      "Epoch 42 Batch 1088 Loss 0.7553 Accuracy 0.3188\n",
      "Epoch 42 Batch 1152 Loss 0.7540 Accuracy 0.3187\n",
      "Epoch 42 Batch 1216 Loss 0.7504 Accuracy 0.3187\n",
      "Epoch 42 Batch 1280 Loss 0.7467 Accuracy 0.3201\n",
      "Epoch 42 Batch 1344 Loss 0.7462 Accuracy 0.3196\n",
      "Epoch 42 Batch 1408 Loss 0.7456 Accuracy 0.3198\n",
      "Epoch 42 Batch 1472 Loss 0.7446 Accuracy 0.3200\n",
      "Epoch 42 Batch 1536 Loss 0.7430 Accuracy 0.3199\n",
      "Epoch 42 Batch 1600 Loss 0.7419 Accuracy 0.3202\n",
      "Epoch 42 Batch 1664 Loss 0.7433 Accuracy 0.3196\n",
      "Epoch 42 Batch 1728 Loss 0.7451 Accuracy 0.3197\n",
      "Epoch 42 Batch 1792 Loss 0.7473 Accuracy 0.3195\n",
      "Epoch 42 Batch 1856 Loss 0.7461 Accuracy 0.3198\n",
      "Epoch 42 Batch 1920 Loss 0.7457 Accuracy 0.3201\n",
      "Epoch 42 Batch 1984 Loss 0.7459 Accuracy 0.3205\n",
      "Epoch 42 Batch 2048 Loss 0.7463 Accuracy 0.3205\n",
      "Epoch 42 Batch 2112 Loss 0.7453 Accuracy 0.3206\n",
      "Epoch 42 Batch 2176 Loss 0.7452 Accuracy 0.3206\n",
      "Epoch 42 Batch 2240 Loss 0.7437 Accuracy 0.3209\n",
      "Epoch 42 Batch 2304 Loss 0.7434 Accuracy 0.3208\n",
      "Epoch 42 Batch 2368 Loss 0.7421 Accuracy 0.3211\n",
      "Epoch 42 Batch 2432 Loss 0.7413 Accuracy 0.3212\n",
      "Epoch 42 Batch 2496 Loss 0.7428 Accuracy 0.3212\n",
      "Epoch 42 Batch 2560 Loss 0.7432 Accuracy 0.3211\n",
      "Epoch 42 Batch 2624 Loss 0.7439 Accuracy 0.3212\n",
      "Epoch 42 Batch 2688 Loss 0.7436 Accuracy 0.3213\n",
      "Epoch 42 Batch 2752 Loss 0.7435 Accuracy 0.3213\n",
      "Epoch 42 Batch 2816 Loss 0.7418 Accuracy 0.3216\n",
      "Epoch 42 Batch 2880 Loss 0.7410 Accuracy 0.3217\n",
      "Epoch 42 Batch 2944 Loss 0.7405 Accuracy 0.3220\n",
      "Epoch 42 Batch 3008 Loss 0.7406 Accuracy 0.3223\n",
      "Epoch 42 Batch 3072 Loss 0.7394 Accuracy 0.3226\n",
      "Epoch 42 Batch 3136 Loss 0.7392 Accuracy 0.3225\n",
      "Epoch 42 Loss 0.7392 Accuracy 0.3225\n",
      "Time taken for 1 epoch: 38.021934032440186 secs\n",
      "\n",
      "Epoch 43 Batch 64 Loss 0.6987 Accuracy 0.3303\n",
      "Epoch 43 Batch 128 Loss 0.7026 Accuracy 0.3312\n",
      "Epoch 43 Batch 192 Loss 0.7302 Accuracy 0.3253\n",
      "Epoch 43 Batch 256 Loss 0.7316 Accuracy 0.3242\n",
      "Epoch 43 Batch 320 Loss 0.7396 Accuracy 0.3236\n",
      "Epoch 43 Batch 384 Loss 0.7499 Accuracy 0.3224\n",
      "Epoch 43 Batch 448 Loss 0.7494 Accuracy 0.3216\n",
      "Epoch 43 Batch 512 Loss 0.7437 Accuracy 0.3222\n",
      "Epoch 43 Batch 576 Loss 0.7392 Accuracy 0.3215\n",
      "Epoch 43 Batch 640 Loss 0.7333 Accuracy 0.3211\n",
      "Epoch 43 Batch 704 Loss 0.7327 Accuracy 0.3206\n",
      "Epoch 43 Batch 768 Loss 0.7319 Accuracy 0.3221\n",
      "Epoch 43 Batch 832 Loss 0.7261 Accuracy 0.3231\n",
      "Epoch 43 Batch 896 Loss 0.7256 Accuracy 0.3229\n",
      "Epoch 43 Batch 960 Loss 0.7264 Accuracy 0.3227\n",
      "Epoch 43 Batch 1024 Loss 0.7253 Accuracy 0.3226\n",
      "Epoch 43 Batch 1088 Loss 0.7292 Accuracy 0.3216\n",
      "Epoch 43 Batch 1152 Loss 0.7317 Accuracy 0.3208\n",
      "Epoch 43 Batch 1216 Loss 0.7328 Accuracy 0.3205\n",
      "Epoch 43 Batch 1280 Loss 0.7332 Accuracy 0.3199\n",
      "Epoch 43 Batch 1344 Loss 0.7343 Accuracy 0.3202\n",
      "Epoch 43 Batch 1408 Loss 0.7343 Accuracy 0.3204\n",
      "Epoch 43 Batch 1472 Loss 0.7367 Accuracy 0.3203\n",
      "Epoch 43 Batch 1536 Loss 0.7382 Accuracy 0.3204\n",
      "Epoch 43 Batch 1600 Loss 0.7374 Accuracy 0.3208\n",
      "Epoch 43 Batch 1664 Loss 0.7361 Accuracy 0.3207\n",
      "Epoch 43 Batch 1728 Loss 0.7360 Accuracy 0.3210\n",
      "Epoch 43 Batch 1792 Loss 0.7349 Accuracy 0.3207\n",
      "Epoch 43 Batch 1856 Loss 0.7334 Accuracy 0.3210\n",
      "Epoch 43 Batch 1920 Loss 0.7329 Accuracy 0.3210\n",
      "Epoch 43 Batch 1984 Loss 0.7345 Accuracy 0.3207\n",
      "Epoch 43 Batch 2048 Loss 0.7321 Accuracy 0.3220\n",
      "Epoch 43 Batch 2112 Loss 0.7311 Accuracy 0.3224\n",
      "Epoch 43 Batch 2176 Loss 0.7318 Accuracy 0.3225\n",
      "Epoch 43 Batch 2240 Loss 0.7317 Accuracy 0.3228\n",
      "Epoch 43 Batch 2304 Loss 0.7315 Accuracy 0.3229\n",
      "Epoch 43 Batch 2368 Loss 0.7316 Accuracy 0.3231\n",
      "Epoch 43 Batch 2432 Loss 0.7309 Accuracy 0.3234\n",
      "Epoch 43 Batch 2496 Loss 0.7306 Accuracy 0.3236\n",
      "Epoch 43 Batch 2560 Loss 0.7290 Accuracy 0.3238\n",
      "Epoch 43 Batch 2624 Loss 0.7274 Accuracy 0.3243\n",
      "Epoch 43 Batch 2688 Loss 0.7279 Accuracy 0.3239\n",
      "Epoch 43 Batch 2752 Loss 0.7274 Accuracy 0.3243\n",
      "Epoch 43 Batch 2816 Loss 0.7270 Accuracy 0.3242\n",
      "Epoch 43 Batch 2880 Loss 0.7265 Accuracy 0.3242\n",
      "Epoch 43 Batch 2944 Loss 0.7271 Accuracy 0.3247\n",
      "Epoch 43 Batch 3008 Loss 0.7256 Accuracy 0.3250\n",
      "Epoch 43 Batch 3072 Loss 0.7252 Accuracy 0.3250\n",
      "Epoch 43 Batch 3136 Loss 0.7251 Accuracy 0.3249\n",
      "Epoch 43 Loss 0.7251 Accuracy 0.3249\n",
      "Time taken for 1 epoch: 38.91646909713745 secs\n",
      "\n",
      "Epoch 44 Batch 64 Loss 0.7511 Accuracy 0.3202\n",
      "Epoch 44 Batch 128 Loss 0.7625 Accuracy 0.3193\n",
      "Epoch 44 Batch 192 Loss 0.7526 Accuracy 0.3213\n",
      "Epoch 44 Batch 256 Loss 0.7461 Accuracy 0.3222\n",
      "Epoch 44 Batch 320 Loss 0.7353 Accuracy 0.3216\n",
      "Epoch 44 Batch 384 Loss 0.7503 Accuracy 0.3231\n",
      "Epoch 44 Batch 448 Loss 0.7449 Accuracy 0.3231\n",
      "Epoch 44 Batch 512 Loss 0.7377 Accuracy 0.3235\n",
      "Epoch 44 Batch 576 Loss 0.7363 Accuracy 0.3235\n",
      "Epoch 44 Batch 640 Loss 0.7353 Accuracy 0.3230\n",
      "Epoch 44 Batch 704 Loss 0.7292 Accuracy 0.3236\n",
      "Epoch 44 Batch 768 Loss 0.7302 Accuracy 0.3232\n",
      "Epoch 44 Batch 832 Loss 0.7282 Accuracy 0.3234\n",
      "Epoch 44 Batch 896 Loss 0.7236 Accuracy 0.3245\n",
      "Epoch 44 Batch 960 Loss 0.7205 Accuracy 0.3249\n",
      "Epoch 44 Batch 1024 Loss 0.7148 Accuracy 0.3263\n",
      "Epoch 44 Batch 1088 Loss 0.7130 Accuracy 0.3263\n",
      "Epoch 44 Batch 1152 Loss 0.7122 Accuracy 0.3260\n",
      "Epoch 44 Batch 1216 Loss 0.7099 Accuracy 0.3260\n",
      "Epoch 44 Batch 1280 Loss 0.7097 Accuracy 0.3259\n",
      "Epoch 44 Batch 1344 Loss 0.7136 Accuracy 0.3254\n",
      "Epoch 44 Batch 1408 Loss 0.7130 Accuracy 0.3254\n",
      "Epoch 44 Batch 1472 Loss 0.7113 Accuracy 0.3258\n",
      "Epoch 44 Batch 1536 Loss 0.7132 Accuracy 0.3252\n",
      "Epoch 44 Batch 1600 Loss 0.7121 Accuracy 0.3256\n",
      "Epoch 44 Batch 1664 Loss 0.7118 Accuracy 0.3253\n",
      "Epoch 44 Batch 1728 Loss 0.7115 Accuracy 0.3253\n",
      "Epoch 44 Batch 1792 Loss 0.7102 Accuracy 0.3260\n",
      "Epoch 44 Batch 1856 Loss 0.7102 Accuracy 0.3265\n",
      "Epoch 44 Batch 1920 Loss 0.7096 Accuracy 0.3267\n",
      "Epoch 44 Batch 1984 Loss 0.7087 Accuracy 0.3264\n",
      "Epoch 44 Batch 2048 Loss 0.7083 Accuracy 0.3265\n",
      "Epoch 44 Batch 2112 Loss 0.7083 Accuracy 0.3264\n",
      "Epoch 44 Batch 2176 Loss 0.7076 Accuracy 0.3266\n",
      "Epoch 44 Batch 2240 Loss 0.7061 Accuracy 0.3266\n",
      "Epoch 44 Batch 2304 Loss 0.7044 Accuracy 0.3266\n",
      "Epoch 44 Batch 2368 Loss 0.7025 Accuracy 0.3271\n",
      "Epoch 44 Batch 2432 Loss 0.7025 Accuracy 0.3275\n",
      "Epoch 44 Batch 2496 Loss 0.7035 Accuracy 0.3276\n",
      "Epoch 44 Batch 2560 Loss 0.7026 Accuracy 0.3276\n",
      "Epoch 44 Batch 2624 Loss 0.7014 Accuracy 0.3276\n",
      "Epoch 44 Batch 2688 Loss 0.7001 Accuracy 0.3276\n",
      "Epoch 44 Batch 2752 Loss 0.6994 Accuracy 0.3280\n",
      "Epoch 44 Batch 2816 Loss 0.6998 Accuracy 0.3282\n",
      "Epoch 44 Batch 2880 Loss 0.7006 Accuracy 0.3283\n",
      "Epoch 44 Batch 2944 Loss 0.7002 Accuracy 0.3280\n",
      "Epoch 44 Batch 3008 Loss 0.7017 Accuracy 0.3276\n",
      "Epoch 44 Batch 3072 Loss 0.7012 Accuracy 0.3277\n",
      "Epoch 44 Batch 3136 Loss 0.7015 Accuracy 0.3278\n",
      "Epoch 44 Loss 0.7015 Accuracy 0.3278\n",
      "Time taken for 1 epoch: 39.19004797935486 secs\n",
      "\n",
      "Epoch 45 Batch 64 Loss 0.6727 Accuracy 0.3390\n",
      "Epoch 45 Batch 128 Loss 0.6623 Accuracy 0.3375\n",
      "Epoch 45 Batch 192 Loss 0.6722 Accuracy 0.3353\n",
      "Epoch 45 Batch 256 Loss 0.6696 Accuracy 0.3342\n",
      "Epoch 45 Batch 320 Loss 0.6718 Accuracy 0.3347\n",
      "Epoch 45 Batch 384 Loss 0.6744 Accuracy 0.3330\n",
      "Epoch 45 Batch 448 Loss 0.6819 Accuracy 0.3326\n",
      "Epoch 45 Batch 512 Loss 0.6910 Accuracy 0.3317\n",
      "Epoch 45 Batch 576 Loss 0.7029 Accuracy 0.3295\n",
      "Epoch 45 Batch 640 Loss 0.7044 Accuracy 0.3286\n",
      "Epoch 45 Batch 704 Loss 0.7019 Accuracy 0.3283\n",
      "Epoch 45 Batch 768 Loss 0.7007 Accuracy 0.3287\n",
      "Epoch 45 Batch 832 Loss 0.6998 Accuracy 0.3295\n",
      "Epoch 45 Batch 896 Loss 0.6992 Accuracy 0.3292\n",
      "Epoch 45 Batch 960 Loss 0.6999 Accuracy 0.3303\n",
      "Epoch 45 Batch 1024 Loss 0.6994 Accuracy 0.3309\n",
      "Epoch 45 Batch 1088 Loss 0.6979 Accuracy 0.3304\n",
      "Epoch 45 Batch 1152 Loss 0.6963 Accuracy 0.3303\n",
      "Epoch 45 Batch 1216 Loss 0.6951 Accuracy 0.3302\n",
      "Epoch 45 Batch 1280 Loss 0.6975 Accuracy 0.3301\n",
      "Epoch 45 Batch 1344 Loss 0.6981 Accuracy 0.3298\n",
      "Epoch 45 Batch 1408 Loss 0.6965 Accuracy 0.3302\n",
      "Epoch 45 Batch 1472 Loss 0.6980 Accuracy 0.3298\n",
      "Epoch 45 Batch 1536 Loss 0.7014 Accuracy 0.3294\n",
      "Epoch 45 Batch 1600 Loss 0.7014 Accuracy 0.3295\n",
      "Epoch 45 Batch 1664 Loss 0.6987 Accuracy 0.3293\n",
      "Epoch 45 Batch 1728 Loss 0.6976 Accuracy 0.3294\n",
      "Epoch 45 Batch 1792 Loss 0.6980 Accuracy 0.3289\n",
      "Epoch 45 Batch 1856 Loss 0.6980 Accuracy 0.3284\n",
      "Epoch 45 Batch 1920 Loss 0.6978 Accuracy 0.3283\n",
      "Epoch 45 Batch 1984 Loss 0.6964 Accuracy 0.3286\n",
      "Epoch 45 Batch 2048 Loss 0.6968 Accuracy 0.3285\n",
      "Epoch 45 Batch 2112 Loss 0.6963 Accuracy 0.3280\n",
      "Epoch 45 Batch 2176 Loss 0.6969 Accuracy 0.3284\n",
      "Epoch 45 Batch 2240 Loss 0.6968 Accuracy 0.3280\n",
      "Epoch 45 Batch 2304 Loss 0.6984 Accuracy 0.3279\n",
      "Epoch 45 Batch 2368 Loss 0.6995 Accuracy 0.3276\n",
      "Epoch 45 Batch 2432 Loss 0.6999 Accuracy 0.3275\n",
      "Epoch 45 Batch 2496 Loss 0.6995 Accuracy 0.3273\n",
      "Epoch 45 Batch 2560 Loss 0.7005 Accuracy 0.3276\n",
      "Epoch 45 Batch 2624 Loss 0.6999 Accuracy 0.3275\n",
      "Epoch 45 Batch 2688 Loss 0.6983 Accuracy 0.3280\n",
      "Epoch 45 Batch 2752 Loss 0.6978 Accuracy 0.3280\n",
      "Epoch 45 Batch 2816 Loss 0.6978 Accuracy 0.3281\n",
      "Epoch 45 Batch 2880 Loss 0.6979 Accuracy 0.3280\n",
      "Epoch 45 Batch 2944 Loss 0.6978 Accuracy 0.3282\n",
      "Epoch 45 Batch 3008 Loss 0.6965 Accuracy 0.3285\n",
      "Epoch 45 Batch 3072 Loss 0.6966 Accuracy 0.3285\n",
      "Epoch 45 Batch 3136 Loss 0.6967 Accuracy 0.3284\n",
      "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
      "Epoch 45 Loss 0.6967 Accuracy 0.3284\n",
      "Time taken for 1 epoch: 39.99629807472229 secs\n",
      "\n",
      "Epoch 46 Batch 64 Loss 0.7018 Accuracy 0.3199\n",
      "Epoch 46 Batch 128 Loss 0.6914 Accuracy 0.3222\n",
      "Epoch 46 Batch 192 Loss 0.6999 Accuracy 0.3251\n",
      "Epoch 46 Batch 256 Loss 0.6998 Accuracy 0.3246\n",
      "Epoch 46 Batch 320 Loss 0.7041 Accuracy 0.3239\n",
      "Epoch 46 Batch 384 Loss 0.7010 Accuracy 0.3256\n",
      "Epoch 46 Batch 448 Loss 0.6921 Accuracy 0.3278\n",
      "Epoch 46 Batch 512 Loss 0.6924 Accuracy 0.3282\n",
      "Epoch 46 Batch 576 Loss 0.6952 Accuracy 0.3289\n",
      "Epoch 46 Batch 640 Loss 0.6875 Accuracy 0.3289\n",
      "Epoch 46 Batch 704 Loss 0.6890 Accuracy 0.3290\n",
      "Epoch 46 Batch 768 Loss 0.6880 Accuracy 0.3291\n",
      "Epoch 46 Batch 832 Loss 0.6918 Accuracy 0.3282\n",
      "Epoch 46 Batch 896 Loss 0.6905 Accuracy 0.3290\n",
      "Epoch 46 Batch 960 Loss 0.6906 Accuracy 0.3300\n",
      "Epoch 46 Batch 1024 Loss 0.6860 Accuracy 0.3310\n",
      "Epoch 46 Batch 1088 Loss 0.6817 Accuracy 0.3311\n",
      "Epoch 46 Batch 1152 Loss 0.6819 Accuracy 0.3312\n",
      "Epoch 46 Batch 1216 Loss 0.6796 Accuracy 0.3312\n",
      "Epoch 46 Batch 1280 Loss 0.6814 Accuracy 0.3304\n",
      "Epoch 46 Batch 1344 Loss 0.6807 Accuracy 0.3299\n",
      "Epoch 46 Batch 1408 Loss 0.6780 Accuracy 0.3307\n",
      "Epoch 46 Batch 1472 Loss 0.6768 Accuracy 0.3305\n",
      "Epoch 46 Batch 1536 Loss 0.6761 Accuracy 0.3307\n",
      "Epoch 46 Batch 1600 Loss 0.6773 Accuracy 0.3301\n",
      "Epoch 46 Batch 1664 Loss 0.6787 Accuracy 0.3301\n",
      "Epoch 46 Batch 1728 Loss 0.6767 Accuracy 0.3301\n",
      "Epoch 46 Batch 1792 Loss 0.6761 Accuracy 0.3301\n",
      "Epoch 46 Batch 1856 Loss 0.6779 Accuracy 0.3302\n",
      "Epoch 46 Batch 1920 Loss 0.6798 Accuracy 0.3300\n",
      "Epoch 46 Batch 1984 Loss 0.6789 Accuracy 0.3301\n",
      "Epoch 46 Batch 2048 Loss 0.6803 Accuracy 0.3299\n",
      "Epoch 46 Batch 2112 Loss 0.6821 Accuracy 0.3297\n",
      "Epoch 46 Batch 2176 Loss 0.6818 Accuracy 0.3298\n",
      "Epoch 46 Batch 2240 Loss 0.6819 Accuracy 0.3300\n",
      "Epoch 46 Batch 2304 Loss 0.6819 Accuracy 0.3302\n",
      "Epoch 46 Batch 2368 Loss 0.6803 Accuracy 0.3306\n",
      "Epoch 46 Batch 2432 Loss 0.6800 Accuracy 0.3308\n",
      "Epoch 46 Batch 2496 Loss 0.6789 Accuracy 0.3310\n",
      "Epoch 46 Batch 2560 Loss 0.6784 Accuracy 0.3312\n",
      "Epoch 46 Batch 2624 Loss 0.6773 Accuracy 0.3315\n",
      "Epoch 46 Batch 2688 Loss 0.6764 Accuracy 0.3316\n",
      "Epoch 46 Batch 2752 Loss 0.6766 Accuracy 0.3315\n",
      "Epoch 46 Batch 2816 Loss 0.6772 Accuracy 0.3314\n",
      "Epoch 46 Batch 2880 Loss 0.6777 Accuracy 0.3312\n",
      "Epoch 46 Batch 2944 Loss 0.6779 Accuracy 0.3312\n",
      "Epoch 46 Batch 3008 Loss 0.6774 Accuracy 0.3312\n",
      "Epoch 46 Batch 3072 Loss 0.6772 Accuracy 0.3311\n",
      "Epoch 46 Batch 3136 Loss 0.6763 Accuracy 0.3314\n",
      "Epoch 46 Loss 0.6763 Accuracy 0.3314\n",
      "Time taken for 1 epoch: 39.136101961135864 secs\n",
      "\n",
      "Epoch 47 Batch 64 Loss 0.6646 Accuracy 0.3291\n",
      "Epoch 47 Batch 128 Loss 0.6383 Accuracy 0.3288\n",
      "Epoch 47 Batch 192 Loss 0.6403 Accuracy 0.3321\n",
      "Epoch 47 Batch 256 Loss 0.6346 Accuracy 0.3341\n",
      "Epoch 47 Batch 320 Loss 0.6222 Accuracy 0.3358\n",
      "Epoch 47 Batch 384 Loss 0.6316 Accuracy 0.3350\n",
      "Epoch 47 Batch 448 Loss 0.6360 Accuracy 0.3336\n",
      "Epoch 47 Batch 512 Loss 0.6431 Accuracy 0.3329\n",
      "Epoch 47 Batch 576 Loss 0.6448 Accuracy 0.3336\n",
      "Epoch 47 Batch 640 Loss 0.6453 Accuracy 0.3327\n",
      "Epoch 47 Batch 704 Loss 0.6478 Accuracy 0.3331\n",
      "Epoch 47 Batch 768 Loss 0.6538 Accuracy 0.3327\n",
      "Epoch 47 Batch 832 Loss 0.6567 Accuracy 0.3322\n",
      "Epoch 47 Batch 896 Loss 0.6579 Accuracy 0.3324\n",
      "Epoch 47 Batch 960 Loss 0.6581 Accuracy 0.3329\n",
      "Epoch 47 Batch 1024 Loss 0.6575 Accuracy 0.3334\n",
      "Epoch 47 Batch 1088 Loss 0.6587 Accuracy 0.3338\n",
      "Epoch 47 Batch 1152 Loss 0.6600 Accuracy 0.3334\n",
      "Epoch 47 Batch 1216 Loss 0.6597 Accuracy 0.3324\n",
      "Epoch 47 Batch 1280 Loss 0.6607 Accuracy 0.3327\n",
      "Epoch 47 Batch 1344 Loss 0.6594 Accuracy 0.3334\n",
      "Epoch 47 Batch 1408 Loss 0.6589 Accuracy 0.3333\n",
      "Epoch 47 Batch 1472 Loss 0.6582 Accuracy 0.3333\n",
      "Epoch 47 Batch 1536 Loss 0.6598 Accuracy 0.3341\n",
      "Epoch 47 Batch 1600 Loss 0.6619 Accuracy 0.3338\n",
      "Epoch 47 Batch 1664 Loss 0.6613 Accuracy 0.3341\n",
      "Epoch 47 Batch 1728 Loss 0.6607 Accuracy 0.3344\n",
      "Epoch 47 Batch 1792 Loss 0.6598 Accuracy 0.3340\n",
      "Epoch 47 Batch 1856 Loss 0.6609 Accuracy 0.3339\n",
      "Epoch 47 Batch 1920 Loss 0.6619 Accuracy 0.3337\n",
      "Epoch 47 Batch 1984 Loss 0.6634 Accuracy 0.3337\n",
      "Epoch 47 Batch 2048 Loss 0.6628 Accuracy 0.3340\n",
      "Epoch 47 Batch 2112 Loss 0.6614 Accuracy 0.3340\n",
      "Epoch 47 Batch 2176 Loss 0.6626 Accuracy 0.3343\n",
      "Epoch 47 Batch 2240 Loss 0.6633 Accuracy 0.3342\n",
      "Epoch 47 Batch 2304 Loss 0.6621 Accuracy 0.3343\n",
      "Epoch 47 Batch 2368 Loss 0.6637 Accuracy 0.3343\n",
      "Epoch 47 Batch 2432 Loss 0.6636 Accuracy 0.3343\n",
      "Epoch 47 Batch 2496 Loss 0.6630 Accuracy 0.3347\n",
      "Epoch 47 Batch 2560 Loss 0.6621 Accuracy 0.3349\n",
      "Epoch 47 Batch 2624 Loss 0.6624 Accuracy 0.3348\n",
      "Epoch 47 Batch 2688 Loss 0.6629 Accuracy 0.3349\n",
      "Epoch 47 Batch 2752 Loss 0.6618 Accuracy 0.3348\n",
      "Epoch 47 Batch 2816 Loss 0.6612 Accuracy 0.3345\n",
      "Epoch 47 Batch 2880 Loss 0.6615 Accuracy 0.3343\n",
      "Epoch 47 Batch 2944 Loss 0.6611 Accuracy 0.3340\n",
      "Epoch 47 Batch 3008 Loss 0.6620 Accuracy 0.3342\n",
      "Epoch 47 Batch 3072 Loss 0.6617 Accuracy 0.3338\n",
      "Epoch 47 Batch 3136 Loss 0.6610 Accuracy 0.3340\n",
      "Epoch 47 Loss 0.6610 Accuracy 0.3340\n",
      "Time taken for 1 epoch: 37.852805852890015 secs\n",
      "\n",
      "Epoch 48 Batch 64 Loss 0.6641 Accuracy 0.3315\n",
      "Epoch 48 Batch 128 Loss 0.7003 Accuracy 0.3306\n",
      "Epoch 48 Batch 192 Loss 0.6752 Accuracy 0.3361\n",
      "Epoch 48 Batch 256 Loss 0.6834 Accuracy 0.3361\n",
      "Epoch 48 Batch 320 Loss 0.6810 Accuracy 0.3363\n",
      "Epoch 48 Batch 384 Loss 0.6749 Accuracy 0.3364\n",
      "Epoch 48 Batch 448 Loss 0.6678 Accuracy 0.3384\n",
      "Epoch 48 Batch 512 Loss 0.6657 Accuracy 0.3362\n",
      "Epoch 48 Batch 576 Loss 0.6633 Accuracy 0.3350\n",
      "Epoch 48 Batch 640 Loss 0.6646 Accuracy 0.3353\n",
      "Epoch 48 Batch 704 Loss 0.6619 Accuracy 0.3354\n",
      "Epoch 48 Batch 768 Loss 0.6595 Accuracy 0.3350\n",
      "Epoch 48 Batch 832 Loss 0.6593 Accuracy 0.3355\n",
      "Epoch 48 Batch 896 Loss 0.6578 Accuracy 0.3354\n",
      "Epoch 48 Batch 960 Loss 0.6547 Accuracy 0.3346\n",
      "Epoch 48 Batch 1024 Loss 0.6540 Accuracy 0.3351\n",
      "Epoch 48 Batch 1088 Loss 0.6533 Accuracy 0.3352\n",
      "Epoch 48 Batch 1152 Loss 0.6544 Accuracy 0.3344\n",
      "Epoch 48 Batch 1216 Loss 0.6546 Accuracy 0.3342\n",
      "Epoch 48 Batch 1280 Loss 0.6593 Accuracy 0.3336\n",
      "Epoch 48 Batch 1344 Loss 0.6597 Accuracy 0.3333\n",
      "Epoch 48 Batch 1408 Loss 0.6595 Accuracy 0.3334\n",
      "Epoch 48 Batch 1472 Loss 0.6580 Accuracy 0.3344\n",
      "Epoch 48 Batch 1536 Loss 0.6568 Accuracy 0.3346\n",
      "Epoch 48 Batch 1600 Loss 0.6557 Accuracy 0.3345\n",
      "Epoch 48 Batch 1664 Loss 0.6568 Accuracy 0.3347\n",
      "Epoch 48 Batch 1728 Loss 0.6563 Accuracy 0.3342\n",
      "Epoch 48 Batch 1792 Loss 0.6568 Accuracy 0.3340\n",
      "Epoch 48 Batch 1856 Loss 0.6567 Accuracy 0.3341\n",
      "Epoch 48 Batch 1920 Loss 0.6548 Accuracy 0.3343\n",
      "Epoch 48 Batch 1984 Loss 0.6540 Accuracy 0.3346\n",
      "Epoch 48 Batch 2048 Loss 0.6530 Accuracy 0.3349\n",
      "Epoch 48 Batch 2112 Loss 0.6527 Accuracy 0.3347\n",
      "Epoch 48 Batch 2176 Loss 0.6517 Accuracy 0.3348\n",
      "Epoch 48 Batch 2240 Loss 0.6506 Accuracy 0.3344\n",
      "Epoch 48 Batch 2304 Loss 0.6510 Accuracy 0.3341\n",
      "Epoch 48 Batch 2368 Loss 0.6522 Accuracy 0.3341\n",
      "Epoch 48 Batch 2432 Loss 0.6509 Accuracy 0.3342\n",
      "Epoch 48 Batch 2496 Loss 0.6506 Accuracy 0.3342\n",
      "Epoch 48 Batch 2560 Loss 0.6496 Accuracy 0.3345\n",
      "Epoch 48 Batch 2624 Loss 0.6508 Accuracy 0.3344\n",
      "Epoch 48 Batch 2688 Loss 0.6504 Accuracy 0.3348\n",
      "Epoch 48 Batch 2752 Loss 0.6509 Accuracy 0.3349\n",
      "Epoch 48 Batch 2816 Loss 0.6507 Accuracy 0.3349\n",
      "Epoch 48 Batch 2880 Loss 0.6496 Accuracy 0.3346\n",
      "Epoch 48 Batch 2944 Loss 0.6495 Accuracy 0.3346\n",
      "Epoch 48 Batch 3008 Loss 0.6488 Accuracy 0.3348\n",
      "Epoch 48 Batch 3072 Loss 0.6505 Accuracy 0.3344\n",
      "Epoch 48 Batch 3136 Loss 0.6508 Accuracy 0.3347\n",
      "Epoch 48 Loss 0.6508 Accuracy 0.3347\n",
      "Time taken for 1 epoch: 37.76543307304382 secs\n",
      "\n",
      "Epoch 49 Batch 64 Loss 0.6449 Accuracy 0.3480\n",
      "Epoch 49 Batch 128 Loss 0.6382 Accuracy 0.3412\n",
      "Epoch 49 Batch 192 Loss 0.6365 Accuracy 0.3412\n",
      "Epoch 49 Batch 256 Loss 0.6418 Accuracy 0.3379\n",
      "Epoch 49 Batch 320 Loss 0.6468 Accuracy 0.3369\n",
      "Epoch 49 Batch 384 Loss 0.6435 Accuracy 0.3375\n",
      "Epoch 49 Batch 448 Loss 0.6514 Accuracy 0.3375\n",
      "Epoch 49 Batch 512 Loss 0.6509 Accuracy 0.3380\n",
      "Epoch 49 Batch 576 Loss 0.6510 Accuracy 0.3369\n",
      "Epoch 49 Batch 640 Loss 0.6505 Accuracy 0.3387\n",
      "Epoch 49 Batch 704 Loss 0.6494 Accuracy 0.3387\n",
      "Epoch 49 Batch 768 Loss 0.6466 Accuracy 0.3377\n",
      "Epoch 49 Batch 832 Loss 0.6495 Accuracy 0.3376\n",
      "Epoch 49 Batch 896 Loss 0.6473 Accuracy 0.3372\n",
      "Epoch 49 Batch 960 Loss 0.6476 Accuracy 0.3385\n",
      "Epoch 49 Batch 1024 Loss 0.6431 Accuracy 0.3392\n",
      "Epoch 49 Batch 1088 Loss 0.6455 Accuracy 0.3384\n",
      "Epoch 49 Batch 1152 Loss 0.6478 Accuracy 0.3378\n",
      "Epoch 49 Batch 1216 Loss 0.6498 Accuracy 0.3376\n",
      "Epoch 49 Batch 1280 Loss 0.6489 Accuracy 0.3367\n",
      "Epoch 49 Batch 1344 Loss 0.6469 Accuracy 0.3357\n",
      "Epoch 49 Batch 1408 Loss 0.6467 Accuracy 0.3353\n",
      "Epoch 49 Batch 1472 Loss 0.6451 Accuracy 0.3354\n",
      "Epoch 49 Batch 1536 Loss 0.6459 Accuracy 0.3353\n",
      "Epoch 49 Batch 1600 Loss 0.6465 Accuracy 0.3357\n",
      "Epoch 49 Batch 1664 Loss 0.6452 Accuracy 0.3359\n",
      "Epoch 49 Batch 1728 Loss 0.6441 Accuracy 0.3356\n",
      "Epoch 49 Batch 1792 Loss 0.6447 Accuracy 0.3355\n",
      "Epoch 49 Batch 1856 Loss 0.6447 Accuracy 0.3353\n",
      "Epoch 49 Batch 1920 Loss 0.6446 Accuracy 0.3356\n",
      "Epoch 49 Batch 1984 Loss 0.6427 Accuracy 0.3356\n",
      "Epoch 49 Batch 2048 Loss 0.6406 Accuracy 0.3361\n",
      "Epoch 49 Batch 2112 Loss 0.6421 Accuracy 0.3358\n",
      "Epoch 49 Batch 2176 Loss 0.6421 Accuracy 0.3357\n",
      "Epoch 49 Batch 2240 Loss 0.6427 Accuracy 0.3360\n",
      "Epoch 49 Batch 2304 Loss 0.6408 Accuracy 0.3364\n",
      "Epoch 49 Batch 2368 Loss 0.6407 Accuracy 0.3363\n",
      "Epoch 49 Batch 2432 Loss 0.6399 Accuracy 0.3362\n",
      "Epoch 49 Batch 2496 Loss 0.6406 Accuracy 0.3359\n",
      "Epoch 49 Batch 2560 Loss 0.6428 Accuracy 0.3360\n",
      "Epoch 49 Batch 2624 Loss 0.6430 Accuracy 0.3363\n",
      "Epoch 49 Batch 2688 Loss 0.6432 Accuracy 0.3362\n",
      "Epoch 49 Batch 2752 Loss 0.6421 Accuracy 0.3362\n",
      "Epoch 49 Batch 2816 Loss 0.6409 Accuracy 0.3364\n",
      "Epoch 49 Batch 2880 Loss 0.6394 Accuracy 0.3367\n",
      "Epoch 49 Batch 2944 Loss 0.6384 Accuracy 0.3368\n",
      "Epoch 49 Batch 3008 Loss 0.6375 Accuracy 0.3373\n",
      "Epoch 49 Batch 3072 Loss 0.6372 Accuracy 0.3373\n",
      "Epoch 49 Batch 3136 Loss 0.6368 Accuracy 0.3375\n",
      "Epoch 49 Loss 0.6368 Accuracy 0.3375\n",
      "Time taken for 1 epoch: 38.3947970867157 secs\n",
      "\n",
      "Epoch 50 Batch 64 Loss 0.6251 Accuracy 0.3331\n",
      "Epoch 50 Batch 128 Loss 0.5662 Accuracy 0.3413\n",
      "Epoch 50 Batch 192 Loss 0.5807 Accuracy 0.3395\n",
      "Epoch 50 Batch 256 Loss 0.5964 Accuracy 0.3405\n",
      "Epoch 50 Batch 320 Loss 0.6114 Accuracy 0.3374\n",
      "Epoch 50 Batch 384 Loss 0.6241 Accuracy 0.3373\n",
      "Epoch 50 Batch 448 Loss 0.6310 Accuracy 0.3365\n",
      "Epoch 50 Batch 512 Loss 0.6286 Accuracy 0.3361\n",
      "Epoch 50 Batch 576 Loss 0.6345 Accuracy 0.3345\n",
      "Epoch 50 Batch 640 Loss 0.6373 Accuracy 0.3354\n",
      "Epoch 50 Batch 704 Loss 0.6352 Accuracy 0.3369\n",
      "Epoch 50 Batch 768 Loss 0.6385 Accuracy 0.3356\n",
      "Epoch 50 Batch 832 Loss 0.6407 Accuracy 0.3369\n",
      "Epoch 50 Batch 896 Loss 0.6404 Accuracy 0.3377\n",
      "Epoch 50 Batch 960 Loss 0.6435 Accuracy 0.3370\n",
      "Epoch 50 Batch 1024 Loss 0.6401 Accuracy 0.3381\n",
      "Epoch 50 Batch 1088 Loss 0.6422 Accuracy 0.3379\n",
      "Epoch 50 Batch 1152 Loss 0.6436 Accuracy 0.3378\n",
      "Epoch 50 Batch 1216 Loss 0.6412 Accuracy 0.3378\n",
      "Epoch 50 Batch 1280 Loss 0.6426 Accuracy 0.3377\n",
      "Epoch 50 Batch 1344 Loss 0.6403 Accuracy 0.3377\n",
      "Epoch 50 Batch 1408 Loss 0.6391 Accuracy 0.3381\n",
      "Epoch 50 Batch 1472 Loss 0.6384 Accuracy 0.3387\n",
      "Epoch 50 Batch 1536 Loss 0.6387 Accuracy 0.3387\n",
      "Epoch 50 Batch 1600 Loss 0.6381 Accuracy 0.3384\n",
      "Epoch 50 Batch 1664 Loss 0.6366 Accuracy 0.3386\n",
      "Epoch 50 Batch 1728 Loss 0.6351 Accuracy 0.3384\n",
      "Epoch 50 Batch 1792 Loss 0.6343 Accuracy 0.3386\n",
      "Epoch 50 Batch 1856 Loss 0.6346 Accuracy 0.3386\n",
      "Epoch 50 Batch 1920 Loss 0.6334 Accuracy 0.3384\n",
      "Epoch 50 Batch 1984 Loss 0.6330 Accuracy 0.3384\n",
      "Epoch 50 Batch 2048 Loss 0.6321 Accuracy 0.3385\n",
      "Epoch 50 Batch 2112 Loss 0.6300 Accuracy 0.3386\n",
      "Epoch 50 Batch 2176 Loss 0.6305 Accuracy 0.3384\n",
      "Epoch 50 Batch 2240 Loss 0.6299 Accuracy 0.3383\n",
      "Epoch 50 Batch 2304 Loss 0.6290 Accuracy 0.3383\n",
      "Epoch 50 Batch 2368 Loss 0.6290 Accuracy 0.3384\n",
      "Epoch 50 Batch 2432 Loss 0.6292 Accuracy 0.3386\n",
      "Epoch 50 Batch 2496 Loss 0.6302 Accuracy 0.3383\n",
      "Epoch 50 Batch 2560 Loss 0.6306 Accuracy 0.3384\n",
      "Epoch 50 Batch 2624 Loss 0.6290 Accuracy 0.3384\n",
      "Epoch 50 Batch 2688 Loss 0.6289 Accuracy 0.3387\n",
      "Epoch 50 Batch 2752 Loss 0.6294 Accuracy 0.3388\n",
      "Epoch 50 Batch 2816 Loss 0.6274 Accuracy 0.3388\n",
      "Epoch 50 Batch 2880 Loss 0.6269 Accuracy 0.3389\n",
      "Epoch 50 Batch 2944 Loss 0.6255 Accuracy 0.3390\n",
      "Epoch 50 Batch 3008 Loss 0.6259 Accuracy 0.3390\n",
      "Epoch 50 Batch 3072 Loss 0.6259 Accuracy 0.3389\n",
      "Epoch 50 Batch 3136 Loss 0.6254 Accuracy 0.3389\n",
      "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
      "Epoch 50 Loss 0.6254 Accuracy 0.3389\n",
      "Time taken for 1 epoch: 38.27553629875183 secs\n",
      "\n",
      "Epoch 51 Batch 64 Loss 0.5790 Accuracy 0.3577\n",
      "Epoch 51 Batch 128 Loss 0.6085 Accuracy 0.3458\n",
      "Epoch 51 Batch 192 Loss 0.6013 Accuracy 0.3464\n",
      "Epoch 51 Batch 256 Loss 0.6025 Accuracy 0.3466\n",
      "Epoch 51 Batch 320 Loss 0.6093 Accuracy 0.3444\n",
      "Epoch 51 Batch 384 Loss 0.5991 Accuracy 0.3451\n",
      "Epoch 51 Batch 448 Loss 0.5931 Accuracy 0.3472\n",
      "Epoch 51 Batch 512 Loss 0.5983 Accuracy 0.3447\n",
      "Epoch 51 Batch 576 Loss 0.6019 Accuracy 0.3434\n",
      "Epoch 51 Batch 640 Loss 0.6012 Accuracy 0.3444\n",
      "Epoch 51 Batch 704 Loss 0.6063 Accuracy 0.3439\n",
      "Epoch 51 Batch 768 Loss 0.6074 Accuracy 0.3424\n",
      "Epoch 51 Batch 832 Loss 0.6061 Accuracy 0.3417\n",
      "Epoch 51 Batch 896 Loss 0.6073 Accuracy 0.3413\n",
      "Epoch 51 Batch 960 Loss 0.6072 Accuracy 0.3415\n",
      "Epoch 51 Batch 1024 Loss 0.6060 Accuracy 0.3412\n",
      "Epoch 51 Batch 1088 Loss 0.6060 Accuracy 0.3415\n",
      "Epoch 51 Batch 1152 Loss 0.6053 Accuracy 0.3418\n",
      "Epoch 51 Batch 1216 Loss 0.6062 Accuracy 0.3419\n",
      "Epoch 51 Batch 1280 Loss 0.6064 Accuracy 0.3421\n",
      "Epoch 51 Batch 1344 Loss 0.6057 Accuracy 0.3428\n",
      "Epoch 51 Batch 1408 Loss 0.6076 Accuracy 0.3428\n",
      "Epoch 51 Batch 1472 Loss 0.6069 Accuracy 0.3424\n",
      "Epoch 51 Batch 1536 Loss 0.6053 Accuracy 0.3419\n",
      "Epoch 51 Batch 1600 Loss 0.6055 Accuracy 0.3415\n",
      "Epoch 51 Batch 1664 Loss 0.6049 Accuracy 0.3416\n",
      "Epoch 51 Batch 1728 Loss 0.6064 Accuracy 0.3408\n",
      "Epoch 51 Batch 1792 Loss 0.6063 Accuracy 0.3411\n",
      "Epoch 51 Batch 1856 Loss 0.6083 Accuracy 0.3409\n",
      "Epoch 51 Batch 1920 Loss 0.6069 Accuracy 0.3413\n",
      "Epoch 51 Batch 1984 Loss 0.6055 Accuracy 0.3415\n",
      "Epoch 51 Batch 2048 Loss 0.6057 Accuracy 0.3411\n",
      "Epoch 51 Batch 2112 Loss 0.6062 Accuracy 0.3414\n",
      "Epoch 51 Batch 2176 Loss 0.6046 Accuracy 0.3417\n",
      "Epoch 51 Batch 2240 Loss 0.6056 Accuracy 0.3416\n",
      "Epoch 51 Batch 2304 Loss 0.6040 Accuracy 0.3425\n",
      "Epoch 51 Batch 2368 Loss 0.6037 Accuracy 0.3428\n",
      "Epoch 51 Batch 2432 Loss 0.6029 Accuracy 0.3428\n",
      "Epoch 51 Batch 2496 Loss 0.6041 Accuracy 0.3423\n",
      "Epoch 51 Batch 2560 Loss 0.6051 Accuracy 0.3422\n",
      "Epoch 51 Batch 2624 Loss 0.6039 Accuracy 0.3425\n",
      "Epoch 51 Batch 2688 Loss 0.6025 Accuracy 0.3427\n",
      "Epoch 51 Batch 2752 Loss 0.6019 Accuracy 0.3425\n",
      "Epoch 51 Batch 2816 Loss 0.6023 Accuracy 0.3423\n",
      "Epoch 51 Batch 2880 Loss 0.6031 Accuracy 0.3421\n",
      "Epoch 51 Batch 2944 Loss 0.6034 Accuracy 0.3419\n",
      "Epoch 51 Batch 3008 Loss 0.6027 Accuracy 0.3419\n",
      "Epoch 51 Batch 3072 Loss 0.6032 Accuracy 0.3418\n",
      "Epoch 51 Batch 3136 Loss 0.6035 Accuracy 0.3418\n",
      "Epoch 51 Loss 0.6035 Accuracy 0.3418\n",
      "Time taken for 1 epoch: 36.1857967376709 secs\n",
      "\n",
      "Epoch 52 Batch 64 Loss 0.5804 Accuracy 0.3371\n",
      "Epoch 52 Batch 128 Loss 0.5962 Accuracy 0.3364\n",
      "Epoch 52 Batch 192 Loss 0.6022 Accuracy 0.3379\n",
      "Epoch 52 Batch 256 Loss 0.5922 Accuracy 0.3407\n",
      "Epoch 52 Batch 320 Loss 0.6011 Accuracy 0.3394\n",
      "Epoch 52 Batch 384 Loss 0.6032 Accuracy 0.3396\n",
      "Epoch 52 Batch 448 Loss 0.5988 Accuracy 0.3418\n",
      "Epoch 52 Batch 512 Loss 0.5981 Accuracy 0.3419\n",
      "Epoch 52 Batch 576 Loss 0.5917 Accuracy 0.3428\n",
      "Epoch 52 Batch 640 Loss 0.5938 Accuracy 0.3434\n",
      "Epoch 52 Batch 704 Loss 0.5917 Accuracy 0.3439\n",
      "Epoch 52 Batch 768 Loss 0.5935 Accuracy 0.3435\n",
      "Epoch 52 Batch 832 Loss 0.5991 Accuracy 0.3431\n",
      "Epoch 52 Batch 896 Loss 0.5966 Accuracy 0.3438\n",
      "Epoch 52 Batch 960 Loss 0.5978 Accuracy 0.3432\n",
      "Epoch 52 Batch 1024 Loss 0.5954 Accuracy 0.3444\n",
      "Epoch 52 Batch 1088 Loss 0.5954 Accuracy 0.3446\n",
      "Epoch 52 Batch 1152 Loss 0.5965 Accuracy 0.3447\n",
      "Epoch 52 Batch 1216 Loss 0.5949 Accuracy 0.3447\n",
      "Epoch 52 Batch 1280 Loss 0.5940 Accuracy 0.3451\n",
      "Epoch 52 Batch 1344 Loss 0.5924 Accuracy 0.3451\n",
      "Epoch 52 Batch 1408 Loss 0.5935 Accuracy 0.3448\n",
      "Epoch 52 Batch 1472 Loss 0.5929 Accuracy 0.3452\n",
      "Epoch 52 Batch 1536 Loss 0.5921 Accuracy 0.3451\n",
      "Epoch 52 Batch 1600 Loss 0.5937 Accuracy 0.3443\n",
      "Epoch 52 Batch 1664 Loss 0.5930 Accuracy 0.3443\n",
      "Epoch 52 Batch 1728 Loss 0.5939 Accuracy 0.3438\n",
      "Epoch 52 Batch 1792 Loss 0.5951 Accuracy 0.3435\n",
      "Epoch 52 Batch 1856 Loss 0.5962 Accuracy 0.3439\n",
      "Epoch 52 Batch 1920 Loss 0.5952 Accuracy 0.3438\n",
      "Epoch 52 Batch 1984 Loss 0.5962 Accuracy 0.3432\n",
      "Epoch 52 Batch 2048 Loss 0.5961 Accuracy 0.3433\n",
      "Epoch 52 Batch 2112 Loss 0.5959 Accuracy 0.3435\n",
      "Epoch 52 Batch 2176 Loss 0.5967 Accuracy 0.3431\n",
      "Epoch 52 Batch 2240 Loss 0.5975 Accuracy 0.3430\n",
      "Epoch 52 Batch 2304 Loss 0.5984 Accuracy 0.3427\n",
      "Epoch 52 Batch 2368 Loss 0.5989 Accuracy 0.3425\n",
      "Epoch 52 Batch 2432 Loss 0.5982 Accuracy 0.3422\n",
      "Epoch 52 Batch 2496 Loss 0.5980 Accuracy 0.3423\n",
      "Epoch 52 Batch 2560 Loss 0.5993 Accuracy 0.3422\n",
      "Epoch 52 Batch 2624 Loss 0.6015 Accuracy 0.3423\n",
      "Epoch 52 Batch 2688 Loss 0.6015 Accuracy 0.3422\n",
      "Epoch 52 Batch 2752 Loss 0.6008 Accuracy 0.3423\n",
      "Epoch 52 Batch 2816 Loss 0.5995 Accuracy 0.3425\n",
      "Epoch 52 Batch 2880 Loss 0.6013 Accuracy 0.3421\n",
      "Epoch 52 Batch 2944 Loss 0.6010 Accuracy 0.3422\n",
      "Epoch 52 Batch 3008 Loss 0.6011 Accuracy 0.3421\n",
      "Epoch 52 Batch 3072 Loss 0.6021 Accuracy 0.3419\n",
      "Epoch 52 Batch 3136 Loss 0.6008 Accuracy 0.3419\n",
      "Epoch 52 Loss 0.6008 Accuracy 0.3419\n",
      "Time taken for 1 epoch: 36.044459104537964 secs\n",
      "\n",
      "Epoch 53 Batch 64 Loss 0.5723 Accuracy 0.3555\n",
      "Epoch 53 Batch 128 Loss 0.5668 Accuracy 0.3596\n",
      "Epoch 53 Batch 192 Loss 0.5568 Accuracy 0.3589\n",
      "Epoch 53 Batch 256 Loss 0.5694 Accuracy 0.3588\n",
      "Epoch 53 Batch 320 Loss 0.5738 Accuracy 0.3544\n",
      "Epoch 53 Batch 384 Loss 0.5629 Accuracy 0.3523\n",
      "Epoch 53 Batch 448 Loss 0.5730 Accuracy 0.3499\n",
      "Epoch 53 Batch 512 Loss 0.5746 Accuracy 0.3480\n",
      "Epoch 53 Batch 576 Loss 0.5760 Accuracy 0.3484\n",
      "Epoch 53 Batch 640 Loss 0.5803 Accuracy 0.3472\n",
      "Epoch 53 Batch 704 Loss 0.5875 Accuracy 0.3466\n",
      "Epoch 53 Batch 768 Loss 0.5859 Accuracy 0.3479\n",
      "Epoch 53 Batch 832 Loss 0.5834 Accuracy 0.3479\n",
      "Epoch 53 Batch 896 Loss 0.5867 Accuracy 0.3474\n",
      "Epoch 53 Batch 960 Loss 0.5875 Accuracy 0.3470\n",
      "Epoch 53 Batch 1024 Loss 0.5883 Accuracy 0.3461\n",
      "Epoch 53 Batch 1088 Loss 0.5869 Accuracy 0.3461\n",
      "Epoch 53 Batch 1152 Loss 0.5891 Accuracy 0.3463\n",
      "Epoch 53 Batch 1216 Loss 0.5904 Accuracy 0.3470\n",
      "Epoch 53 Batch 1280 Loss 0.5898 Accuracy 0.3465\n",
      "Epoch 53 Batch 1344 Loss 0.5919 Accuracy 0.3465\n",
      "Epoch 53 Batch 1408 Loss 0.5925 Accuracy 0.3462\n",
      "Epoch 53 Batch 1472 Loss 0.5934 Accuracy 0.3460\n",
      "Epoch 53 Batch 1536 Loss 0.5925 Accuracy 0.3465\n",
      "Epoch 53 Batch 1600 Loss 0.5919 Accuracy 0.3461\n",
      "Epoch 53 Batch 1664 Loss 0.5919 Accuracy 0.3457\n",
      "Epoch 53 Batch 1728 Loss 0.5916 Accuracy 0.3460\n",
      "Epoch 53 Batch 1792 Loss 0.5895 Accuracy 0.3461\n",
      "Epoch 53 Batch 1856 Loss 0.5900 Accuracy 0.3453\n",
      "Epoch 53 Batch 1920 Loss 0.5888 Accuracy 0.3452\n",
      "Epoch 53 Batch 1984 Loss 0.5878 Accuracy 0.3454\n",
      "Epoch 53 Batch 2048 Loss 0.5875 Accuracy 0.3456\n",
      "Epoch 53 Batch 2112 Loss 0.5865 Accuracy 0.3453\n",
      "Epoch 53 Batch 2176 Loss 0.5869 Accuracy 0.3455\n",
      "Epoch 53 Batch 2240 Loss 0.5867 Accuracy 0.3456\n",
      "Epoch 53 Batch 2304 Loss 0.5866 Accuracy 0.3454\n",
      "Epoch 53 Batch 2368 Loss 0.5870 Accuracy 0.3458\n",
      "Epoch 53 Batch 2432 Loss 0.5867 Accuracy 0.3456\n",
      "Epoch 53 Batch 2496 Loss 0.5866 Accuracy 0.3458\n",
      "Epoch 53 Batch 2560 Loss 0.5858 Accuracy 0.3456\n",
      "Epoch 53 Batch 2624 Loss 0.5853 Accuracy 0.3456\n",
      "Epoch 53 Batch 2688 Loss 0.5850 Accuracy 0.3456\n",
      "Epoch 53 Batch 2752 Loss 0.5861 Accuracy 0.3453\n",
      "Epoch 53 Batch 2816 Loss 0.5863 Accuracy 0.3451\n",
      "Epoch 53 Batch 2880 Loss 0.5862 Accuracy 0.3450\n",
      "Epoch 53 Batch 2944 Loss 0.5857 Accuracy 0.3452\n",
      "Epoch 53 Batch 3008 Loss 0.5844 Accuracy 0.3450\n",
      "Epoch 53 Batch 3072 Loss 0.5848 Accuracy 0.3448\n",
      "Epoch 53 Batch 3136 Loss 0.5856 Accuracy 0.3446\n",
      "Epoch 53 Loss 0.5856 Accuracy 0.3446\n",
      "Time taken for 1 epoch: 35.74653673171997 secs\n",
      "\n",
      "Epoch 54 Batch 64 Loss 0.5806 Accuracy 0.3689\n",
      "Epoch 54 Batch 128 Loss 0.5981 Accuracy 0.3554\n",
      "Epoch 54 Batch 192 Loss 0.5986 Accuracy 0.3491\n",
      "Epoch 54 Batch 256 Loss 0.5890 Accuracy 0.3521\n",
      "Epoch 54 Batch 320 Loss 0.5801 Accuracy 0.3510\n",
      "Epoch 54 Batch 384 Loss 0.5847 Accuracy 0.3496\n",
      "Epoch 54 Batch 448 Loss 0.5877 Accuracy 0.3494\n",
      "Epoch 54 Batch 512 Loss 0.5850 Accuracy 0.3476\n",
      "Epoch 54 Batch 576 Loss 0.5800 Accuracy 0.3493\n",
      "Epoch 54 Batch 640 Loss 0.5808 Accuracy 0.3478\n",
      "Epoch 54 Batch 704 Loss 0.5827 Accuracy 0.3474\n",
      "Epoch 54 Batch 768 Loss 0.5824 Accuracy 0.3475\n",
      "Epoch 54 Batch 832 Loss 0.5809 Accuracy 0.3466\n",
      "Epoch 54 Batch 896 Loss 0.5844 Accuracy 0.3457\n",
      "Epoch 54 Batch 960 Loss 0.5851 Accuracy 0.3463\n",
      "Epoch 54 Batch 1024 Loss 0.5812 Accuracy 0.3471\n",
      "Epoch 54 Batch 1088 Loss 0.5807 Accuracy 0.3474\n",
      "Epoch 54 Batch 1152 Loss 0.5800 Accuracy 0.3475\n",
      "Epoch 54 Batch 1216 Loss 0.5806 Accuracy 0.3475\n",
      "Epoch 54 Batch 1280 Loss 0.5835 Accuracy 0.3470\n",
      "Epoch 54 Batch 1344 Loss 0.5812 Accuracy 0.3471\n",
      "Epoch 54 Batch 1408 Loss 0.5805 Accuracy 0.3463\n",
      "Epoch 54 Batch 1472 Loss 0.5806 Accuracy 0.3460\n",
      "Epoch 54 Batch 1536 Loss 0.5819 Accuracy 0.3458\n",
      "Epoch 54 Batch 1600 Loss 0.5813 Accuracy 0.3453\n",
      "Epoch 54 Batch 1664 Loss 0.5810 Accuracy 0.3458\n",
      "Epoch 54 Batch 1728 Loss 0.5800 Accuracy 0.3456\n",
      "Epoch 54 Batch 1792 Loss 0.5800 Accuracy 0.3461\n",
      "Epoch 54 Batch 1856 Loss 0.5807 Accuracy 0.3458\n",
      "Epoch 54 Batch 1920 Loss 0.5802 Accuracy 0.3455\n",
      "Epoch 54 Batch 1984 Loss 0.5794 Accuracy 0.3456\n",
      "Epoch 54 Batch 2048 Loss 0.5767 Accuracy 0.3459\n",
      "Epoch 54 Batch 2112 Loss 0.5780 Accuracy 0.3458\n",
      "Epoch 54 Batch 2176 Loss 0.5786 Accuracy 0.3457\n",
      "Epoch 54 Batch 2240 Loss 0.5794 Accuracy 0.3457\n",
      "Epoch 54 Batch 2304 Loss 0.5794 Accuracy 0.3456\n",
      "Epoch 54 Batch 2368 Loss 0.5787 Accuracy 0.3460\n",
      "Epoch 54 Batch 2432 Loss 0.5791 Accuracy 0.3458\n",
      "Epoch 54 Batch 2496 Loss 0.5792 Accuracy 0.3458\n",
      "Epoch 54 Batch 2560 Loss 0.5778 Accuracy 0.3458\n",
      "Epoch 54 Batch 2624 Loss 0.5780 Accuracy 0.3459\n",
      "Epoch 54 Batch 2688 Loss 0.5786 Accuracy 0.3457\n",
      "Epoch 54 Batch 2752 Loss 0.5783 Accuracy 0.3457\n",
      "Epoch 54 Batch 2816 Loss 0.5782 Accuracy 0.3459\n",
      "Epoch 54 Batch 2880 Loss 0.5778 Accuracy 0.3462\n",
      "Epoch 54 Batch 2944 Loss 0.5770 Accuracy 0.3462\n",
      "Epoch 54 Batch 3008 Loss 0.5773 Accuracy 0.3461\n",
      "Epoch 54 Batch 3072 Loss 0.5758 Accuracy 0.3460\n",
      "Epoch 54 Batch 3136 Loss 0.5758 Accuracy 0.3458\n",
      "Epoch 54 Loss 0.5758 Accuracy 0.3458\n",
      "Time taken for 1 epoch: 35.88677930831909 secs\n",
      "\n",
      "Epoch 55 Batch 64 Loss 0.5972 Accuracy 0.3432\n",
      "Epoch 55 Batch 128 Loss 0.5746 Accuracy 0.3440\n",
      "Epoch 55 Batch 192 Loss 0.5834 Accuracy 0.3425\n",
      "Epoch 55 Batch 256 Loss 0.5663 Accuracy 0.3436\n",
      "Epoch 55 Batch 320 Loss 0.5695 Accuracy 0.3464\n",
      "Epoch 55 Batch 384 Loss 0.5719 Accuracy 0.3474\n",
      "Epoch 55 Batch 448 Loss 0.5657 Accuracy 0.3481\n",
      "Epoch 55 Batch 512 Loss 0.5642 Accuracy 0.3499\n",
      "Epoch 55 Batch 576 Loss 0.5594 Accuracy 0.3513\n",
      "Epoch 55 Batch 640 Loss 0.5604 Accuracy 0.3519\n",
      "Epoch 55 Batch 704 Loss 0.5636 Accuracy 0.3509\n",
      "Epoch 55 Batch 768 Loss 0.5639 Accuracy 0.3506\n",
      "Epoch 55 Batch 832 Loss 0.5662 Accuracy 0.3491\n",
      "Epoch 55 Batch 896 Loss 0.5643 Accuracy 0.3486\n",
      "Epoch 55 Batch 960 Loss 0.5657 Accuracy 0.3474\n",
      "Epoch 55 Batch 1024 Loss 0.5674 Accuracy 0.3478\n",
      "Epoch 55 Batch 1088 Loss 0.5662 Accuracy 0.3486\n",
      "Epoch 55 Batch 1152 Loss 0.5679 Accuracy 0.3479\n",
      "Epoch 55 Batch 1216 Loss 0.5687 Accuracy 0.3479\n",
      "Epoch 55 Batch 1280 Loss 0.5693 Accuracy 0.3475\n",
      "Epoch 55 Batch 1344 Loss 0.5682 Accuracy 0.3482\n",
      "Epoch 55 Batch 1408 Loss 0.5731 Accuracy 0.3473\n",
      "Epoch 55 Batch 1472 Loss 0.5719 Accuracy 0.3476\n",
      "Epoch 55 Batch 1536 Loss 0.5731 Accuracy 0.3474\n",
      "Epoch 55 Batch 1600 Loss 0.5726 Accuracy 0.3474\n",
      "Epoch 55 Batch 1664 Loss 0.5715 Accuracy 0.3475\n",
      "Epoch 55 Batch 1728 Loss 0.5719 Accuracy 0.3476\n",
      "Epoch 55 Batch 1792 Loss 0.5727 Accuracy 0.3475\n",
      "Epoch 55 Batch 1856 Loss 0.5724 Accuracy 0.3472\n",
      "Epoch 55 Batch 1920 Loss 0.5702 Accuracy 0.3474\n",
      "Epoch 55 Batch 1984 Loss 0.5695 Accuracy 0.3473\n",
      "Epoch 55 Batch 2048 Loss 0.5712 Accuracy 0.3472\n",
      "Epoch 55 Batch 2112 Loss 0.5718 Accuracy 0.3468\n",
      "Epoch 55 Batch 2176 Loss 0.5715 Accuracy 0.3467\n",
      "Epoch 55 Batch 2240 Loss 0.5726 Accuracy 0.3465\n",
      "Epoch 55 Batch 2304 Loss 0.5725 Accuracy 0.3462\n",
      "Epoch 55 Batch 2368 Loss 0.5729 Accuracy 0.3460\n",
      "Epoch 55 Batch 2432 Loss 0.5731 Accuracy 0.3459\n",
      "Epoch 55 Batch 2496 Loss 0.5716 Accuracy 0.3460\n",
      "Epoch 55 Batch 2560 Loss 0.5714 Accuracy 0.3461\n",
      "Epoch 55 Batch 2624 Loss 0.5721 Accuracy 0.3464\n",
      "Epoch 55 Batch 2688 Loss 0.5721 Accuracy 0.3467\n",
      "Epoch 55 Batch 2752 Loss 0.5711 Accuracy 0.3471\n",
      "Epoch 55 Batch 2816 Loss 0.5709 Accuracy 0.3472\n",
      "Epoch 55 Batch 2880 Loss 0.5696 Accuracy 0.3471\n",
      "Epoch 55 Batch 2944 Loss 0.5691 Accuracy 0.3470\n",
      "Epoch 55 Batch 3008 Loss 0.5677 Accuracy 0.3472\n",
      "Epoch 55 Batch 3072 Loss 0.5668 Accuracy 0.3473\n",
      "Epoch 55 Batch 3136 Loss 0.5662 Accuracy 0.3470\n",
      "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
      "Epoch 55 Loss 0.5662 Accuracy 0.3470\n",
      "Time taken for 1 epoch: 36.38995814323425 secs\n",
      "\n",
      "Epoch 56 Batch 64 Loss 0.5369 Accuracy 0.3585\n",
      "Epoch 56 Batch 128 Loss 0.5484 Accuracy 0.3539\n",
      "Epoch 56 Batch 192 Loss 0.5602 Accuracy 0.3529\n",
      "Epoch 56 Batch 256 Loss 0.5520 Accuracy 0.3530\n",
      "Epoch 56 Batch 320 Loss 0.5449 Accuracy 0.3515\n",
      "Epoch 56 Batch 384 Loss 0.5455 Accuracy 0.3523\n",
      "Epoch 56 Batch 448 Loss 0.5508 Accuracy 0.3527\n",
      "Epoch 56 Batch 512 Loss 0.5428 Accuracy 0.3517\n",
      "Epoch 56 Batch 576 Loss 0.5485 Accuracy 0.3504\n",
      "Epoch 56 Batch 640 Loss 0.5507 Accuracy 0.3509\n",
      "Epoch 56 Batch 704 Loss 0.5498 Accuracy 0.3512\n",
      "Epoch 56 Batch 768 Loss 0.5484 Accuracy 0.3518\n",
      "Epoch 56 Batch 832 Loss 0.5474 Accuracy 0.3518\n",
      "Epoch 56 Batch 896 Loss 0.5507 Accuracy 0.3515\n",
      "Epoch 56 Batch 960 Loss 0.5492 Accuracy 0.3512\n",
      "Epoch 56 Batch 1024 Loss 0.5500 Accuracy 0.3500\n",
      "Epoch 56 Batch 1088 Loss 0.5497 Accuracy 0.3505\n",
      "Epoch 56 Batch 1152 Loss 0.5519 Accuracy 0.3501\n",
      "Epoch 56 Batch 1216 Loss 0.5529 Accuracy 0.3500\n",
      "Epoch 56 Batch 1280 Loss 0.5545 Accuracy 0.3499\n",
      "Epoch 56 Batch 1344 Loss 0.5551 Accuracy 0.3492\n",
      "Epoch 56 Batch 1408 Loss 0.5557 Accuracy 0.3488\n",
      "Epoch 56 Batch 1472 Loss 0.5573 Accuracy 0.3489\n",
      "Epoch 56 Batch 1536 Loss 0.5571 Accuracy 0.3492\n",
      "Epoch 56 Batch 1600 Loss 0.5566 Accuracy 0.3494\n",
      "Epoch 56 Batch 1664 Loss 0.5575 Accuracy 0.3486\n",
      "Epoch 56 Batch 1728 Loss 0.5575 Accuracy 0.3489\n",
      "Epoch 56 Batch 1792 Loss 0.5579 Accuracy 0.3496\n",
      "Epoch 56 Batch 1856 Loss 0.5571 Accuracy 0.3493\n",
      "Epoch 56 Batch 1920 Loss 0.5560 Accuracy 0.3493\n",
      "Epoch 56 Batch 1984 Loss 0.5546 Accuracy 0.3495\n",
      "Epoch 56 Batch 2048 Loss 0.5542 Accuracy 0.3497\n",
      "Epoch 56 Batch 2112 Loss 0.5542 Accuracy 0.3501\n",
      "Epoch 56 Batch 2176 Loss 0.5531 Accuracy 0.3498\n",
      "Epoch 56 Batch 2240 Loss 0.5541 Accuracy 0.3496\n",
      "Epoch 56 Batch 2304 Loss 0.5544 Accuracy 0.3497\n",
      "Epoch 56 Batch 2368 Loss 0.5540 Accuracy 0.3498\n",
      "Epoch 56 Batch 2432 Loss 0.5541 Accuracy 0.3496\n",
      "Epoch 56 Batch 2496 Loss 0.5532 Accuracy 0.3497\n",
      "Epoch 56 Batch 2560 Loss 0.5527 Accuracy 0.3492\n",
      "Epoch 56 Batch 2624 Loss 0.5523 Accuracy 0.3492\n",
      "Epoch 56 Batch 2688 Loss 0.5521 Accuracy 0.3489\n",
      "Epoch 56 Batch 2752 Loss 0.5517 Accuracy 0.3490\n",
      "Epoch 56 Batch 2816 Loss 0.5526 Accuracy 0.3490\n",
      "Epoch 56 Batch 2880 Loss 0.5525 Accuracy 0.3492\n",
      "Epoch 56 Batch 2944 Loss 0.5524 Accuracy 0.3490\n",
      "Epoch 56 Batch 3008 Loss 0.5518 Accuracy 0.3490\n",
      "Epoch 56 Batch 3072 Loss 0.5510 Accuracy 0.3492\n",
      "Epoch 56 Batch 3136 Loss 0.5504 Accuracy 0.3493\n",
      "Epoch 56 Loss 0.5504 Accuracy 0.3493\n",
      "Time taken for 1 epoch: 36.7691171169281 secs\n",
      "\n",
      "Epoch 57 Batch 64 Loss 0.5735 Accuracy 0.3384\n",
      "Epoch 57 Batch 128 Loss 0.5612 Accuracy 0.3466\n",
      "Epoch 57 Batch 192 Loss 0.5528 Accuracy 0.3465\n",
      "Epoch 57 Batch 256 Loss 0.5531 Accuracy 0.3479\n",
      "Epoch 57 Batch 320 Loss 0.5488 Accuracy 0.3489\n",
      "Epoch 57 Batch 384 Loss 0.5435 Accuracy 0.3502\n",
      "Epoch 57 Batch 448 Loss 0.5449 Accuracy 0.3495\n",
      "Epoch 57 Batch 512 Loss 0.5416 Accuracy 0.3494\n",
      "Epoch 57 Batch 576 Loss 0.5444 Accuracy 0.3481\n",
      "Epoch 57 Batch 640 Loss 0.5420 Accuracy 0.3489\n",
      "Epoch 57 Batch 704 Loss 0.5444 Accuracy 0.3484\n",
      "Epoch 57 Batch 768 Loss 0.5503 Accuracy 0.3482\n",
      "Epoch 57 Batch 832 Loss 0.5496 Accuracy 0.3477\n",
      "Epoch 57 Batch 896 Loss 0.5478 Accuracy 0.3477\n",
      "Epoch 57 Batch 960 Loss 0.5462 Accuracy 0.3481\n",
      "Epoch 57 Batch 1024 Loss 0.5425 Accuracy 0.3487\n",
      "Epoch 57 Batch 1088 Loss 0.5407 Accuracy 0.3486\n",
      "Epoch 57 Batch 1152 Loss 0.5405 Accuracy 0.3498\n",
      "Epoch 57 Batch 1216 Loss 0.5413 Accuracy 0.3498\n",
      "Epoch 57 Batch 1280 Loss 0.5421 Accuracy 0.3497\n",
      "Epoch 57 Batch 1344 Loss 0.5375 Accuracy 0.3501\n",
      "Epoch 57 Batch 1408 Loss 0.5373 Accuracy 0.3504\n",
      "Epoch 57 Batch 1472 Loss 0.5365 Accuracy 0.3501\n",
      "Epoch 57 Batch 1536 Loss 0.5351 Accuracy 0.3506\n",
      "Epoch 57 Batch 1600 Loss 0.5327 Accuracy 0.3505\n",
      "Epoch 57 Batch 1664 Loss 0.5354 Accuracy 0.3506\n",
      "Epoch 57 Batch 1728 Loss 0.5358 Accuracy 0.3509\n",
      "Epoch 57 Batch 1792 Loss 0.5350 Accuracy 0.3508\n",
      "Epoch 57 Batch 1856 Loss 0.5361 Accuracy 0.3505\n",
      "Epoch 57 Batch 1920 Loss 0.5361 Accuracy 0.3507\n",
      "Epoch 57 Batch 1984 Loss 0.5356 Accuracy 0.3506\n",
      "Epoch 57 Batch 2048 Loss 0.5358 Accuracy 0.3509\n",
      "Epoch 57 Batch 2112 Loss 0.5364 Accuracy 0.3515\n",
      "Epoch 57 Batch 2176 Loss 0.5375 Accuracy 0.3517\n",
      "Epoch 57 Batch 2240 Loss 0.5385 Accuracy 0.3515\n",
      "Epoch 57 Batch 2304 Loss 0.5393 Accuracy 0.3511\n",
      "Epoch 57 Batch 2368 Loss 0.5391 Accuracy 0.3511\n",
      "Epoch 57 Batch 2432 Loss 0.5390 Accuracy 0.3511\n",
      "Epoch 57 Batch 2496 Loss 0.5394 Accuracy 0.3512\n",
      "Epoch 57 Batch 2560 Loss 0.5395 Accuracy 0.3515\n",
      "Epoch 57 Batch 2624 Loss 0.5398 Accuracy 0.3515\n",
      "Epoch 57 Batch 2688 Loss 0.5399 Accuracy 0.3516\n",
      "Epoch 57 Batch 2752 Loss 0.5387 Accuracy 0.3515\n",
      "Epoch 57 Batch 2816 Loss 0.5391 Accuracy 0.3510\n",
      "Epoch 57 Batch 2880 Loss 0.5396 Accuracy 0.3508\n",
      "Epoch 57 Batch 2944 Loss 0.5389 Accuracy 0.3512\n",
      "Epoch 57 Batch 3008 Loss 0.5390 Accuracy 0.3513\n",
      "Epoch 57 Batch 3072 Loss 0.5385 Accuracy 0.3514\n",
      "Epoch 57 Batch 3136 Loss 0.5381 Accuracy 0.3513\n",
      "Epoch 57 Loss 0.5381 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 36.290433168411255 secs\n",
      "\n",
      "Epoch 58 Batch 64 Loss 0.4695 Accuracy 0.3464\n",
      "Epoch 58 Batch 128 Loss 0.5193 Accuracy 0.3464\n",
      "Epoch 58 Batch 192 Loss 0.5133 Accuracy 0.3476\n",
      "Epoch 58 Batch 256 Loss 0.5174 Accuracy 0.3474\n",
      "Epoch 58 Batch 320 Loss 0.5200 Accuracy 0.3477\n",
      "Epoch 58 Batch 384 Loss 0.5273 Accuracy 0.3467\n",
      "Epoch 58 Batch 448 Loss 0.5243 Accuracy 0.3450\n",
      "Epoch 58 Batch 512 Loss 0.5187 Accuracy 0.3483\n",
      "Epoch 58 Batch 576 Loss 0.5168 Accuracy 0.3481\n",
      "Epoch 58 Batch 640 Loss 0.5163 Accuracy 0.3485\n",
      "Epoch 58 Batch 704 Loss 0.5176 Accuracy 0.3481\n",
      "Epoch 58 Batch 768 Loss 0.5158 Accuracy 0.3482\n",
      "Epoch 58 Batch 832 Loss 0.5154 Accuracy 0.3493\n",
      "Epoch 58 Batch 896 Loss 0.5176 Accuracy 0.3495\n",
      "Epoch 58 Batch 960 Loss 0.5225 Accuracy 0.3495\n",
      "Epoch 58 Batch 1024 Loss 0.5254 Accuracy 0.3503\n",
      "Epoch 58 Batch 1088 Loss 0.5272 Accuracy 0.3499\n",
      "Epoch 58 Batch 1152 Loss 0.5259 Accuracy 0.3499\n",
      "Epoch 58 Batch 1216 Loss 0.5282 Accuracy 0.3500\n",
      "Epoch 58 Batch 1280 Loss 0.5300 Accuracy 0.3500\n",
      "Epoch 58 Batch 1344 Loss 0.5295 Accuracy 0.3500\n",
      "Epoch 58 Batch 1408 Loss 0.5289 Accuracy 0.3500\n",
      "Epoch 58 Batch 1472 Loss 0.5283 Accuracy 0.3497\n",
      "Epoch 58 Batch 1536 Loss 0.5263 Accuracy 0.3502\n",
      "Epoch 58 Batch 1600 Loss 0.5254 Accuracy 0.3509\n",
      "Epoch 58 Batch 1664 Loss 0.5260 Accuracy 0.3512\n",
      "Epoch 58 Batch 1728 Loss 0.5271 Accuracy 0.3514\n",
      "Epoch 58 Batch 1792 Loss 0.5287 Accuracy 0.3513\n",
      "Epoch 58 Batch 1856 Loss 0.5286 Accuracy 0.3510\n",
      "Epoch 58 Batch 1920 Loss 0.5295 Accuracy 0.3513\n",
      "Epoch 58 Batch 1984 Loss 0.5288 Accuracy 0.3514\n",
      "Epoch 58 Batch 2048 Loss 0.5292 Accuracy 0.3521\n",
      "Epoch 58 Batch 2112 Loss 0.5288 Accuracy 0.3517\n",
      "Epoch 58 Batch 2176 Loss 0.5299 Accuracy 0.3517\n",
      "Epoch 58 Batch 2240 Loss 0.5295 Accuracy 0.3519\n",
      "Epoch 58 Batch 2304 Loss 0.5302 Accuracy 0.3518\n",
      "Epoch 58 Batch 2368 Loss 0.5310 Accuracy 0.3516\n",
      "Epoch 58 Batch 2432 Loss 0.5306 Accuracy 0.3516\n",
      "Epoch 58 Batch 2496 Loss 0.5298 Accuracy 0.3519\n",
      "Epoch 58 Batch 2560 Loss 0.5301 Accuracy 0.3520\n",
      "Epoch 58 Batch 2624 Loss 0.5308 Accuracy 0.3518\n",
      "Epoch 58 Batch 2688 Loss 0.5316 Accuracy 0.3516\n",
      "Epoch 58 Batch 2752 Loss 0.5326 Accuracy 0.3517\n",
      "Epoch 58 Batch 2816 Loss 0.5320 Accuracy 0.3520\n",
      "Epoch 58 Batch 2880 Loss 0.5322 Accuracy 0.3520\n",
      "Epoch 58 Batch 2944 Loss 0.5324 Accuracy 0.3522\n",
      "Epoch 58 Batch 3008 Loss 0.5323 Accuracy 0.3525\n",
      "Epoch 58 Batch 3072 Loss 0.5330 Accuracy 0.3527\n",
      "Epoch 58 Batch 3136 Loss 0.5330 Accuracy 0.3525\n",
      "Epoch 58 Loss 0.5330 Accuracy 0.3525\n",
      "Time taken for 1 epoch: 35.39696168899536 secs\n",
      "\n",
      "Epoch 59 Batch 64 Loss 0.4553 Accuracy 0.3672\n",
      "Epoch 59 Batch 128 Loss 0.4996 Accuracy 0.3596\n",
      "Epoch 59 Batch 192 Loss 0.5135 Accuracy 0.3561\n",
      "Epoch 59 Batch 256 Loss 0.5185 Accuracy 0.3535\n",
      "Epoch 59 Batch 320 Loss 0.5194 Accuracy 0.3532\n",
      "Epoch 59 Batch 384 Loss 0.5223 Accuracy 0.3549\n",
      "Epoch 59 Batch 448 Loss 0.5269 Accuracy 0.3545\n",
      "Epoch 59 Batch 512 Loss 0.5194 Accuracy 0.3537\n",
      "Epoch 59 Batch 576 Loss 0.5220 Accuracy 0.3535\n",
      "Epoch 59 Batch 640 Loss 0.5245 Accuracy 0.3517\n",
      "Epoch 59 Batch 704 Loss 0.5218 Accuracy 0.3533\n",
      "Epoch 59 Batch 768 Loss 0.5225 Accuracy 0.3536\n",
      "Epoch 59 Batch 832 Loss 0.5183 Accuracy 0.3547\n",
      "Epoch 59 Batch 896 Loss 0.5139 Accuracy 0.3553\n",
      "Epoch 59 Batch 960 Loss 0.5124 Accuracy 0.3546\n",
      "Epoch 59 Batch 1024 Loss 0.5099 Accuracy 0.3544\n",
      "Epoch 59 Batch 1088 Loss 0.5103 Accuracy 0.3542\n",
      "Epoch 59 Batch 1152 Loss 0.5125 Accuracy 0.3537\n",
      "Epoch 59 Batch 1216 Loss 0.5139 Accuracy 0.3538\n",
      "Epoch 59 Batch 1280 Loss 0.5125 Accuracy 0.3539\n",
      "Epoch 59 Batch 1344 Loss 0.5129 Accuracy 0.3537\n",
      "Epoch 59 Batch 1408 Loss 0.5158 Accuracy 0.3540\n",
      "Epoch 59 Batch 1472 Loss 0.5178 Accuracy 0.3542\n",
      "Epoch 59 Batch 1536 Loss 0.5182 Accuracy 0.3541\n",
      "Epoch 59 Batch 1600 Loss 0.5185 Accuracy 0.3536\n",
      "Epoch 59 Batch 1664 Loss 0.5163 Accuracy 0.3539\n",
      "Epoch 59 Batch 1728 Loss 0.5175 Accuracy 0.3538\n",
      "Epoch 59 Batch 1792 Loss 0.5170 Accuracy 0.3541\n",
      "Epoch 59 Batch 1856 Loss 0.5162 Accuracy 0.3543\n",
      "Epoch 59 Batch 1920 Loss 0.5155 Accuracy 0.3546\n",
      "Epoch 59 Batch 1984 Loss 0.5145 Accuracy 0.3545\n",
      "Epoch 59 Batch 2048 Loss 0.5151 Accuracy 0.3546\n",
      "Epoch 59 Batch 2112 Loss 0.5149 Accuracy 0.3543\n",
      "Epoch 59 Batch 2176 Loss 0.5152 Accuracy 0.3540\n",
      "Epoch 59 Batch 2240 Loss 0.5149 Accuracy 0.3542\n",
      "Epoch 59 Batch 2304 Loss 0.5174 Accuracy 0.3544\n",
      "Epoch 59 Batch 2368 Loss 0.5166 Accuracy 0.3547\n",
      "Epoch 59 Batch 2432 Loss 0.5178 Accuracy 0.3546\n",
      "Epoch 59 Batch 2496 Loss 0.5177 Accuracy 0.3547\n",
      "Epoch 59 Batch 2560 Loss 0.5171 Accuracy 0.3545\n",
      "Epoch 59 Batch 2624 Loss 0.5174 Accuracy 0.3545\n",
      "Epoch 59 Batch 2688 Loss 0.5161 Accuracy 0.3542\n",
      "Epoch 59 Batch 2752 Loss 0.5162 Accuracy 0.3541\n",
      "Epoch 59 Batch 2816 Loss 0.5154 Accuracy 0.3540\n",
      "Epoch 59 Batch 2880 Loss 0.5159 Accuracy 0.3543\n",
      "Epoch 59 Batch 2944 Loss 0.5160 Accuracy 0.3541\n",
      "Epoch 59 Batch 3008 Loss 0.5160 Accuracy 0.3542\n",
      "Epoch 59 Batch 3072 Loss 0.5152 Accuracy 0.3542\n",
      "Epoch 59 Batch 3136 Loss 0.5145 Accuracy 0.3545\n",
      "Epoch 59 Loss 0.5145 Accuracy 0.3545\n",
      "Time taken for 1 epoch: 36.898985147476196 secs\n",
      "\n",
      "Epoch 60 Batch 64 Loss 0.5005 Accuracy 0.3404\n",
      "Epoch 60 Batch 128 Loss 0.4962 Accuracy 0.3507\n",
      "Epoch 60 Batch 192 Loss 0.4982 Accuracy 0.3507\n",
      "Epoch 60 Batch 256 Loss 0.4886 Accuracy 0.3502\n",
      "Epoch 60 Batch 320 Loss 0.4909 Accuracy 0.3534\n",
      "Epoch 60 Batch 384 Loss 0.4940 Accuracy 0.3541\n",
      "Epoch 60 Batch 448 Loss 0.4959 Accuracy 0.3537\n",
      "Epoch 60 Batch 512 Loss 0.4965 Accuracy 0.3545\n",
      "Epoch 60 Batch 576 Loss 0.4995 Accuracy 0.3554\n",
      "Epoch 60 Batch 640 Loss 0.5008 Accuracy 0.3558\n",
      "Epoch 60 Batch 704 Loss 0.4991 Accuracy 0.3569\n",
      "Epoch 60 Batch 768 Loss 0.5025 Accuracy 0.3565\n",
      "Epoch 60 Batch 832 Loss 0.5029 Accuracy 0.3567\n",
      "Epoch 60 Batch 896 Loss 0.5016 Accuracy 0.3565\n",
      "Epoch 60 Batch 960 Loss 0.5035 Accuracy 0.3567\n",
      "Epoch 60 Batch 1024 Loss 0.5065 Accuracy 0.3561\n",
      "Epoch 60 Batch 1088 Loss 0.5053 Accuracy 0.3563\n",
      "Epoch 60 Batch 1152 Loss 0.5088 Accuracy 0.3562\n",
      "Epoch 60 Batch 1216 Loss 0.5065 Accuracy 0.3563\n",
      "Epoch 60 Batch 1280 Loss 0.5057 Accuracy 0.3562\n",
      "Epoch 60 Batch 1344 Loss 0.5077 Accuracy 0.3560\n",
      "Epoch 60 Batch 1408 Loss 0.5096 Accuracy 0.3560\n",
      "Epoch 60 Batch 1472 Loss 0.5114 Accuracy 0.3556\n",
      "Epoch 60 Batch 1536 Loss 0.5099 Accuracy 0.3560\n",
      "Epoch 60 Batch 1600 Loss 0.5107 Accuracy 0.3552\n",
      "Epoch 60 Batch 1664 Loss 0.5104 Accuracy 0.3555\n",
      "Epoch 60 Batch 1728 Loss 0.5082 Accuracy 0.3558\n",
      "Epoch 60 Batch 1792 Loss 0.5085 Accuracy 0.3562\n",
      "Epoch 60 Batch 1856 Loss 0.5073 Accuracy 0.3563\n",
      "Epoch 60 Batch 1920 Loss 0.5087 Accuracy 0.3556\n",
      "Epoch 60 Batch 1984 Loss 0.5095 Accuracy 0.3560\n",
      "Epoch 60 Batch 2048 Loss 0.5087 Accuracy 0.3561\n",
      "Epoch 60 Batch 2112 Loss 0.5083 Accuracy 0.3562\n",
      "Epoch 60 Batch 2176 Loss 0.5081 Accuracy 0.3565\n",
      "Epoch 60 Batch 2240 Loss 0.5072 Accuracy 0.3565\n",
      "Epoch 60 Batch 2304 Loss 0.5083 Accuracy 0.3561\n",
      "Epoch 60 Batch 2368 Loss 0.5073 Accuracy 0.3564\n",
      "Epoch 60 Batch 2432 Loss 0.5072 Accuracy 0.3562\n",
      "Epoch 60 Batch 2496 Loss 0.5072 Accuracy 0.3563\n",
      "Epoch 60 Batch 2560 Loss 0.5077 Accuracy 0.3565\n",
      "Epoch 60 Batch 2624 Loss 0.5076 Accuracy 0.3564\n",
      "Epoch 60 Batch 2688 Loss 0.5087 Accuracy 0.3562\n",
      "Epoch 60 Batch 2752 Loss 0.5094 Accuracy 0.3561\n",
      "Epoch 60 Batch 2816 Loss 0.5088 Accuracy 0.3561\n",
      "Epoch 60 Batch 2880 Loss 0.5086 Accuracy 0.3563\n",
      "Epoch 60 Batch 2944 Loss 0.5076 Accuracy 0.3563\n",
      "Epoch 60 Batch 3008 Loss 0.5066 Accuracy 0.3566\n",
      "Epoch 60 Batch 3072 Loss 0.5067 Accuracy 0.3563\n",
      "Epoch 60 Batch 3136 Loss 0.5072 Accuracy 0.3561\n",
      "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
      "Epoch 60 Loss 0.5072 Accuracy 0.3561\n",
      "Time taken for 1 epoch: 36.287272930145264 secs\n",
      "\n",
      "Epoch 61 Batch 64 Loss 0.5019 Accuracy 0.3468\n",
      "Epoch 61 Batch 128 Loss 0.4957 Accuracy 0.3427\n",
      "Epoch 61 Batch 192 Loss 0.4874 Accuracy 0.3501\n",
      "Epoch 61 Batch 256 Loss 0.4855 Accuracy 0.3535\n",
      "Epoch 61 Batch 320 Loss 0.4909 Accuracy 0.3518\n",
      "Epoch 61 Batch 384 Loss 0.4928 Accuracy 0.3532\n",
      "Epoch 61 Batch 448 Loss 0.5007 Accuracy 0.3524\n",
      "Epoch 61 Batch 512 Loss 0.5074 Accuracy 0.3524\n",
      "Epoch 61 Batch 576 Loss 0.5072 Accuracy 0.3512\n",
      "Epoch 61 Batch 640 Loss 0.5045 Accuracy 0.3530\n",
      "Epoch 61 Batch 704 Loss 0.5032 Accuracy 0.3529\n",
      "Epoch 61 Batch 768 Loss 0.5021 Accuracy 0.3522\n",
      "Epoch 61 Batch 832 Loss 0.4996 Accuracy 0.3528\n",
      "Epoch 61 Batch 896 Loss 0.5015 Accuracy 0.3522\n",
      "Epoch 61 Batch 960 Loss 0.5039 Accuracy 0.3525\n",
      "Epoch 61 Batch 1024 Loss 0.5043 Accuracy 0.3531\n",
      "Epoch 61 Batch 1088 Loss 0.5055 Accuracy 0.3531\n",
      "Epoch 61 Batch 1152 Loss 0.5056 Accuracy 0.3533\n",
      "Epoch 61 Batch 1216 Loss 0.5050 Accuracy 0.3531\n",
      "Epoch 61 Batch 1280 Loss 0.5046 Accuracy 0.3534\n",
      "Epoch 61 Batch 1344 Loss 0.5048 Accuracy 0.3532\n",
      "Epoch 61 Batch 1408 Loss 0.5053 Accuracy 0.3530\n",
      "Epoch 61 Batch 1472 Loss 0.5051 Accuracy 0.3530\n",
      "Epoch 61 Batch 1536 Loss 0.5070 Accuracy 0.3530\n",
      "Epoch 61 Batch 1600 Loss 0.5080 Accuracy 0.3529\n",
      "Epoch 61 Batch 1664 Loss 0.5066 Accuracy 0.3536\n",
      "Epoch 61 Batch 1728 Loss 0.5066 Accuracy 0.3537\n",
      "Epoch 61 Batch 1792 Loss 0.5049 Accuracy 0.3536\n",
      "Epoch 61 Batch 1856 Loss 0.5052 Accuracy 0.3534\n",
      "Epoch 61 Batch 1920 Loss 0.5047 Accuracy 0.3538\n",
      "Epoch 61 Batch 1984 Loss 0.5048 Accuracy 0.3540\n",
      "Epoch 61 Batch 2048 Loss 0.5051 Accuracy 0.3536\n",
      "Epoch 61 Batch 2112 Loss 0.5043 Accuracy 0.3538\n",
      "Epoch 61 Batch 2176 Loss 0.5043 Accuracy 0.3539\n",
      "Epoch 61 Batch 2240 Loss 0.5038 Accuracy 0.3544\n",
      "Epoch 61 Batch 2304 Loss 0.5029 Accuracy 0.3545\n",
      "Epoch 61 Batch 2368 Loss 0.5033 Accuracy 0.3547\n",
      "Epoch 61 Batch 2432 Loss 0.5020 Accuracy 0.3548\n",
      "Epoch 61 Batch 2496 Loss 0.5014 Accuracy 0.3548\n",
      "Epoch 61 Batch 2560 Loss 0.5004 Accuracy 0.3556\n",
      "Epoch 61 Batch 2624 Loss 0.4994 Accuracy 0.3556\n",
      "Epoch 61 Batch 2688 Loss 0.4991 Accuracy 0.3559\n",
      "Epoch 61 Batch 2752 Loss 0.4982 Accuracy 0.3563\n",
      "Epoch 61 Batch 2816 Loss 0.4982 Accuracy 0.3564\n",
      "Epoch 61 Batch 2880 Loss 0.4984 Accuracy 0.3563\n",
      "Epoch 61 Batch 2944 Loss 0.4978 Accuracy 0.3567\n",
      "Epoch 61 Batch 3008 Loss 0.4980 Accuracy 0.3567\n",
      "Epoch 61 Batch 3072 Loss 0.4986 Accuracy 0.3569\n",
      "Epoch 61 Batch 3136 Loss 0.4979 Accuracy 0.3569\n",
      "Epoch 61 Loss 0.4979 Accuracy 0.3569\n",
      "Time taken for 1 epoch: 35.83592915534973 secs\n",
      "\n",
      "Epoch 62 Batch 64 Loss 0.4853 Accuracy 0.3692\n",
      "Epoch 62 Batch 128 Loss 0.4977 Accuracy 0.3664\n",
      "Epoch 62 Batch 192 Loss 0.4963 Accuracy 0.3639\n",
      "Epoch 62 Batch 256 Loss 0.4997 Accuracy 0.3616\n",
      "Epoch 62 Batch 320 Loss 0.4947 Accuracy 0.3585\n",
      "Epoch 62 Batch 384 Loss 0.4999 Accuracy 0.3571\n",
      "Epoch 62 Batch 448 Loss 0.4986 Accuracy 0.3584\n",
      "Epoch 62 Batch 512 Loss 0.5035 Accuracy 0.3578\n",
      "Epoch 62 Batch 576 Loss 0.5055 Accuracy 0.3585\n",
      "Epoch 62 Batch 640 Loss 0.5028 Accuracy 0.3570\n",
      "Epoch 62 Batch 704 Loss 0.4983 Accuracy 0.3570\n",
      "Epoch 62 Batch 768 Loss 0.4976 Accuracy 0.3582\n",
      "Epoch 62 Batch 832 Loss 0.4938 Accuracy 0.3587\n",
      "Epoch 62 Batch 896 Loss 0.4937 Accuracy 0.3588\n",
      "Epoch 62 Batch 960 Loss 0.4948 Accuracy 0.3585\n",
      "Epoch 62 Batch 1024 Loss 0.4916 Accuracy 0.3594\n",
      "Epoch 62 Batch 1088 Loss 0.4918 Accuracy 0.3600\n",
      "Epoch 62 Batch 1152 Loss 0.4942 Accuracy 0.3603\n",
      "Epoch 62 Batch 1216 Loss 0.4927 Accuracy 0.3605\n",
      "Epoch 62 Batch 1280 Loss 0.4921 Accuracy 0.3595\n",
      "Epoch 62 Batch 1344 Loss 0.4930 Accuracy 0.3596\n",
      "Epoch 62 Batch 1408 Loss 0.4948 Accuracy 0.3596\n",
      "Epoch 62 Batch 1472 Loss 0.4951 Accuracy 0.3587\n",
      "Epoch 62 Batch 1536 Loss 0.4958 Accuracy 0.3590\n",
      "Epoch 62 Batch 1600 Loss 0.4941 Accuracy 0.3598\n",
      "Epoch 62 Batch 1664 Loss 0.4937 Accuracy 0.3597\n",
      "Epoch 62 Batch 1728 Loss 0.4924 Accuracy 0.3604\n",
      "Epoch 62 Batch 1792 Loss 0.4905 Accuracy 0.3601\n",
      "Epoch 62 Batch 1856 Loss 0.4897 Accuracy 0.3599\n",
      "Epoch 62 Batch 1920 Loss 0.4885 Accuracy 0.3600\n",
      "Epoch 62 Batch 1984 Loss 0.4895 Accuracy 0.3595\n",
      "Epoch 62 Batch 2048 Loss 0.4895 Accuracy 0.3597\n",
      "Epoch 62 Batch 2112 Loss 0.4887 Accuracy 0.3598\n",
      "Epoch 62 Batch 2176 Loss 0.4900 Accuracy 0.3598\n",
      "Epoch 62 Batch 2240 Loss 0.4909 Accuracy 0.3598\n",
      "Epoch 62 Batch 2304 Loss 0.4896 Accuracy 0.3601\n",
      "Epoch 62 Batch 2368 Loss 0.4890 Accuracy 0.3597\n",
      "Epoch 62 Batch 2432 Loss 0.4888 Accuracy 0.3594\n",
      "Epoch 62 Batch 2496 Loss 0.4885 Accuracy 0.3592\n",
      "Epoch 62 Batch 2560 Loss 0.4877 Accuracy 0.3589\n",
      "Epoch 62 Batch 2624 Loss 0.4873 Accuracy 0.3592\n",
      "Epoch 62 Batch 2688 Loss 0.4878 Accuracy 0.3589\n",
      "Epoch 62 Batch 2752 Loss 0.4872 Accuracy 0.3589\n",
      "Epoch 62 Batch 2816 Loss 0.4878 Accuracy 0.3588\n",
      "Epoch 62 Batch 2880 Loss 0.4866 Accuracy 0.3590\n",
      "Epoch 62 Batch 2944 Loss 0.4862 Accuracy 0.3590\n",
      "Epoch 62 Batch 3008 Loss 0.4867 Accuracy 0.3588\n",
      "Epoch 62 Batch 3072 Loss 0.4852 Accuracy 0.3587\n",
      "Epoch 62 Batch 3136 Loss 0.4855 Accuracy 0.3587\n",
      "Epoch 62 Loss 0.4855 Accuracy 0.3587\n",
      "Time taken for 1 epoch: 36.145756244659424 secs\n",
      "\n",
      "Epoch 63 Batch 64 Loss 0.4958 Accuracy 0.3642\n",
      "Epoch 63 Batch 128 Loss 0.4945 Accuracy 0.3575\n",
      "Epoch 63 Batch 192 Loss 0.4887 Accuracy 0.3568\n",
      "Epoch 63 Batch 256 Loss 0.4905 Accuracy 0.3536\n",
      "Epoch 63 Batch 320 Loss 0.4838 Accuracy 0.3550\n",
      "Epoch 63 Batch 384 Loss 0.4823 Accuracy 0.3560\n",
      "Epoch 63 Batch 448 Loss 0.4869 Accuracy 0.3553\n",
      "Epoch 63 Batch 512 Loss 0.4827 Accuracy 0.3566\n",
      "Epoch 63 Batch 576 Loss 0.4894 Accuracy 0.3552\n",
      "Epoch 63 Batch 640 Loss 0.4900 Accuracy 0.3553\n",
      "Epoch 63 Batch 704 Loss 0.4890 Accuracy 0.3554\n",
      "Epoch 63 Batch 768 Loss 0.4881 Accuracy 0.3563\n",
      "Epoch 63 Batch 832 Loss 0.4878 Accuracy 0.3570\n",
      "Epoch 63 Batch 896 Loss 0.4887 Accuracy 0.3580\n",
      "Epoch 63 Batch 960 Loss 0.4904 Accuracy 0.3579\n",
      "Epoch 63 Batch 1024 Loss 0.4878 Accuracy 0.3584\n",
      "Epoch 63 Batch 1088 Loss 0.4889 Accuracy 0.3582\n",
      "Epoch 63 Batch 1152 Loss 0.4926 Accuracy 0.3580\n",
      "Epoch 63 Batch 1216 Loss 0.4936 Accuracy 0.3579\n",
      "Epoch 63 Batch 1280 Loss 0.4935 Accuracy 0.3576\n",
      "Epoch 63 Batch 1344 Loss 0.4941 Accuracy 0.3578\n",
      "Epoch 63 Batch 1408 Loss 0.4932 Accuracy 0.3579\n",
      "Epoch 63 Batch 1472 Loss 0.4943 Accuracy 0.3576\n",
      "Epoch 63 Batch 1536 Loss 0.4935 Accuracy 0.3570\n",
      "Epoch 63 Batch 1600 Loss 0.4931 Accuracy 0.3570\n",
      "Epoch 63 Batch 1664 Loss 0.4925 Accuracy 0.3571\n",
      "Epoch 63 Batch 1728 Loss 0.4912 Accuracy 0.3572\n",
      "Epoch 63 Batch 1792 Loss 0.4924 Accuracy 0.3573\n",
      "Epoch 63 Batch 1856 Loss 0.4905 Accuracy 0.3572\n",
      "Epoch 63 Batch 1920 Loss 0.4898 Accuracy 0.3573\n",
      "Epoch 63 Batch 1984 Loss 0.4895 Accuracy 0.3573\n",
      "Epoch 63 Batch 2048 Loss 0.4887 Accuracy 0.3573\n",
      "Epoch 63 Batch 2112 Loss 0.4885 Accuracy 0.3575\n",
      "Epoch 63 Batch 2176 Loss 0.4876 Accuracy 0.3575\n",
      "Epoch 63 Batch 2240 Loss 0.4871 Accuracy 0.3578\n",
      "Epoch 63 Batch 2304 Loss 0.4871 Accuracy 0.3576\n",
      "Epoch 63 Batch 2368 Loss 0.4869 Accuracy 0.3579\n",
      "Epoch 63 Batch 2432 Loss 0.4867 Accuracy 0.3583\n",
      "Epoch 63 Batch 2496 Loss 0.4867 Accuracy 0.3583\n",
      "Epoch 63 Batch 2560 Loss 0.4863 Accuracy 0.3582\n",
      "Epoch 63 Batch 2624 Loss 0.4869 Accuracy 0.3580\n",
      "Epoch 63 Batch 2688 Loss 0.4871 Accuracy 0.3584\n",
      "Epoch 63 Batch 2752 Loss 0.4863 Accuracy 0.3584\n",
      "Epoch 63 Batch 2816 Loss 0.4861 Accuracy 0.3581\n",
      "Epoch 63 Batch 2880 Loss 0.4858 Accuracy 0.3582\n",
      "Epoch 63 Batch 2944 Loss 0.4856 Accuracy 0.3584\n",
      "Epoch 63 Batch 3008 Loss 0.4849 Accuracy 0.3585\n",
      "Epoch 63 Batch 3072 Loss 0.4854 Accuracy 0.3586\n",
      "Epoch 63 Batch 3136 Loss 0.4852 Accuracy 0.3586\n",
      "Epoch 63 Loss 0.4852 Accuracy 0.3586\n",
      "Time taken for 1 epoch: 36.23898410797119 secs\n",
      "\n",
      "Epoch 64 Batch 64 Loss 0.4403 Accuracy 0.3631\n",
      "Epoch 64 Batch 128 Loss 0.4648 Accuracy 0.3589\n",
      "Epoch 64 Batch 192 Loss 0.4690 Accuracy 0.3597\n",
      "Epoch 64 Batch 256 Loss 0.4746 Accuracy 0.3569\n",
      "Epoch 64 Batch 320 Loss 0.4720 Accuracy 0.3577\n",
      "Epoch 64 Batch 384 Loss 0.4775 Accuracy 0.3592\n",
      "Epoch 64 Batch 448 Loss 0.4678 Accuracy 0.3603\n",
      "Epoch 64 Batch 512 Loss 0.4688 Accuracy 0.3600\n",
      "Epoch 64 Batch 576 Loss 0.4669 Accuracy 0.3603\n",
      "Epoch 64 Batch 640 Loss 0.4656 Accuracy 0.3610\n",
      "Epoch 64 Batch 704 Loss 0.4664 Accuracy 0.3607\n",
      "Epoch 64 Batch 768 Loss 0.4620 Accuracy 0.3628\n",
      "Epoch 64 Batch 832 Loss 0.4603 Accuracy 0.3621\n",
      "Epoch 64 Batch 896 Loss 0.4608 Accuracy 0.3604\n",
      "Epoch 64 Batch 960 Loss 0.4595 Accuracy 0.3612\n",
      "Epoch 64 Batch 1024 Loss 0.4607 Accuracy 0.3612\n",
      "Epoch 64 Batch 1088 Loss 0.4584 Accuracy 0.3628\n",
      "Epoch 64 Batch 1152 Loss 0.4600 Accuracy 0.3620\n",
      "Epoch 64 Batch 1216 Loss 0.4589 Accuracy 0.3624\n",
      "Epoch 64 Batch 1280 Loss 0.4588 Accuracy 0.3629\n",
      "Epoch 64 Batch 1344 Loss 0.4584 Accuracy 0.3636\n",
      "Epoch 64 Batch 1408 Loss 0.4588 Accuracy 0.3634\n",
      "Epoch 64 Batch 1472 Loss 0.4568 Accuracy 0.3633\n",
      "Epoch 64 Batch 1536 Loss 0.4592 Accuracy 0.3635\n",
      "Epoch 64 Batch 1600 Loss 0.4602 Accuracy 0.3636\n",
      "Epoch 64 Batch 1664 Loss 0.4615 Accuracy 0.3631\n",
      "Epoch 64 Batch 1728 Loss 0.4625 Accuracy 0.3629\n",
      "Epoch 64 Batch 1792 Loss 0.4620 Accuracy 0.3633\n",
      "Epoch 64 Batch 1856 Loss 0.4625 Accuracy 0.3633\n",
      "Epoch 64 Batch 1920 Loss 0.4624 Accuracy 0.3635\n",
      "Epoch 64 Batch 1984 Loss 0.4620 Accuracy 0.3634\n",
      "Epoch 64 Batch 2048 Loss 0.4613 Accuracy 0.3633\n",
      "Epoch 64 Batch 2112 Loss 0.4621 Accuracy 0.3630\n",
      "Epoch 64 Batch 2176 Loss 0.4614 Accuracy 0.3630\n",
      "Epoch 64 Batch 2240 Loss 0.4633 Accuracy 0.3628\n",
      "Epoch 64 Batch 2304 Loss 0.4644 Accuracy 0.3627\n",
      "Epoch 64 Batch 2368 Loss 0.4645 Accuracy 0.3626\n",
      "Epoch 64 Batch 2432 Loss 0.4639 Accuracy 0.3628\n",
      "Epoch 64 Batch 2496 Loss 0.4630 Accuracy 0.3628\n",
      "Epoch 64 Batch 2560 Loss 0.4638 Accuracy 0.3627\n",
      "Epoch 64 Batch 2624 Loss 0.4644 Accuracy 0.3625\n",
      "Epoch 64 Batch 2688 Loss 0.4652 Accuracy 0.3628\n",
      "Epoch 64 Batch 2752 Loss 0.4646 Accuracy 0.3624\n",
      "Epoch 64 Batch 2816 Loss 0.4658 Accuracy 0.3625\n",
      "Epoch 64 Batch 2880 Loss 0.4653 Accuracy 0.3622\n",
      "Epoch 64 Batch 2944 Loss 0.4648 Accuracy 0.3621\n",
      "Epoch 64 Batch 3008 Loss 0.4647 Accuracy 0.3617\n",
      "Epoch 64 Batch 3072 Loss 0.4649 Accuracy 0.3619\n",
      "Epoch 64 Batch 3136 Loss 0.4648 Accuracy 0.3618\n",
      "Epoch 64 Loss 0.4648 Accuracy 0.3618\n",
      "Time taken for 1 epoch: 39.44526696205139 secs\n",
      "\n",
      "Epoch 65 Batch 64 Loss 0.4545 Accuracy 0.3551\n",
      "Epoch 65 Batch 128 Loss 0.4525 Accuracy 0.3610\n",
      "Epoch 65 Batch 192 Loss 0.4690 Accuracy 0.3640\n",
      "Epoch 65 Batch 256 Loss 0.4704 Accuracy 0.3605\n",
      "Epoch 65 Batch 320 Loss 0.4627 Accuracy 0.3591\n",
      "Epoch 65 Batch 384 Loss 0.4676 Accuracy 0.3610\n",
      "Epoch 65 Batch 448 Loss 0.4706 Accuracy 0.3620\n",
      "Epoch 65 Batch 512 Loss 0.4692 Accuracy 0.3618\n",
      "Epoch 65 Batch 576 Loss 0.4594 Accuracy 0.3617\n",
      "Epoch 65 Batch 640 Loss 0.4593 Accuracy 0.3610\n",
      "Epoch 65 Batch 704 Loss 0.4563 Accuracy 0.3601\n",
      "Epoch 65 Batch 768 Loss 0.4582 Accuracy 0.3583\n",
      "Epoch 65 Batch 832 Loss 0.4573 Accuracy 0.3588\n",
      "Epoch 65 Batch 896 Loss 0.4608 Accuracy 0.3591\n",
      "Epoch 65 Batch 960 Loss 0.4611 Accuracy 0.3600\n",
      "Epoch 65 Batch 1024 Loss 0.4601 Accuracy 0.3601\n",
      "Epoch 65 Batch 1088 Loss 0.4564 Accuracy 0.3614\n",
      "Epoch 65 Batch 1152 Loss 0.4554 Accuracy 0.3618\n",
      "Epoch 65 Batch 1216 Loss 0.4554 Accuracy 0.3622\n",
      "Epoch 65 Batch 1280 Loss 0.4550 Accuracy 0.3617\n",
      "Epoch 65 Batch 1344 Loss 0.4549 Accuracy 0.3618\n",
      "Epoch 65 Batch 1408 Loss 0.4572 Accuracy 0.3615\n",
      "Epoch 65 Batch 1472 Loss 0.4566 Accuracy 0.3616\n",
      "Epoch 65 Batch 1536 Loss 0.4549 Accuracy 0.3618\n",
      "Epoch 65 Batch 1600 Loss 0.4540 Accuracy 0.3619\n",
      "Epoch 65 Batch 1664 Loss 0.4530 Accuracy 0.3624\n",
      "Epoch 65 Batch 1728 Loss 0.4533 Accuracy 0.3625\n",
      "Epoch 65 Batch 1792 Loss 0.4531 Accuracy 0.3628\n",
      "Epoch 65 Batch 1856 Loss 0.4546 Accuracy 0.3625\n",
      "Epoch 65 Batch 1920 Loss 0.4546 Accuracy 0.3632\n",
      "Epoch 65 Batch 1984 Loss 0.4552 Accuracy 0.3630\n",
      "Epoch 65 Batch 2048 Loss 0.4562 Accuracy 0.3630\n",
      "Epoch 65 Batch 2112 Loss 0.4556 Accuracy 0.3627\n",
      "Epoch 65 Batch 2176 Loss 0.4549 Accuracy 0.3624\n",
      "Epoch 65 Batch 2240 Loss 0.4565 Accuracy 0.3623\n",
      "Epoch 65 Batch 2304 Loss 0.4567 Accuracy 0.3623\n",
      "Epoch 65 Batch 2368 Loss 0.4569 Accuracy 0.3622\n",
      "Epoch 65 Batch 2432 Loss 0.4569 Accuracy 0.3622\n",
      "Epoch 65 Batch 2496 Loss 0.4572 Accuracy 0.3621\n",
      "Epoch 65 Batch 2560 Loss 0.4581 Accuracy 0.3618\n",
      "Epoch 65 Batch 2624 Loss 0.4576 Accuracy 0.3613\n",
      "Epoch 65 Batch 2688 Loss 0.4574 Accuracy 0.3611\n",
      "Epoch 65 Batch 2752 Loss 0.4573 Accuracy 0.3611\n",
      "Epoch 65 Batch 2816 Loss 0.4580 Accuracy 0.3611\n",
      "Epoch 65 Batch 2880 Loss 0.4574 Accuracy 0.3612\n",
      "Epoch 65 Batch 2944 Loss 0.4567 Accuracy 0.3616\n",
      "Epoch 65 Batch 3008 Loss 0.4568 Accuracy 0.3619\n",
      "Epoch 65 Batch 3072 Loss 0.4567 Accuracy 0.3618\n",
      "Epoch 65 Batch 3136 Loss 0.4563 Accuracy 0.3621\n",
      "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
      "Epoch 65 Loss 0.4563 Accuracy 0.3621\n",
      "Time taken for 1 epoch: 36.77017021179199 secs\n",
      "\n",
      "Epoch 66 Batch 64 Loss 0.4712 Accuracy 0.3586\n",
      "Epoch 66 Batch 128 Loss 0.4460 Accuracy 0.3590\n",
      "Epoch 66 Batch 192 Loss 0.4390 Accuracy 0.3610\n",
      "Epoch 66 Batch 256 Loss 0.4408 Accuracy 0.3612\n",
      "Epoch 66 Batch 320 Loss 0.4488 Accuracy 0.3601\n",
      "Epoch 66 Batch 384 Loss 0.4472 Accuracy 0.3606\n",
      "Epoch 66 Batch 448 Loss 0.4514 Accuracy 0.3607\n",
      "Epoch 66 Batch 512 Loss 0.4505 Accuracy 0.3612\n",
      "Epoch 66 Batch 576 Loss 0.4532 Accuracy 0.3608\n",
      "Epoch 66 Batch 640 Loss 0.4512 Accuracy 0.3605\n",
      "Epoch 66 Batch 704 Loss 0.4490 Accuracy 0.3609\n",
      "Epoch 66 Batch 768 Loss 0.4507 Accuracy 0.3615\n",
      "Epoch 66 Batch 832 Loss 0.4533 Accuracy 0.3612\n",
      "Epoch 66 Batch 896 Loss 0.4529 Accuracy 0.3615\n",
      "Epoch 66 Batch 960 Loss 0.4493 Accuracy 0.3626\n",
      "Epoch 66 Batch 1024 Loss 0.4497 Accuracy 0.3623\n",
      "Epoch 66 Batch 1088 Loss 0.4505 Accuracy 0.3626\n",
      "Epoch 66 Batch 1152 Loss 0.4507 Accuracy 0.3619\n",
      "Epoch 66 Batch 1216 Loss 0.4500 Accuracy 0.3621\n",
      "Epoch 66 Batch 1280 Loss 0.4508 Accuracy 0.3623\n",
      "Epoch 66 Batch 1344 Loss 0.4514 Accuracy 0.3622\n",
      "Epoch 66 Batch 1408 Loss 0.4509 Accuracy 0.3630\n",
      "Epoch 66 Batch 1472 Loss 0.4500 Accuracy 0.3637\n",
      "Epoch 66 Batch 1536 Loss 0.4483 Accuracy 0.3649\n",
      "Epoch 66 Batch 1600 Loss 0.4493 Accuracy 0.3646\n",
      "Epoch 66 Batch 1664 Loss 0.4492 Accuracy 0.3646\n",
      "Epoch 66 Batch 1728 Loss 0.4474 Accuracy 0.3648\n",
      "Epoch 66 Batch 1792 Loss 0.4484 Accuracy 0.3649\n",
      "Epoch 66 Batch 1856 Loss 0.4483 Accuracy 0.3651\n",
      "Epoch 66 Batch 1920 Loss 0.4472 Accuracy 0.3652\n",
      "Epoch 66 Batch 1984 Loss 0.4476 Accuracy 0.3655\n",
      "Epoch 66 Batch 2048 Loss 0.4470 Accuracy 0.3655\n",
      "Epoch 66 Batch 2112 Loss 0.4457 Accuracy 0.3655\n",
      "Epoch 66 Batch 2176 Loss 0.4455 Accuracy 0.3655\n",
      "Epoch 66 Batch 2240 Loss 0.4460 Accuracy 0.3655\n",
      "Epoch 66 Batch 2304 Loss 0.4457 Accuracy 0.3653\n",
      "Epoch 66 Batch 2368 Loss 0.4456 Accuracy 0.3654\n",
      "Epoch 66 Batch 2432 Loss 0.4455 Accuracy 0.3654\n",
      "Epoch 66 Batch 2496 Loss 0.4446 Accuracy 0.3654\n",
      "Epoch 66 Batch 2560 Loss 0.4435 Accuracy 0.3654\n",
      "Epoch 66 Batch 2624 Loss 0.4435 Accuracy 0.3652\n",
      "Epoch 66 Batch 2688 Loss 0.4412 Accuracy 0.3654\n",
      "Epoch 66 Batch 2752 Loss 0.4400 Accuracy 0.3657\n",
      "Epoch 66 Batch 2816 Loss 0.4410 Accuracy 0.3656\n",
      "Epoch 66 Batch 2880 Loss 0.4410 Accuracy 0.3652\n",
      "Epoch 66 Batch 2944 Loss 0.4411 Accuracy 0.3651\n",
      "Epoch 66 Batch 3008 Loss 0.4412 Accuracy 0.3656\n",
      "Epoch 66 Batch 3072 Loss 0.4416 Accuracy 0.3655\n",
      "Epoch 66 Batch 3136 Loss 0.4411 Accuracy 0.3652\n",
      "Epoch 66 Loss 0.4411 Accuracy 0.3652\n",
      "Time taken for 1 epoch: 36.01355504989624 secs\n",
      "\n",
      "Epoch 67 Batch 64 Loss 0.4781 Accuracy 0.3591\n",
      "Epoch 67 Batch 128 Loss 0.4640 Accuracy 0.3656\n",
      "Epoch 67 Batch 192 Loss 0.4421 Accuracy 0.3663\n",
      "Epoch 67 Batch 256 Loss 0.4367 Accuracy 0.3631\n",
      "Epoch 67 Batch 320 Loss 0.4388 Accuracy 0.3635\n",
      "Epoch 67 Batch 384 Loss 0.4329 Accuracy 0.3649\n",
      "Epoch 67 Batch 448 Loss 0.4309 Accuracy 0.3650\n",
      "Epoch 67 Batch 512 Loss 0.4279 Accuracy 0.3642\n",
      "Epoch 67 Batch 576 Loss 0.4314 Accuracy 0.3642\n",
      "Epoch 67 Batch 640 Loss 0.4282 Accuracy 0.3649\n",
      "Epoch 67 Batch 704 Loss 0.4276 Accuracy 0.3653\n",
      "Epoch 67 Batch 768 Loss 0.4285 Accuracy 0.3646\n",
      "Epoch 67 Batch 832 Loss 0.4298 Accuracy 0.3636\n",
      "Epoch 67 Batch 896 Loss 0.4339 Accuracy 0.3635\n",
      "Epoch 67 Batch 960 Loss 0.4362 Accuracy 0.3635\n",
      "Epoch 67 Batch 1024 Loss 0.4350 Accuracy 0.3640\n",
      "Epoch 67 Batch 1088 Loss 0.4358 Accuracy 0.3640\n",
      "Epoch 67 Batch 1152 Loss 0.4351 Accuracy 0.3643\n",
      "Epoch 67 Batch 1216 Loss 0.4349 Accuracy 0.3649\n",
      "Epoch 67 Batch 1280 Loss 0.4331 Accuracy 0.3645\n",
      "Epoch 67 Batch 1344 Loss 0.4333 Accuracy 0.3655\n",
      "Epoch 67 Batch 1408 Loss 0.4359 Accuracy 0.3653\n",
      "Epoch 67 Batch 1472 Loss 0.4335 Accuracy 0.3654\n",
      "Epoch 67 Batch 1536 Loss 0.4364 Accuracy 0.3655\n",
      "Epoch 67 Batch 1600 Loss 0.4361 Accuracy 0.3652\n",
      "Epoch 67 Batch 1664 Loss 0.4361 Accuracy 0.3653\n",
      "Epoch 67 Batch 1728 Loss 0.4351 Accuracy 0.3654\n",
      "Epoch 67 Batch 1792 Loss 0.4348 Accuracy 0.3651\n",
      "Epoch 67 Batch 1856 Loss 0.4365 Accuracy 0.3650\n",
      "Epoch 67 Batch 1920 Loss 0.4381 Accuracy 0.3649\n",
      "Epoch 67 Batch 1984 Loss 0.4385 Accuracy 0.3645\n",
      "Epoch 67 Batch 2048 Loss 0.4397 Accuracy 0.3644\n",
      "Epoch 67 Batch 2112 Loss 0.4390 Accuracy 0.3641\n",
      "Epoch 67 Batch 2176 Loss 0.4389 Accuracy 0.3638\n",
      "Epoch 67 Batch 2240 Loss 0.4403 Accuracy 0.3640\n",
      "Epoch 67 Batch 2304 Loss 0.4406 Accuracy 0.3641\n",
      "Epoch 67 Batch 2368 Loss 0.4425 Accuracy 0.3640\n",
      "Epoch 67 Batch 2432 Loss 0.4427 Accuracy 0.3641\n",
      "Epoch 67 Batch 2496 Loss 0.4425 Accuracy 0.3642\n",
      "Epoch 67 Batch 2560 Loss 0.4428 Accuracy 0.3646\n",
      "Epoch 67 Batch 2624 Loss 0.4422 Accuracy 0.3644\n",
      "Epoch 67 Batch 2688 Loss 0.4425 Accuracy 0.3642\n",
      "Epoch 67 Batch 2752 Loss 0.4418 Accuracy 0.3647\n",
      "Epoch 67 Batch 2816 Loss 0.4412 Accuracy 0.3648\n",
      "Epoch 67 Batch 2880 Loss 0.4412 Accuracy 0.3648\n",
      "Epoch 67 Batch 2944 Loss 0.4412 Accuracy 0.3650\n",
      "Epoch 67 Batch 3008 Loss 0.4410 Accuracy 0.3648\n",
      "Epoch 67 Batch 3072 Loss 0.4417 Accuracy 0.3649\n",
      "Epoch 67 Batch 3136 Loss 0.4408 Accuracy 0.3649\n",
      "Epoch 67 Loss 0.4408 Accuracy 0.3649\n",
      "Time taken for 1 epoch: 35.89654588699341 secs\n",
      "\n",
      "Epoch 68 Batch 64 Loss 0.3738 Accuracy 0.3820\n",
      "Epoch 68 Batch 128 Loss 0.4155 Accuracy 0.3750\n",
      "Epoch 68 Batch 192 Loss 0.4202 Accuracy 0.3712\n",
      "Epoch 68 Batch 256 Loss 0.4112 Accuracy 0.3729\n",
      "Epoch 68 Batch 320 Loss 0.4163 Accuracy 0.3704\n",
      "Epoch 68 Batch 384 Loss 0.4169 Accuracy 0.3713\n",
      "Epoch 68 Batch 448 Loss 0.4217 Accuracy 0.3717\n",
      "Epoch 68 Batch 512 Loss 0.4178 Accuracy 0.3734\n",
      "Epoch 68 Batch 576 Loss 0.4194 Accuracy 0.3732\n",
      "Epoch 68 Batch 640 Loss 0.4220 Accuracy 0.3726\n",
      "Epoch 68 Batch 704 Loss 0.4193 Accuracy 0.3725\n",
      "Epoch 68 Batch 768 Loss 0.4204 Accuracy 0.3712\n",
      "Epoch 68 Batch 832 Loss 0.4218 Accuracy 0.3714\n",
      "Epoch 68 Batch 896 Loss 0.4196 Accuracy 0.3719\n",
      "Epoch 68 Batch 960 Loss 0.4220 Accuracy 0.3718\n",
      "Epoch 68 Batch 1024 Loss 0.4219 Accuracy 0.3714\n",
      "Epoch 68 Batch 1088 Loss 0.4216 Accuracy 0.3716\n",
      "Epoch 68 Batch 1152 Loss 0.4182 Accuracy 0.3712\n",
      "Epoch 68 Batch 1216 Loss 0.4166 Accuracy 0.3709\n",
      "Epoch 68 Batch 1280 Loss 0.4177 Accuracy 0.3702\n",
      "Epoch 68 Batch 1344 Loss 0.4161 Accuracy 0.3697\n",
      "Epoch 68 Batch 1408 Loss 0.4172 Accuracy 0.3691\n",
      "Epoch 68 Batch 1472 Loss 0.4175 Accuracy 0.3689\n",
      "Epoch 68 Batch 1536 Loss 0.4183 Accuracy 0.3684\n",
      "Epoch 68 Batch 1600 Loss 0.4189 Accuracy 0.3688\n",
      "Epoch 68 Batch 1664 Loss 0.4202 Accuracy 0.3688\n",
      "Epoch 68 Batch 1728 Loss 0.4215 Accuracy 0.3685\n",
      "Epoch 68 Batch 1792 Loss 0.4221 Accuracy 0.3680\n",
      "Epoch 68 Batch 1856 Loss 0.4223 Accuracy 0.3682\n",
      "Epoch 68 Batch 1920 Loss 0.4226 Accuracy 0.3679\n",
      "Epoch 68 Batch 1984 Loss 0.4228 Accuracy 0.3677\n",
      "Epoch 68 Batch 2048 Loss 0.4233 Accuracy 0.3676\n",
      "Epoch 68 Batch 2112 Loss 0.4239 Accuracy 0.3679\n",
      "Epoch 68 Batch 2176 Loss 0.4234 Accuracy 0.3679\n",
      "Epoch 68 Batch 2240 Loss 0.4223 Accuracy 0.3674\n",
      "Epoch 68 Batch 2304 Loss 0.4229 Accuracy 0.3672\n",
      "Epoch 68 Batch 2368 Loss 0.4238 Accuracy 0.3673\n",
      "Epoch 68 Batch 2432 Loss 0.4229 Accuracy 0.3672\n",
      "Epoch 68 Batch 2496 Loss 0.4220 Accuracy 0.3674\n",
      "Epoch 68 Batch 2560 Loss 0.4235 Accuracy 0.3673\n",
      "Epoch 68 Batch 2624 Loss 0.4231 Accuracy 0.3674\n",
      "Epoch 68 Batch 2688 Loss 0.4234 Accuracy 0.3676\n",
      "Epoch 68 Batch 2752 Loss 0.4241 Accuracy 0.3672\n",
      "Epoch 68 Batch 2816 Loss 0.4235 Accuracy 0.3671\n",
      "Epoch 68 Batch 2880 Loss 0.4230 Accuracy 0.3674\n",
      "Epoch 68 Batch 2944 Loss 0.4233 Accuracy 0.3674\n",
      "Epoch 68 Batch 3008 Loss 0.4230 Accuracy 0.3673\n",
      "Epoch 68 Batch 3072 Loss 0.4221 Accuracy 0.3677\n",
      "Epoch 68 Batch 3136 Loss 0.4223 Accuracy 0.3677\n",
      "Epoch 68 Loss 0.4223 Accuracy 0.3677\n",
      "Time taken for 1 epoch: 35.78603196144104 secs\n",
      "\n",
      "Epoch 69 Batch 64 Loss 0.4095 Accuracy 0.3722\n",
      "Epoch 69 Batch 128 Loss 0.3958 Accuracy 0.3779\n",
      "Epoch 69 Batch 192 Loss 0.3970 Accuracy 0.3758\n",
      "Epoch 69 Batch 256 Loss 0.4027 Accuracy 0.3724\n",
      "Epoch 69 Batch 320 Loss 0.4065 Accuracy 0.3701\n",
      "Epoch 69 Batch 384 Loss 0.4146 Accuracy 0.3687\n",
      "Epoch 69 Batch 448 Loss 0.4138 Accuracy 0.3667\n",
      "Epoch 69 Batch 512 Loss 0.4182 Accuracy 0.3665\n",
      "Epoch 69 Batch 576 Loss 0.4232 Accuracy 0.3679\n",
      "Epoch 69 Batch 640 Loss 0.4189 Accuracy 0.3701\n",
      "Epoch 69 Batch 704 Loss 0.4214 Accuracy 0.3704\n",
      "Epoch 69 Batch 768 Loss 0.4227 Accuracy 0.3687\n",
      "Epoch 69 Batch 832 Loss 0.4207 Accuracy 0.3686\n",
      "Epoch 69 Batch 896 Loss 0.4212 Accuracy 0.3679\n",
      "Epoch 69 Batch 960 Loss 0.4203 Accuracy 0.3676\n",
      "Epoch 69 Batch 1024 Loss 0.4213 Accuracy 0.3676\n",
      "Epoch 69 Batch 1088 Loss 0.4203 Accuracy 0.3675\n",
      "Epoch 69 Batch 1152 Loss 0.4213 Accuracy 0.3676\n",
      "Epoch 69 Batch 1216 Loss 0.4223 Accuracy 0.3671\n",
      "Epoch 69 Batch 1280 Loss 0.4221 Accuracy 0.3675\n",
      "Epoch 69 Batch 1344 Loss 0.4223 Accuracy 0.3672\n",
      "Epoch 69 Batch 1408 Loss 0.4209 Accuracy 0.3671\n",
      "Epoch 69 Batch 1472 Loss 0.4228 Accuracy 0.3664\n",
      "Epoch 69 Batch 1536 Loss 0.4221 Accuracy 0.3673\n",
      "Epoch 69 Batch 1600 Loss 0.4214 Accuracy 0.3672\n",
      "Epoch 69 Batch 1664 Loss 0.4221 Accuracy 0.3675\n",
      "Epoch 69 Batch 1728 Loss 0.4228 Accuracy 0.3671\n",
      "Epoch 69 Batch 1792 Loss 0.4225 Accuracy 0.3671\n",
      "Epoch 69 Batch 1856 Loss 0.4224 Accuracy 0.3673\n",
      "Epoch 69 Batch 1920 Loss 0.4221 Accuracy 0.3677\n",
      "Epoch 69 Batch 1984 Loss 0.4237 Accuracy 0.3673\n",
      "Epoch 69 Batch 2048 Loss 0.4241 Accuracy 0.3676\n",
      "Epoch 69 Batch 2112 Loss 0.4224 Accuracy 0.3673\n",
      "Epoch 69 Batch 2176 Loss 0.4223 Accuracy 0.3671\n",
      "Epoch 69 Batch 2240 Loss 0.4219 Accuracy 0.3670\n",
      "Epoch 69 Batch 2304 Loss 0.4211 Accuracy 0.3674\n",
      "Epoch 69 Batch 2368 Loss 0.4215 Accuracy 0.3674\n",
      "Epoch 69 Batch 2432 Loss 0.4209 Accuracy 0.3673\n",
      "Epoch 69 Batch 2496 Loss 0.4208 Accuracy 0.3673\n",
      "Epoch 69 Batch 2560 Loss 0.4214 Accuracy 0.3676\n",
      "Epoch 69 Batch 2624 Loss 0.4222 Accuracy 0.3674\n",
      "Epoch 69 Batch 2688 Loss 0.4215 Accuracy 0.3673\n",
      "Epoch 69 Batch 2752 Loss 0.4214 Accuracy 0.3670\n",
      "Epoch 69 Batch 2816 Loss 0.4216 Accuracy 0.3668\n",
      "Epoch 69 Batch 2880 Loss 0.4224 Accuracy 0.3669\n",
      "Epoch 69 Batch 2944 Loss 0.4223 Accuracy 0.3670\n",
      "Epoch 69 Batch 3008 Loss 0.4222 Accuracy 0.3670\n",
      "Epoch 69 Batch 3072 Loss 0.4222 Accuracy 0.3672\n",
      "Epoch 69 Batch 3136 Loss 0.4225 Accuracy 0.3672\n",
      "Epoch 69 Loss 0.4225 Accuracy 0.3672\n",
      "Time taken for 1 epoch: 35.91932797431946 secs\n",
      "\n",
      "Epoch 70 Batch 64 Loss 0.3814 Accuracy 0.3645\n",
      "Epoch 70 Batch 128 Loss 0.3890 Accuracy 0.3673\n",
      "Epoch 70 Batch 192 Loss 0.3990 Accuracy 0.3637\n",
      "Epoch 70 Batch 256 Loss 0.4002 Accuracy 0.3676\n",
      "Epoch 70 Batch 320 Loss 0.4009 Accuracy 0.3682\n",
      "Epoch 70 Batch 384 Loss 0.3996 Accuracy 0.3683\n",
      "Epoch 70 Batch 448 Loss 0.4027 Accuracy 0.3695\n",
      "Epoch 70 Batch 512 Loss 0.4073 Accuracy 0.3705\n",
      "Epoch 70 Batch 576 Loss 0.4054 Accuracy 0.3718\n",
      "Epoch 70 Batch 640 Loss 0.4054 Accuracy 0.3715\n",
      "Epoch 70 Batch 704 Loss 0.4093 Accuracy 0.3711\n",
      "Epoch 70 Batch 768 Loss 0.4118 Accuracy 0.3692\n",
      "Epoch 70 Batch 832 Loss 0.4122 Accuracy 0.3696\n",
      "Epoch 70 Batch 896 Loss 0.4129 Accuracy 0.3701\n",
      "Epoch 70 Batch 960 Loss 0.4157 Accuracy 0.3703\n",
      "Epoch 70 Batch 1024 Loss 0.4164 Accuracy 0.3704\n",
      "Epoch 70 Batch 1088 Loss 0.4158 Accuracy 0.3701\n",
      "Epoch 70 Batch 1152 Loss 0.4191 Accuracy 0.3694\n",
      "Epoch 70 Batch 1216 Loss 0.4178 Accuracy 0.3694\n",
      "Epoch 70 Batch 1280 Loss 0.4179 Accuracy 0.3691\n",
      "Epoch 70 Batch 1344 Loss 0.4171 Accuracy 0.3692\n",
      "Epoch 70 Batch 1408 Loss 0.4165 Accuracy 0.3699\n",
      "Epoch 70 Batch 1472 Loss 0.4151 Accuracy 0.3702\n",
      "Epoch 70 Batch 1536 Loss 0.4155 Accuracy 0.3694\n",
      "Epoch 70 Batch 1600 Loss 0.4147 Accuracy 0.3692\n",
      "Epoch 70 Batch 1664 Loss 0.4150 Accuracy 0.3685\n",
      "Epoch 70 Batch 1728 Loss 0.4159 Accuracy 0.3681\n",
      "Epoch 70 Batch 1792 Loss 0.4133 Accuracy 0.3685\n",
      "Epoch 70 Batch 1856 Loss 0.4126 Accuracy 0.3679\n",
      "Epoch 70 Batch 1920 Loss 0.4135 Accuracy 0.3677\n",
      "Epoch 70 Batch 1984 Loss 0.4135 Accuracy 0.3678\n",
      "Epoch 70 Batch 2048 Loss 0.4147 Accuracy 0.3679\n",
      "Epoch 70 Batch 2112 Loss 0.4136 Accuracy 0.3684\n",
      "Epoch 70 Batch 2176 Loss 0.4146 Accuracy 0.3686\n",
      "Epoch 70 Batch 2240 Loss 0.4147 Accuracy 0.3688\n",
      "Epoch 70 Batch 2304 Loss 0.4132 Accuracy 0.3694\n",
      "Epoch 70 Batch 2368 Loss 0.4130 Accuracy 0.3693\n",
      "Epoch 70 Batch 2432 Loss 0.4137 Accuracy 0.3691\n",
      "Epoch 70 Batch 2496 Loss 0.4128 Accuracy 0.3693\n",
      "Epoch 70 Batch 2560 Loss 0.4131 Accuracy 0.3692\n",
      "Epoch 70 Batch 2624 Loss 0.4133 Accuracy 0.3693\n",
      "Epoch 70 Batch 2688 Loss 0.4128 Accuracy 0.3690\n",
      "Epoch 70 Batch 2752 Loss 0.4115 Accuracy 0.3692\n",
      "Epoch 70 Batch 2816 Loss 0.4113 Accuracy 0.3694\n",
      "Epoch 70 Batch 2880 Loss 0.4111 Accuracy 0.3696\n",
      "Epoch 70 Batch 2944 Loss 0.4109 Accuracy 0.3696\n",
      "Epoch 70 Batch 3008 Loss 0.4098 Accuracy 0.3696\n",
      "Epoch 70 Batch 3072 Loss 0.4095 Accuracy 0.3697\n",
      "Epoch 70 Batch 3136 Loss 0.4094 Accuracy 0.3694\n",
      "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
      "Epoch 70 Loss 0.4094 Accuracy 0.3694\n",
      "Time taken for 1 epoch: 36.73113703727722 secs\n",
      "\n",
      "Epoch 71 Batch 64 Loss 0.4009 Accuracy 0.3741\n",
      "Epoch 71 Batch 128 Loss 0.4050 Accuracy 0.3767\n",
      "Epoch 71 Batch 192 Loss 0.4137 Accuracy 0.3723\n",
      "Epoch 71 Batch 256 Loss 0.4162 Accuracy 0.3714\n",
      "Epoch 71 Batch 320 Loss 0.4263 Accuracy 0.3709\n",
      "Epoch 71 Batch 384 Loss 0.4258 Accuracy 0.3696\n",
      "Epoch 71 Batch 448 Loss 0.4176 Accuracy 0.3720\n",
      "Epoch 71 Batch 512 Loss 0.4109 Accuracy 0.3717\n",
      "Epoch 71 Batch 576 Loss 0.4154 Accuracy 0.3717\n",
      "Epoch 71 Batch 640 Loss 0.4154 Accuracy 0.3712\n",
      "Epoch 71 Batch 704 Loss 0.4102 Accuracy 0.3720\n",
      "Epoch 71 Batch 768 Loss 0.4077 Accuracy 0.3735\n",
      "Epoch 71 Batch 832 Loss 0.4069 Accuracy 0.3729\n",
      "Epoch 71 Batch 896 Loss 0.4054 Accuracy 0.3727\n",
      "Epoch 71 Batch 960 Loss 0.4048 Accuracy 0.3728\n",
      "Epoch 71 Batch 1024 Loss 0.4041 Accuracy 0.3726\n",
      "Epoch 71 Batch 1088 Loss 0.4017 Accuracy 0.3739\n",
      "Epoch 71 Batch 1152 Loss 0.4031 Accuracy 0.3730\n",
      "Epoch 71 Batch 1216 Loss 0.4015 Accuracy 0.3728\n",
      "Epoch 71 Batch 1280 Loss 0.4034 Accuracy 0.3724\n",
      "Epoch 71 Batch 1344 Loss 0.4029 Accuracy 0.3723\n",
      "Epoch 71 Batch 1408 Loss 0.4055 Accuracy 0.3723\n",
      "Epoch 71 Batch 1472 Loss 0.4060 Accuracy 0.3721\n",
      "Epoch 71 Batch 1536 Loss 0.4051 Accuracy 0.3716\n",
      "Epoch 71 Batch 1600 Loss 0.4052 Accuracy 0.3706\n",
      "Epoch 71 Batch 1664 Loss 0.4050 Accuracy 0.3709\n",
      "Epoch 71 Batch 1728 Loss 0.4048 Accuracy 0.3708\n",
      "Epoch 71 Batch 1792 Loss 0.4058 Accuracy 0.3708\n",
      "Epoch 71 Batch 1856 Loss 0.4056 Accuracy 0.3704\n",
      "Epoch 71 Batch 1920 Loss 0.4053 Accuracy 0.3708\n",
      "Epoch 71 Batch 1984 Loss 0.4069 Accuracy 0.3705\n",
      "Epoch 71 Batch 2048 Loss 0.4061 Accuracy 0.3702\n",
      "Epoch 71 Batch 2112 Loss 0.4071 Accuracy 0.3696\n",
      "Epoch 71 Batch 2176 Loss 0.4071 Accuracy 0.3699\n",
      "Epoch 71 Batch 2240 Loss 0.4074 Accuracy 0.3699\n",
      "Epoch 71 Batch 2304 Loss 0.4076 Accuracy 0.3700\n",
      "Epoch 71 Batch 2368 Loss 0.4078 Accuracy 0.3699\n",
      "Epoch 71 Batch 2432 Loss 0.4079 Accuracy 0.3696\n",
      "Epoch 71 Batch 2496 Loss 0.4072 Accuracy 0.3697\n",
      "Epoch 71 Batch 2560 Loss 0.4064 Accuracy 0.3695\n",
      "Epoch 71 Batch 2624 Loss 0.4057 Accuracy 0.3693\n",
      "Epoch 71 Batch 2688 Loss 0.4054 Accuracy 0.3695\n",
      "Epoch 71 Batch 2752 Loss 0.4049 Accuracy 0.3695\n",
      "Epoch 71 Batch 2816 Loss 0.4044 Accuracy 0.3696\n",
      "Epoch 71 Batch 2880 Loss 0.4042 Accuracy 0.3697\n",
      "Epoch 71 Batch 2944 Loss 0.4039 Accuracy 0.3698\n",
      "Epoch 71 Batch 3008 Loss 0.4040 Accuracy 0.3698\n",
      "Epoch 71 Batch 3072 Loss 0.4050 Accuracy 0.3696\n",
      "Epoch 71 Batch 3136 Loss 0.4047 Accuracy 0.3698\n",
      "Epoch 71 Loss 0.4047 Accuracy 0.3698\n",
      "Time taken for 1 epoch: 36.868744134902954 secs\n",
      "\n",
      "Epoch 72 Batch 64 Loss 0.3767 Accuracy 0.3908\n",
      "Epoch 72 Batch 128 Loss 0.3760 Accuracy 0.3842\n",
      "Epoch 72 Batch 192 Loss 0.3838 Accuracy 0.3870\n",
      "Epoch 72 Batch 256 Loss 0.3847 Accuracy 0.3798\n",
      "Epoch 72 Batch 320 Loss 0.3978 Accuracy 0.3741\n",
      "Epoch 72 Batch 384 Loss 0.3944 Accuracy 0.3713\n",
      "Epoch 72 Batch 448 Loss 0.3979 Accuracy 0.3721\n",
      "Epoch 72 Batch 512 Loss 0.3952 Accuracy 0.3712\n",
      "Epoch 72 Batch 576 Loss 0.3957 Accuracy 0.3713\n",
      "Epoch 72 Batch 640 Loss 0.3996 Accuracy 0.3724\n",
      "Epoch 72 Batch 704 Loss 0.3975 Accuracy 0.3733\n",
      "Epoch 72 Batch 768 Loss 0.3983 Accuracy 0.3723\n",
      "Epoch 72 Batch 832 Loss 0.3956 Accuracy 0.3730\n",
      "Epoch 72 Batch 896 Loss 0.3958 Accuracy 0.3731\n",
      "Epoch 72 Batch 960 Loss 0.3947 Accuracy 0.3728\n",
      "Epoch 72 Batch 1024 Loss 0.3949 Accuracy 0.3723\n",
      "Epoch 72 Batch 1088 Loss 0.3962 Accuracy 0.3723\n",
      "Epoch 72 Batch 1152 Loss 0.3979 Accuracy 0.3720\n",
      "Epoch 72 Batch 1216 Loss 0.3980 Accuracy 0.3717\n",
      "Epoch 72 Batch 1280 Loss 0.3978 Accuracy 0.3724\n",
      "Epoch 72 Batch 1344 Loss 0.3981 Accuracy 0.3723\n",
      "Epoch 72 Batch 1408 Loss 0.3979 Accuracy 0.3726\n",
      "Epoch 72 Batch 1472 Loss 0.3965 Accuracy 0.3728\n",
      "Epoch 72 Batch 1536 Loss 0.3968 Accuracy 0.3730\n",
      "Epoch 72 Batch 1600 Loss 0.3978 Accuracy 0.3727\n",
      "Epoch 72 Batch 1664 Loss 0.3984 Accuracy 0.3719\n",
      "Epoch 72 Batch 1728 Loss 0.3982 Accuracy 0.3713\n",
      "Epoch 72 Batch 1792 Loss 0.3980 Accuracy 0.3710\n",
      "Epoch 72 Batch 1856 Loss 0.3977 Accuracy 0.3710\n",
      "Epoch 72 Batch 1920 Loss 0.3968 Accuracy 0.3711\n",
      "Epoch 72 Batch 1984 Loss 0.3980 Accuracy 0.3710\n",
      "Epoch 72 Batch 2048 Loss 0.3974 Accuracy 0.3709\n",
      "Epoch 72 Batch 2112 Loss 0.3966 Accuracy 0.3710\n",
      "Epoch 72 Batch 2176 Loss 0.3966 Accuracy 0.3714\n",
      "Epoch 72 Batch 2240 Loss 0.3965 Accuracy 0.3714\n",
      "Epoch 72 Batch 2304 Loss 0.3959 Accuracy 0.3714\n",
      "Epoch 72 Batch 2368 Loss 0.3949 Accuracy 0.3715\n",
      "Epoch 72 Batch 2432 Loss 0.3950 Accuracy 0.3714\n",
      "Epoch 72 Batch 2496 Loss 0.3953 Accuracy 0.3712\n",
      "Epoch 72 Batch 2560 Loss 0.3955 Accuracy 0.3712\n",
      "Epoch 72 Batch 2624 Loss 0.3952 Accuracy 0.3712\n",
      "Epoch 72 Batch 2688 Loss 0.3950 Accuracy 0.3711\n",
      "Epoch 72 Batch 2752 Loss 0.3944 Accuracy 0.3709\n",
      "Epoch 72 Batch 2816 Loss 0.3943 Accuracy 0.3710\n",
      "Epoch 72 Batch 2880 Loss 0.3943 Accuracy 0.3712\n",
      "Epoch 72 Batch 2944 Loss 0.3942 Accuracy 0.3715\n",
      "Epoch 72 Batch 3008 Loss 0.3939 Accuracy 0.3717\n",
      "Epoch 72 Batch 3072 Loss 0.3945 Accuracy 0.3713\n",
      "Epoch 72 Batch 3136 Loss 0.3942 Accuracy 0.3715\n",
      "Epoch 72 Loss 0.3942 Accuracy 0.3715\n",
      "Time taken for 1 epoch: 38.3570191860199 secs\n",
      "\n",
      "Epoch 73 Batch 64 Loss 0.3698 Accuracy 0.3744\n",
      "Epoch 73 Batch 128 Loss 0.3589 Accuracy 0.3789\n",
      "Epoch 73 Batch 192 Loss 0.3615 Accuracy 0.3765\n",
      "Epoch 73 Batch 256 Loss 0.3735 Accuracy 0.3756\n",
      "Epoch 73 Batch 320 Loss 0.3783 Accuracy 0.3745\n",
      "Epoch 73 Batch 384 Loss 0.3745 Accuracy 0.3730\n",
      "Epoch 73 Batch 448 Loss 0.3720 Accuracy 0.3734\n",
      "Epoch 73 Batch 512 Loss 0.3752 Accuracy 0.3751\n",
      "Epoch 73 Batch 576 Loss 0.3767 Accuracy 0.3742\n",
      "Epoch 73 Batch 640 Loss 0.3801 Accuracy 0.3718\n",
      "Epoch 73 Batch 704 Loss 0.3826 Accuracy 0.3712\n",
      "Epoch 73 Batch 768 Loss 0.3857 Accuracy 0.3693\n",
      "Epoch 73 Batch 832 Loss 0.3855 Accuracy 0.3689\n",
      "Epoch 73 Batch 896 Loss 0.3861 Accuracy 0.3681\n",
      "Epoch 73 Batch 960 Loss 0.3861 Accuracy 0.3692\n",
      "Epoch 73 Batch 1024 Loss 0.3844 Accuracy 0.3694\n",
      "Epoch 73 Batch 1088 Loss 0.3852 Accuracy 0.3697\n",
      "Epoch 73 Batch 1152 Loss 0.3839 Accuracy 0.3693\n",
      "Epoch 73 Batch 1216 Loss 0.3833 Accuracy 0.3692\n",
      "Epoch 73 Batch 1280 Loss 0.3823 Accuracy 0.3702\n",
      "Epoch 73 Batch 1344 Loss 0.3815 Accuracy 0.3712\n",
      "Epoch 73 Batch 1408 Loss 0.3817 Accuracy 0.3709\n",
      "Epoch 73 Batch 1472 Loss 0.3826 Accuracy 0.3709\n",
      "Epoch 73 Batch 1536 Loss 0.3828 Accuracy 0.3712\n",
      "Epoch 73 Batch 1600 Loss 0.3828 Accuracy 0.3719\n",
      "Epoch 73 Batch 1664 Loss 0.3824 Accuracy 0.3717\n",
      "Epoch 73 Batch 1728 Loss 0.3811 Accuracy 0.3715\n",
      "Epoch 73 Batch 1792 Loss 0.3823 Accuracy 0.3716\n",
      "Epoch 73 Batch 1856 Loss 0.3813 Accuracy 0.3716\n",
      "Epoch 73 Batch 1920 Loss 0.3818 Accuracy 0.3723\n",
      "Epoch 73 Batch 1984 Loss 0.3808 Accuracy 0.3725\n",
      "Epoch 73 Batch 2048 Loss 0.3816 Accuracy 0.3725\n",
      "Epoch 73 Batch 2112 Loss 0.3824 Accuracy 0.3729\n",
      "Epoch 73 Batch 2176 Loss 0.3827 Accuracy 0.3726\n",
      "Epoch 73 Batch 2240 Loss 0.3830 Accuracy 0.3726\n",
      "Epoch 73 Batch 2304 Loss 0.3834 Accuracy 0.3725\n",
      "Epoch 73 Batch 2368 Loss 0.3834 Accuracy 0.3723\n",
      "Epoch 73 Batch 2432 Loss 0.3834 Accuracy 0.3724\n",
      "Epoch 73 Batch 2496 Loss 0.3834 Accuracy 0.3719\n",
      "Epoch 73 Batch 2560 Loss 0.3835 Accuracy 0.3722\n",
      "Epoch 73 Batch 2624 Loss 0.3835 Accuracy 0.3722\n",
      "Epoch 73 Batch 2688 Loss 0.3837 Accuracy 0.3723\n",
      "Epoch 73 Batch 2752 Loss 0.3836 Accuracy 0.3724\n",
      "Epoch 73 Batch 2816 Loss 0.3829 Accuracy 0.3730\n",
      "Epoch 73 Batch 2880 Loss 0.3834 Accuracy 0.3732\n",
      "Epoch 73 Batch 2944 Loss 0.3833 Accuracy 0.3733\n",
      "Epoch 73 Batch 3008 Loss 0.3834 Accuracy 0.3734\n",
      "Epoch 73 Batch 3072 Loss 0.3828 Accuracy 0.3734\n",
      "Epoch 73 Batch 3136 Loss 0.3820 Accuracy 0.3736\n",
      "Epoch 73 Loss 0.3820 Accuracy 0.3736\n",
      "Time taken for 1 epoch: 37.190479040145874 secs\n",
      "\n",
      "Epoch 74 Batch 64 Loss 0.3448 Accuracy 0.3864\n",
      "Epoch 74 Batch 128 Loss 0.3887 Accuracy 0.3721\n",
      "Epoch 74 Batch 192 Loss 0.3874 Accuracy 0.3744\n",
      "Epoch 74 Batch 256 Loss 0.3826 Accuracy 0.3729\n",
      "Epoch 74 Batch 320 Loss 0.3906 Accuracy 0.3711\n",
      "Epoch 74 Batch 384 Loss 0.3841 Accuracy 0.3717\n",
      "Epoch 74 Batch 448 Loss 0.3779 Accuracy 0.3727\n",
      "Epoch 74 Batch 512 Loss 0.3777 Accuracy 0.3728\n",
      "Epoch 74 Batch 576 Loss 0.3772 Accuracy 0.3742\n",
      "Epoch 74 Batch 640 Loss 0.3750 Accuracy 0.3749\n",
      "Epoch 74 Batch 704 Loss 0.3740 Accuracy 0.3754\n",
      "Epoch 74 Batch 768 Loss 0.3728 Accuracy 0.3746\n",
      "Epoch 74 Batch 832 Loss 0.3713 Accuracy 0.3760\n",
      "Epoch 74 Batch 896 Loss 0.3736 Accuracy 0.3764\n",
      "Epoch 74 Batch 960 Loss 0.3714 Accuracy 0.3769\n",
      "Epoch 74 Batch 1024 Loss 0.3692 Accuracy 0.3773\n",
      "Epoch 74 Batch 1088 Loss 0.3697 Accuracy 0.3762\n",
      "Epoch 74 Batch 1152 Loss 0.3728 Accuracy 0.3757\n",
      "Epoch 74 Batch 1216 Loss 0.3737 Accuracy 0.3747\n",
      "Epoch 74 Batch 1280 Loss 0.3748 Accuracy 0.3740\n",
      "Epoch 74 Batch 1344 Loss 0.3761 Accuracy 0.3743\n",
      "Epoch 74 Batch 1408 Loss 0.3745 Accuracy 0.3751\n",
      "Epoch 74 Batch 1472 Loss 0.3740 Accuracy 0.3750\n",
      "Epoch 74 Batch 1536 Loss 0.3729 Accuracy 0.3752\n",
      "Epoch 74 Batch 1600 Loss 0.3731 Accuracy 0.3746\n",
      "Epoch 74 Batch 1664 Loss 0.3733 Accuracy 0.3746\n",
      "Epoch 74 Batch 1728 Loss 0.3747 Accuracy 0.3750\n",
      "Epoch 74 Batch 1792 Loss 0.3760 Accuracy 0.3749\n",
      "Epoch 74 Batch 1856 Loss 0.3772 Accuracy 0.3751\n",
      "Epoch 74 Batch 1920 Loss 0.3785 Accuracy 0.3752\n",
      "Epoch 74 Batch 1984 Loss 0.3788 Accuracy 0.3756\n",
      "Epoch 74 Batch 2048 Loss 0.3783 Accuracy 0.3756\n",
      "Epoch 74 Batch 2112 Loss 0.3781 Accuracy 0.3756\n",
      "Epoch 74 Batch 2176 Loss 0.3781 Accuracy 0.3752\n",
      "Epoch 74 Batch 2240 Loss 0.3780 Accuracy 0.3748\n",
      "Epoch 74 Batch 2304 Loss 0.3767 Accuracy 0.3746\n",
      "Epoch 74 Batch 2368 Loss 0.3765 Accuracy 0.3747\n",
      "Epoch 74 Batch 2432 Loss 0.3771 Accuracy 0.3747\n",
      "Epoch 74 Batch 2496 Loss 0.3772 Accuracy 0.3749\n",
      "Epoch 74 Batch 2560 Loss 0.3767 Accuracy 0.3746\n",
      "Epoch 74 Batch 2624 Loss 0.3763 Accuracy 0.3747\n",
      "Epoch 74 Batch 2688 Loss 0.3772 Accuracy 0.3742\n",
      "Epoch 74 Batch 2752 Loss 0.3771 Accuracy 0.3743\n",
      "Epoch 74 Batch 2816 Loss 0.3777 Accuracy 0.3742\n",
      "Epoch 74 Batch 2880 Loss 0.3781 Accuracy 0.3737\n",
      "Epoch 74 Batch 2944 Loss 0.3777 Accuracy 0.3736\n",
      "Epoch 74 Batch 3008 Loss 0.3780 Accuracy 0.3737\n",
      "Epoch 74 Batch 3072 Loss 0.3783 Accuracy 0.3738\n",
      "Epoch 74 Batch 3136 Loss 0.3780 Accuracy 0.3739\n",
      "Epoch 74 Loss 0.3780 Accuracy 0.3739\n",
      "Time taken for 1 epoch: 36.35282588005066 secs\n",
      "\n",
      "Epoch 75 Batch 64 Loss 0.3341 Accuracy 0.3810\n",
      "Epoch 75 Batch 128 Loss 0.3711 Accuracy 0.3767\n",
      "Epoch 75 Batch 192 Loss 0.3568 Accuracy 0.3744\n",
      "Epoch 75 Batch 256 Loss 0.3551 Accuracy 0.3739\n",
      "Epoch 75 Batch 320 Loss 0.3560 Accuracy 0.3743\n",
      "Epoch 75 Batch 384 Loss 0.3592 Accuracy 0.3730\n",
      "Epoch 75 Batch 448 Loss 0.3631 Accuracy 0.3718\n",
      "Epoch 75 Batch 512 Loss 0.3586 Accuracy 0.3731\n",
      "Epoch 75 Batch 576 Loss 0.3558 Accuracy 0.3743\n",
      "Epoch 75 Batch 640 Loss 0.3569 Accuracy 0.3744\n",
      "Epoch 75 Batch 704 Loss 0.3545 Accuracy 0.3745\n",
      "Epoch 75 Batch 768 Loss 0.3589 Accuracy 0.3750\n",
      "Epoch 75 Batch 832 Loss 0.3588 Accuracy 0.3751\n",
      "Epoch 75 Batch 896 Loss 0.3600 Accuracy 0.3757\n",
      "Epoch 75 Batch 960 Loss 0.3606 Accuracy 0.3772\n",
      "Epoch 75 Batch 1024 Loss 0.3609 Accuracy 0.3759\n",
      "Epoch 75 Batch 1088 Loss 0.3605 Accuracy 0.3770\n",
      "Epoch 75 Batch 1152 Loss 0.3632 Accuracy 0.3763\n",
      "Epoch 75 Batch 1216 Loss 0.3626 Accuracy 0.3765\n",
      "Epoch 75 Batch 1280 Loss 0.3648 Accuracy 0.3758\n",
      "Epoch 75 Batch 1344 Loss 0.3643 Accuracy 0.3761\n",
      "Epoch 75 Batch 1408 Loss 0.3634 Accuracy 0.3760\n",
      "Epoch 75 Batch 1472 Loss 0.3613 Accuracy 0.3762\n",
      "Epoch 75 Batch 1536 Loss 0.3638 Accuracy 0.3760\n",
      "Epoch 75 Batch 1600 Loss 0.3638 Accuracy 0.3750\n",
      "Epoch 75 Batch 1664 Loss 0.3635 Accuracy 0.3753\n",
      "Epoch 75 Batch 1728 Loss 0.3629 Accuracy 0.3758\n",
      "Epoch 75 Batch 1792 Loss 0.3623 Accuracy 0.3760\n",
      "Epoch 75 Batch 1856 Loss 0.3623 Accuracy 0.3760\n",
      "Epoch 75 Batch 1920 Loss 0.3633 Accuracy 0.3766\n",
      "Epoch 75 Batch 1984 Loss 0.3649 Accuracy 0.3766\n",
      "Epoch 75 Batch 2048 Loss 0.3651 Accuracy 0.3766\n",
      "Epoch 75 Batch 2112 Loss 0.3651 Accuracy 0.3763\n",
      "Epoch 75 Batch 2176 Loss 0.3640 Accuracy 0.3767\n",
      "Epoch 75 Batch 2240 Loss 0.3634 Accuracy 0.3766\n",
      "Epoch 75 Batch 2304 Loss 0.3622 Accuracy 0.3767\n",
      "Epoch 75 Batch 2368 Loss 0.3622 Accuracy 0.3768\n",
      "Epoch 75 Batch 2432 Loss 0.3637 Accuracy 0.3764\n",
      "Epoch 75 Batch 2496 Loss 0.3636 Accuracy 0.3764\n",
      "Epoch 75 Batch 2560 Loss 0.3637 Accuracy 0.3764\n",
      "Epoch 75 Batch 2624 Loss 0.3652 Accuracy 0.3760\n",
      "Epoch 75 Batch 2688 Loss 0.3657 Accuracy 0.3756\n",
      "Epoch 75 Batch 2752 Loss 0.3659 Accuracy 0.3758\n",
      "Epoch 75 Batch 2816 Loss 0.3659 Accuracy 0.3755\n",
      "Epoch 75 Batch 2880 Loss 0.3664 Accuracy 0.3757\n",
      "Epoch 75 Batch 2944 Loss 0.3665 Accuracy 0.3757\n",
      "Epoch 75 Batch 3008 Loss 0.3666 Accuracy 0.3756\n",
      "Epoch 75 Batch 3072 Loss 0.3667 Accuracy 0.3752\n",
      "Epoch 75 Batch 3136 Loss 0.3672 Accuracy 0.3752\n",
      "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
      "Epoch 75 Loss 0.3672 Accuracy 0.3752\n",
      "Time taken for 1 epoch: 36.57602024078369 secs\n",
      "\n",
      "Epoch 76 Batch 64 Loss 0.3003 Accuracy 0.3667\n",
      "Epoch 76 Batch 128 Loss 0.3425 Accuracy 0.3746\n",
      "Epoch 76 Batch 192 Loss 0.3439 Accuracy 0.3773\n",
      "Epoch 76 Batch 256 Loss 0.3496 Accuracy 0.3770\n",
      "Epoch 76 Batch 320 Loss 0.3508 Accuracy 0.3757\n",
      "Epoch 76 Batch 384 Loss 0.3548 Accuracy 0.3746\n",
      "Epoch 76 Batch 448 Loss 0.3500 Accuracy 0.3746\n",
      "Epoch 76 Batch 512 Loss 0.3488 Accuracy 0.3748\n",
      "Epoch 76 Batch 576 Loss 0.3527 Accuracy 0.3733\n",
      "Epoch 76 Batch 640 Loss 0.3541 Accuracy 0.3732\n",
      "Epoch 76 Batch 704 Loss 0.3524 Accuracy 0.3739\n",
      "Epoch 76 Batch 768 Loss 0.3577 Accuracy 0.3734\n",
      "Epoch 76 Batch 832 Loss 0.3655 Accuracy 0.3723\n",
      "Epoch 76 Batch 896 Loss 0.3654 Accuracy 0.3720\n",
      "Epoch 76 Batch 960 Loss 0.3653 Accuracy 0.3716\n",
      "Epoch 76 Batch 1024 Loss 0.3646 Accuracy 0.3716\n",
      "Epoch 76 Batch 1088 Loss 0.3667 Accuracy 0.3718\n",
      "Epoch 76 Batch 1152 Loss 0.3646 Accuracy 0.3722\n",
      "Epoch 76 Batch 1216 Loss 0.3644 Accuracy 0.3727\n",
      "Epoch 76 Batch 1280 Loss 0.3631 Accuracy 0.3727\n",
      "Epoch 76 Batch 1344 Loss 0.3622 Accuracy 0.3735\n",
      "Epoch 76 Batch 1408 Loss 0.3618 Accuracy 0.3738\n",
      "Epoch 76 Batch 1472 Loss 0.3613 Accuracy 0.3744\n",
      "Epoch 76 Batch 1536 Loss 0.3625 Accuracy 0.3741\n",
      "Epoch 76 Batch 1600 Loss 0.3613 Accuracy 0.3737\n",
      "Epoch 76 Batch 1664 Loss 0.3592 Accuracy 0.3741\n",
      "Epoch 76 Batch 1728 Loss 0.3599 Accuracy 0.3746\n",
      "Epoch 76 Batch 1792 Loss 0.3585 Accuracy 0.3753\n",
      "Epoch 76 Batch 1856 Loss 0.3585 Accuracy 0.3750\n",
      "Epoch 76 Batch 1920 Loss 0.3596 Accuracy 0.3749\n",
      "Epoch 76 Batch 1984 Loss 0.3589 Accuracy 0.3754\n",
      "Epoch 76 Batch 2048 Loss 0.3594 Accuracy 0.3753\n",
      "Epoch 76 Batch 2112 Loss 0.3596 Accuracy 0.3752\n",
      "Epoch 76 Batch 2176 Loss 0.3594 Accuracy 0.3751\n",
      "Epoch 76 Batch 2240 Loss 0.3585 Accuracy 0.3752\n",
      "Epoch 76 Batch 2304 Loss 0.3582 Accuracy 0.3755\n",
      "Epoch 76 Batch 2368 Loss 0.3582 Accuracy 0.3756\n",
      "Epoch 76 Batch 2432 Loss 0.3578 Accuracy 0.3758\n",
      "Epoch 76 Batch 2496 Loss 0.3580 Accuracy 0.3761\n",
      "Epoch 76 Batch 2560 Loss 0.3585 Accuracy 0.3762\n",
      "Epoch 76 Batch 2624 Loss 0.3603 Accuracy 0.3759\n",
      "Epoch 76 Batch 2688 Loss 0.3602 Accuracy 0.3759\n",
      "Epoch 76 Batch 2752 Loss 0.3601 Accuracy 0.3763\n",
      "Epoch 76 Batch 2816 Loss 0.3602 Accuracy 0.3761\n",
      "Epoch 76 Batch 2880 Loss 0.3604 Accuracy 0.3760\n",
      "Epoch 76 Batch 2944 Loss 0.3605 Accuracy 0.3759\n",
      "Epoch 76 Batch 3008 Loss 0.3599 Accuracy 0.3759\n",
      "Epoch 76 Batch 3072 Loss 0.3607 Accuracy 0.3758\n",
      "Epoch 76 Batch 3136 Loss 0.3612 Accuracy 0.3759\n",
      "Epoch 76 Loss 0.3612 Accuracy 0.3759\n",
      "Time taken for 1 epoch: 36.50080108642578 secs\n",
      "\n",
      "Epoch 77 Batch 64 Loss 0.3610 Accuracy 0.3783\n",
      "Epoch 77 Batch 128 Loss 0.3551 Accuracy 0.3829\n",
      "Epoch 77 Batch 192 Loss 0.3479 Accuracy 0.3820\n",
      "Epoch 77 Batch 256 Loss 0.3502 Accuracy 0.3821\n",
      "Epoch 77 Batch 320 Loss 0.3467 Accuracy 0.3824\n",
      "Epoch 77 Batch 384 Loss 0.3550 Accuracy 0.3812\n",
      "Epoch 77 Batch 448 Loss 0.3540 Accuracy 0.3807\n",
      "Epoch 77 Batch 512 Loss 0.3559 Accuracy 0.3810\n",
      "Epoch 77 Batch 576 Loss 0.3558 Accuracy 0.3808\n",
      "Epoch 77 Batch 640 Loss 0.3567 Accuracy 0.3789\n",
      "Epoch 77 Batch 704 Loss 0.3631 Accuracy 0.3782\n",
      "Epoch 77 Batch 768 Loss 0.3641 Accuracy 0.3774\n",
      "Epoch 77 Batch 832 Loss 0.3648 Accuracy 0.3777\n",
      "Epoch 77 Batch 896 Loss 0.3660 Accuracy 0.3786\n",
      "Epoch 77 Batch 960 Loss 0.3647 Accuracy 0.3779\n",
      "Epoch 77 Batch 1024 Loss 0.3652 Accuracy 0.3776\n",
      "Epoch 77 Batch 1088 Loss 0.3644 Accuracy 0.3778\n",
      "Epoch 77 Batch 1152 Loss 0.3659 Accuracy 0.3771\n",
      "Epoch 77 Batch 1216 Loss 0.3640 Accuracy 0.3769\n",
      "Epoch 77 Batch 1280 Loss 0.3660 Accuracy 0.3763\n",
      "Epoch 77 Batch 1344 Loss 0.3651 Accuracy 0.3756\n",
      "Epoch 77 Batch 1408 Loss 0.3642 Accuracy 0.3755\n",
      "Epoch 77 Batch 1472 Loss 0.3645 Accuracy 0.3754\n",
      "Epoch 77 Batch 1536 Loss 0.3631 Accuracy 0.3753\n",
      "Epoch 77 Batch 1600 Loss 0.3634 Accuracy 0.3750\n",
      "Epoch 77 Batch 1664 Loss 0.3636 Accuracy 0.3752\n",
      "Epoch 77 Batch 1728 Loss 0.3624 Accuracy 0.3750\n",
      "Epoch 77 Batch 1792 Loss 0.3609 Accuracy 0.3750\n",
      "Epoch 77 Batch 1856 Loss 0.3603 Accuracy 0.3750\n",
      "Epoch 77 Batch 1920 Loss 0.3608 Accuracy 0.3746\n",
      "Epoch 77 Batch 1984 Loss 0.3588 Accuracy 0.3754\n",
      "Epoch 77 Batch 2048 Loss 0.3590 Accuracy 0.3753\n",
      "Epoch 77 Batch 2112 Loss 0.3574 Accuracy 0.3756\n",
      "Epoch 77 Batch 2176 Loss 0.3577 Accuracy 0.3758\n",
      "Epoch 77 Batch 2240 Loss 0.3573 Accuracy 0.3758\n",
      "Epoch 77 Batch 2304 Loss 0.3571 Accuracy 0.3761\n",
      "Epoch 77 Batch 2368 Loss 0.3569 Accuracy 0.3760\n",
      "Epoch 77 Batch 2432 Loss 0.3563 Accuracy 0.3765\n",
      "Epoch 77 Batch 2496 Loss 0.3566 Accuracy 0.3764\n",
      "Epoch 77 Batch 2560 Loss 0.3572 Accuracy 0.3762\n",
      "Epoch 77 Batch 2624 Loss 0.3568 Accuracy 0.3762\n",
      "Epoch 77 Batch 2688 Loss 0.3568 Accuracy 0.3763\n",
      "Epoch 77 Batch 2752 Loss 0.3573 Accuracy 0.3763\n",
      "Epoch 77 Batch 2816 Loss 0.3561 Accuracy 0.3767\n",
      "Epoch 77 Batch 2880 Loss 0.3568 Accuracy 0.3764\n",
      "Epoch 77 Batch 2944 Loss 0.3565 Accuracy 0.3765\n",
      "Epoch 77 Batch 3008 Loss 0.3557 Accuracy 0.3765\n",
      "Epoch 77 Batch 3072 Loss 0.3554 Accuracy 0.3765\n",
      "Epoch 77 Batch 3136 Loss 0.3555 Accuracy 0.3768\n",
      "Epoch 77 Loss 0.3555 Accuracy 0.3768\n",
      "Time taken for 1 epoch: 36.32296395301819 secs\n",
      "\n",
      "Epoch 78 Batch 64 Loss 0.3361 Accuracy 0.3788\n",
      "Epoch 78 Batch 128 Loss 0.3305 Accuracy 0.3795\n",
      "Epoch 78 Batch 192 Loss 0.3522 Accuracy 0.3789\n",
      "Epoch 78 Batch 256 Loss 0.3522 Accuracy 0.3795\n",
      "Epoch 78 Batch 320 Loss 0.3547 Accuracy 0.3770\n",
      "Epoch 78 Batch 384 Loss 0.3595 Accuracy 0.3756\n",
      "Epoch 78 Batch 448 Loss 0.3510 Accuracy 0.3753\n",
      "Epoch 78 Batch 512 Loss 0.3548 Accuracy 0.3763\n",
      "Epoch 78 Batch 576 Loss 0.3535 Accuracy 0.3753\n",
      "Epoch 78 Batch 640 Loss 0.3489 Accuracy 0.3754\n",
      "Epoch 78 Batch 704 Loss 0.3524 Accuracy 0.3748\n",
      "Epoch 78 Batch 768 Loss 0.3495 Accuracy 0.3753\n",
      "Epoch 78 Batch 832 Loss 0.3501 Accuracy 0.3753\n",
      "Epoch 78 Batch 896 Loss 0.3453 Accuracy 0.3769\n",
      "Epoch 78 Batch 960 Loss 0.3446 Accuracy 0.3762\n",
      "Epoch 78 Batch 1024 Loss 0.3436 Accuracy 0.3766\n",
      "Epoch 78 Batch 1088 Loss 0.3453 Accuracy 0.3770\n",
      "Epoch 78 Batch 1152 Loss 0.3457 Accuracy 0.3765\n",
      "Epoch 78 Batch 1216 Loss 0.3443 Accuracy 0.3765\n",
      "Epoch 78 Batch 1280 Loss 0.3440 Accuracy 0.3769\n",
      "Epoch 78 Batch 1344 Loss 0.3431 Accuracy 0.3772\n",
      "Epoch 78 Batch 1408 Loss 0.3437 Accuracy 0.3774\n",
      "Epoch 78 Batch 1472 Loss 0.3448 Accuracy 0.3777\n",
      "Epoch 78 Batch 1536 Loss 0.3439 Accuracy 0.3781\n",
      "Epoch 78 Batch 1600 Loss 0.3434 Accuracy 0.3784\n",
      "Epoch 78 Batch 1664 Loss 0.3422 Accuracy 0.3790\n",
      "Epoch 78 Batch 1728 Loss 0.3426 Accuracy 0.3793\n",
      "Epoch 78 Batch 1792 Loss 0.3428 Accuracy 0.3792\n",
      "Epoch 78 Batch 1856 Loss 0.3431 Accuracy 0.3792\n",
      "Epoch 78 Batch 1920 Loss 0.3425 Accuracy 0.3791\n",
      "Epoch 78 Batch 1984 Loss 0.3420 Accuracy 0.3789\n",
      "Epoch 78 Batch 2048 Loss 0.3438 Accuracy 0.3786\n",
      "Epoch 78 Batch 2112 Loss 0.3447 Accuracy 0.3784\n",
      "Epoch 78 Batch 2176 Loss 0.3442 Accuracy 0.3783\n",
      "Epoch 78 Batch 2240 Loss 0.3445 Accuracy 0.3779\n",
      "Epoch 78 Batch 2304 Loss 0.3443 Accuracy 0.3777\n",
      "Epoch 78 Batch 2368 Loss 0.3460 Accuracy 0.3776\n",
      "Epoch 78 Batch 2432 Loss 0.3464 Accuracy 0.3778\n",
      "Epoch 78 Batch 2496 Loss 0.3462 Accuracy 0.3776\n",
      "Epoch 78 Batch 2560 Loss 0.3466 Accuracy 0.3777\n",
      "Epoch 78 Batch 2624 Loss 0.3484 Accuracy 0.3771\n",
      "Epoch 78 Batch 2688 Loss 0.3471 Accuracy 0.3774\n",
      "Epoch 78 Batch 2752 Loss 0.3470 Accuracy 0.3774\n",
      "Epoch 78 Batch 2816 Loss 0.3469 Accuracy 0.3776\n",
      "Epoch 78 Batch 2880 Loss 0.3460 Accuracy 0.3778\n",
      "Epoch 78 Batch 2944 Loss 0.3460 Accuracy 0.3782\n",
      "Epoch 78 Batch 3008 Loss 0.3456 Accuracy 0.3784\n",
      "Epoch 78 Batch 3072 Loss 0.3457 Accuracy 0.3785\n",
      "Epoch 78 Batch 3136 Loss 0.3452 Accuracy 0.3787\n",
      "Epoch 78 Loss 0.3452 Accuracy 0.3787\n",
      "Time taken for 1 epoch: 36.42997980117798 secs\n",
      "\n",
      "Epoch 79 Batch 64 Loss 0.3533 Accuracy 0.3859\n",
      "Epoch 79 Batch 128 Loss 0.3386 Accuracy 0.3771\n",
      "Epoch 79 Batch 192 Loss 0.3222 Accuracy 0.3799\n",
      "Epoch 79 Batch 256 Loss 0.3227 Accuracy 0.3814\n",
      "Epoch 79 Batch 320 Loss 0.3261 Accuracy 0.3853\n",
      "Epoch 79 Batch 384 Loss 0.3260 Accuracy 0.3852\n",
      "Epoch 79 Batch 448 Loss 0.3295 Accuracy 0.3839\n",
      "Epoch 79 Batch 512 Loss 0.3287 Accuracy 0.3820\n",
      "Epoch 79 Batch 576 Loss 0.3315 Accuracy 0.3827\n",
      "Epoch 79 Batch 640 Loss 0.3346 Accuracy 0.3819\n",
      "Epoch 79 Batch 704 Loss 0.3356 Accuracy 0.3811\n",
      "Epoch 79 Batch 768 Loss 0.3360 Accuracy 0.3807\n",
      "Epoch 79 Batch 832 Loss 0.3356 Accuracy 0.3800\n",
      "Epoch 79 Batch 896 Loss 0.3371 Accuracy 0.3797\n",
      "Epoch 79 Batch 960 Loss 0.3371 Accuracy 0.3791\n",
      "Epoch 79 Batch 1024 Loss 0.3370 Accuracy 0.3796\n",
      "Epoch 79 Batch 1088 Loss 0.3357 Accuracy 0.3796\n",
      "Epoch 79 Batch 1152 Loss 0.3333 Accuracy 0.3797\n",
      "Epoch 79 Batch 1216 Loss 0.3350 Accuracy 0.3796\n",
      "Epoch 79 Batch 1280 Loss 0.3348 Accuracy 0.3798\n",
      "Epoch 79 Batch 1344 Loss 0.3351 Accuracy 0.3800\n",
      "Epoch 79 Batch 1408 Loss 0.3344 Accuracy 0.3802\n",
      "Epoch 79 Batch 1472 Loss 0.3347 Accuracy 0.3800\n",
      "Epoch 79 Batch 1536 Loss 0.3348 Accuracy 0.3801\n",
      "Epoch 79 Batch 1600 Loss 0.3346 Accuracy 0.3805\n",
      "Epoch 79 Batch 1664 Loss 0.3342 Accuracy 0.3802\n",
      "Epoch 79 Batch 1728 Loss 0.3331 Accuracy 0.3801\n",
      "Epoch 79 Batch 1792 Loss 0.3324 Accuracy 0.3799\n",
      "Epoch 79 Batch 1856 Loss 0.3327 Accuracy 0.3803\n",
      "Epoch 79 Batch 1920 Loss 0.3324 Accuracy 0.3799\n",
      "Epoch 79 Batch 1984 Loss 0.3319 Accuracy 0.3805\n",
      "Epoch 79 Batch 2048 Loss 0.3318 Accuracy 0.3804\n",
      "Epoch 79 Batch 2112 Loss 0.3318 Accuracy 0.3801\n",
      "Epoch 79 Batch 2176 Loss 0.3331 Accuracy 0.3800\n",
      "Epoch 79 Batch 2240 Loss 0.3328 Accuracy 0.3802\n",
      "Epoch 79 Batch 2304 Loss 0.3344 Accuracy 0.3802\n",
      "Epoch 79 Batch 2368 Loss 0.3350 Accuracy 0.3801\n",
      "Epoch 79 Batch 2432 Loss 0.3369 Accuracy 0.3796\n",
      "Epoch 79 Batch 2496 Loss 0.3364 Accuracy 0.3797\n",
      "Epoch 79 Batch 2560 Loss 0.3374 Accuracy 0.3795\n",
      "Epoch 79 Batch 2624 Loss 0.3382 Accuracy 0.3794\n",
      "Epoch 79 Batch 2688 Loss 0.3397 Accuracy 0.3789\n",
      "Epoch 79 Batch 2752 Loss 0.3406 Accuracy 0.3788\n",
      "Epoch 79 Batch 2816 Loss 0.3397 Accuracy 0.3790\n",
      "Epoch 79 Batch 2880 Loss 0.3400 Accuracy 0.3792\n",
      "Epoch 79 Batch 2944 Loss 0.3407 Accuracy 0.3790\n",
      "Epoch 79 Batch 3008 Loss 0.3419 Accuracy 0.3786\n",
      "Epoch 79 Batch 3072 Loss 0.3426 Accuracy 0.3787\n",
      "Epoch 79 Batch 3136 Loss 0.3424 Accuracy 0.3787\n",
      "Epoch 79 Loss 0.3424 Accuracy 0.3787\n",
      "Time taken for 1 epoch: 35.86108589172363 secs\n",
      "\n",
      "Epoch 80 Batch 64 Loss 0.3472 Accuracy 0.3607\n",
      "Epoch 80 Batch 128 Loss 0.3535 Accuracy 0.3807\n",
      "Epoch 80 Batch 192 Loss 0.3579 Accuracy 0.3724\n",
      "Epoch 80 Batch 256 Loss 0.3612 Accuracy 0.3723\n",
      "Epoch 80 Batch 320 Loss 0.3500 Accuracy 0.3760\n",
      "Epoch 80 Batch 384 Loss 0.3500 Accuracy 0.3760\n",
      "Epoch 80 Batch 448 Loss 0.3510 Accuracy 0.3782\n",
      "Epoch 80 Batch 512 Loss 0.3491 Accuracy 0.3779\n",
      "Epoch 80 Batch 576 Loss 0.3467 Accuracy 0.3788\n",
      "Epoch 80 Batch 640 Loss 0.3502 Accuracy 0.3788\n",
      "Epoch 80 Batch 704 Loss 0.3478 Accuracy 0.3795\n",
      "Epoch 80 Batch 768 Loss 0.3472 Accuracy 0.3797\n",
      "Epoch 80 Batch 832 Loss 0.3480 Accuracy 0.3796\n",
      "Epoch 80 Batch 896 Loss 0.3552 Accuracy 0.3796\n",
      "Epoch 80 Batch 960 Loss 0.3530 Accuracy 0.3790\n",
      "Epoch 80 Batch 1024 Loss 0.3524 Accuracy 0.3786\n",
      "Epoch 80 Batch 1088 Loss 0.3523 Accuracy 0.3784\n",
      "Epoch 80 Batch 1152 Loss 0.3521 Accuracy 0.3781\n",
      "Epoch 80 Batch 1216 Loss 0.3504 Accuracy 0.3777\n",
      "Epoch 80 Batch 1280 Loss 0.3497 Accuracy 0.3768\n",
      "Epoch 80 Batch 1344 Loss 0.3490 Accuracy 0.3765\n",
      "Epoch 80 Batch 1408 Loss 0.3482 Accuracy 0.3764\n",
      "Epoch 80 Batch 1472 Loss 0.3480 Accuracy 0.3767\n",
      "Epoch 80 Batch 1536 Loss 0.3482 Accuracy 0.3772\n",
      "Epoch 80 Batch 1600 Loss 0.3481 Accuracy 0.3768\n",
      "Epoch 80 Batch 1664 Loss 0.3452 Accuracy 0.3775\n",
      "Epoch 80 Batch 1728 Loss 0.3447 Accuracy 0.3777\n",
      "Epoch 80 Batch 1792 Loss 0.3437 Accuracy 0.3780\n",
      "Epoch 80 Batch 1856 Loss 0.3427 Accuracy 0.3784\n",
      "Epoch 80 Batch 1920 Loss 0.3423 Accuracy 0.3785\n",
      "Epoch 80 Batch 1984 Loss 0.3410 Accuracy 0.3789\n",
      "Epoch 80 Batch 2048 Loss 0.3399 Accuracy 0.3794\n",
      "Epoch 80 Batch 2112 Loss 0.3385 Accuracy 0.3797\n",
      "Epoch 80 Batch 2176 Loss 0.3383 Accuracy 0.3800\n",
      "Epoch 80 Batch 2240 Loss 0.3379 Accuracy 0.3801\n",
      "Epoch 80 Batch 2304 Loss 0.3375 Accuracy 0.3804\n",
      "Epoch 80 Batch 2368 Loss 0.3386 Accuracy 0.3802\n",
      "Epoch 80 Batch 2432 Loss 0.3388 Accuracy 0.3803\n",
      "Epoch 80 Batch 2496 Loss 0.3377 Accuracy 0.3803\n",
      "Epoch 80 Batch 2560 Loss 0.3370 Accuracy 0.3805\n",
      "Epoch 80 Batch 2624 Loss 0.3370 Accuracy 0.3802\n",
      "Epoch 80 Batch 2688 Loss 0.3364 Accuracy 0.3799\n",
      "Epoch 80 Batch 2752 Loss 0.3366 Accuracy 0.3802\n",
      "Epoch 80 Batch 2816 Loss 0.3366 Accuracy 0.3803\n",
      "Epoch 80 Batch 2880 Loss 0.3363 Accuracy 0.3800\n",
      "Epoch 80 Batch 2944 Loss 0.3365 Accuracy 0.3802\n",
      "Epoch 80 Batch 3008 Loss 0.3361 Accuracy 0.3802\n",
      "Epoch 80 Batch 3072 Loss 0.3357 Accuracy 0.3804\n",
      "Epoch 80 Batch 3136 Loss 0.3362 Accuracy 0.3803\n",
      "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
      "Epoch 80 Loss 0.3362 Accuracy 0.3803\n",
      "Time taken for 1 epoch: 37.352338790893555 secs\n",
      "\n",
      "Epoch 81 Batch 64 Loss 0.3478 Accuracy 0.3765\n",
      "Epoch 81 Batch 128 Loss 0.3214 Accuracy 0.3797\n",
      "Epoch 81 Batch 192 Loss 0.3332 Accuracy 0.3795\n",
      "Epoch 81 Batch 256 Loss 0.3319 Accuracy 0.3816\n",
      "Epoch 81 Batch 320 Loss 0.3358 Accuracy 0.3850\n",
      "Epoch 81 Batch 384 Loss 0.3318 Accuracy 0.3849\n",
      "Epoch 81 Batch 448 Loss 0.3295 Accuracy 0.3839\n",
      "Epoch 81 Batch 512 Loss 0.3326 Accuracy 0.3854\n",
      "Epoch 81 Batch 576 Loss 0.3294 Accuracy 0.3854\n",
      "Epoch 81 Batch 640 Loss 0.3282 Accuracy 0.3847\n",
      "Epoch 81 Batch 704 Loss 0.3296 Accuracy 0.3835\n",
      "Epoch 81 Batch 768 Loss 0.3262 Accuracy 0.3838\n",
      "Epoch 81 Batch 832 Loss 0.3251 Accuracy 0.3839\n",
      "Epoch 81 Batch 896 Loss 0.3252 Accuracy 0.3834\n",
      "Epoch 81 Batch 960 Loss 0.3240 Accuracy 0.3822\n",
      "Epoch 81 Batch 1024 Loss 0.3221 Accuracy 0.3819\n",
      "Epoch 81 Batch 1088 Loss 0.3230 Accuracy 0.3811\n",
      "Epoch 81 Batch 1152 Loss 0.3236 Accuracy 0.3806\n",
      "Epoch 81 Batch 1216 Loss 0.3254 Accuracy 0.3808\n",
      "Epoch 81 Batch 1280 Loss 0.3250 Accuracy 0.3810\n",
      "Epoch 81 Batch 1344 Loss 0.3281 Accuracy 0.3806\n",
      "Epoch 81 Batch 1408 Loss 0.3282 Accuracy 0.3807\n",
      "Epoch 81 Batch 1472 Loss 0.3286 Accuracy 0.3807\n",
      "Epoch 81 Batch 1536 Loss 0.3283 Accuracy 0.3811\n",
      "Epoch 81 Batch 1600 Loss 0.3292 Accuracy 0.3808\n",
      "Epoch 81 Batch 1664 Loss 0.3294 Accuracy 0.3807\n",
      "Epoch 81 Batch 1728 Loss 0.3305 Accuracy 0.3808\n",
      "Epoch 81 Batch 1792 Loss 0.3320 Accuracy 0.3812\n",
      "Epoch 81 Batch 1856 Loss 0.3335 Accuracy 0.3811\n",
      "Epoch 81 Batch 1920 Loss 0.3336 Accuracy 0.3805\n",
      "Epoch 81 Batch 1984 Loss 0.3334 Accuracy 0.3803\n",
      "Epoch 81 Batch 2048 Loss 0.3333 Accuracy 0.3803\n",
      "Epoch 81 Batch 2112 Loss 0.3337 Accuracy 0.3799\n",
      "Epoch 81 Batch 2176 Loss 0.3322 Accuracy 0.3799\n",
      "Epoch 81 Batch 2240 Loss 0.3331 Accuracy 0.3798\n",
      "Epoch 81 Batch 2304 Loss 0.3331 Accuracy 0.3795\n",
      "Epoch 81 Batch 2368 Loss 0.3328 Accuracy 0.3798\n",
      "Epoch 81 Batch 2432 Loss 0.3321 Accuracy 0.3798\n",
      "Epoch 81 Batch 2496 Loss 0.3312 Accuracy 0.3800\n",
      "Epoch 81 Batch 2560 Loss 0.3305 Accuracy 0.3802\n",
      "Epoch 81 Batch 2624 Loss 0.3292 Accuracy 0.3800\n",
      "Epoch 81 Batch 2688 Loss 0.3296 Accuracy 0.3800\n",
      "Epoch 81 Batch 2752 Loss 0.3301 Accuracy 0.3800\n",
      "Epoch 81 Batch 2816 Loss 0.3294 Accuracy 0.3802\n",
      "Epoch 81 Batch 2880 Loss 0.3279 Accuracy 0.3807\n",
      "Epoch 81 Batch 2944 Loss 0.3275 Accuracy 0.3810\n",
      "Epoch 81 Batch 3008 Loss 0.3271 Accuracy 0.3811\n",
      "Epoch 81 Batch 3072 Loss 0.3276 Accuracy 0.3813\n",
      "Epoch 81 Batch 3136 Loss 0.3270 Accuracy 0.3811\n",
      "Epoch 81 Loss 0.3270 Accuracy 0.3811\n",
      "Time taken for 1 epoch: 36.546480894088745 secs\n",
      "\n",
      "Epoch 82 Batch 64 Loss 0.3278 Accuracy 0.3828\n",
      "Epoch 82 Batch 128 Loss 0.3063 Accuracy 0.3876\n",
      "Epoch 82 Batch 192 Loss 0.3196 Accuracy 0.3847\n",
      "Epoch 82 Batch 256 Loss 0.3176 Accuracy 0.3817\n",
      "Epoch 82 Batch 320 Loss 0.3156 Accuracy 0.3853\n",
      "Epoch 82 Batch 384 Loss 0.3182 Accuracy 0.3859\n",
      "Epoch 82 Batch 448 Loss 0.3156 Accuracy 0.3877\n",
      "Epoch 82 Batch 512 Loss 0.3154 Accuracy 0.3867\n",
      "Epoch 82 Batch 576 Loss 0.3150 Accuracy 0.3858\n",
      "Epoch 82 Batch 640 Loss 0.3147 Accuracy 0.3851\n",
      "Epoch 82 Batch 704 Loss 0.3118 Accuracy 0.3850\n",
      "Epoch 82 Batch 768 Loss 0.3088 Accuracy 0.3862\n",
      "Epoch 82 Batch 832 Loss 0.3134 Accuracy 0.3863\n",
      "Epoch 82 Batch 896 Loss 0.3147 Accuracy 0.3864\n",
      "Epoch 82 Batch 960 Loss 0.3107 Accuracy 0.3872\n",
      "Epoch 82 Batch 1024 Loss 0.3097 Accuracy 0.3864\n",
      "Epoch 82 Batch 1088 Loss 0.3089 Accuracy 0.3853\n",
      "Epoch 82 Batch 1152 Loss 0.3089 Accuracy 0.3862\n",
      "Epoch 82 Batch 1216 Loss 0.3104 Accuracy 0.3861\n",
      "Epoch 82 Batch 1280 Loss 0.3087 Accuracy 0.3864\n",
      "Epoch 82 Batch 1344 Loss 0.3072 Accuracy 0.3856\n",
      "Epoch 82 Batch 1408 Loss 0.3092 Accuracy 0.3853\n",
      "Epoch 82 Batch 1472 Loss 0.3078 Accuracy 0.3858\n",
      "Epoch 82 Batch 1536 Loss 0.3088 Accuracy 0.3854\n",
      "Epoch 82 Batch 1600 Loss 0.3088 Accuracy 0.3849\n",
      "Epoch 82 Batch 1664 Loss 0.3088 Accuracy 0.3847\n",
      "Epoch 82 Batch 1728 Loss 0.3086 Accuracy 0.3846\n",
      "Epoch 82 Batch 1792 Loss 0.3092 Accuracy 0.3849\n",
      "Epoch 82 Batch 1856 Loss 0.3097 Accuracy 0.3845\n",
      "Epoch 82 Batch 1920 Loss 0.3093 Accuracy 0.3845\n",
      "Epoch 82 Batch 1984 Loss 0.3088 Accuracy 0.3844\n",
      "Epoch 82 Batch 2048 Loss 0.3085 Accuracy 0.3843\n",
      "Epoch 82 Batch 2112 Loss 0.3098 Accuracy 0.3838\n",
      "Epoch 82 Batch 2176 Loss 0.3100 Accuracy 0.3836\n",
      "Epoch 82 Batch 2240 Loss 0.3114 Accuracy 0.3835\n",
      "Epoch 82 Batch 2304 Loss 0.3124 Accuracy 0.3836\n",
      "Epoch 82 Batch 2368 Loss 0.3122 Accuracy 0.3831\n",
      "Epoch 82 Batch 2432 Loss 0.3138 Accuracy 0.3829\n",
      "Epoch 82 Batch 2496 Loss 0.3143 Accuracy 0.3831\n",
      "Epoch 82 Batch 2560 Loss 0.3143 Accuracy 0.3830\n",
      "Epoch 82 Batch 2624 Loss 0.3146 Accuracy 0.3828\n",
      "Epoch 82 Batch 2688 Loss 0.3151 Accuracy 0.3826\n",
      "Epoch 82 Batch 2752 Loss 0.3164 Accuracy 0.3829\n",
      "Epoch 82 Batch 2816 Loss 0.3172 Accuracy 0.3829\n",
      "Epoch 82 Batch 2880 Loss 0.3171 Accuracy 0.3832\n",
      "Epoch 82 Batch 2944 Loss 0.3181 Accuracy 0.3831\n",
      "Epoch 82 Batch 3008 Loss 0.3177 Accuracy 0.3830\n",
      "Epoch 82 Batch 3072 Loss 0.3182 Accuracy 0.3827\n",
      "Epoch 82 Batch 3136 Loss 0.3184 Accuracy 0.3824\n",
      "Epoch 82 Loss 0.3184 Accuracy 0.3824\n",
      "Time taken for 1 epoch: 35.977351903915405 secs\n",
      "\n",
      "Epoch 83 Batch 64 Loss 0.4059 Accuracy 0.3651\n",
      "Epoch 83 Batch 128 Loss 0.3702 Accuracy 0.3811\n",
      "Epoch 83 Batch 192 Loss 0.3538 Accuracy 0.3797\n",
      "Epoch 83 Batch 256 Loss 0.3524 Accuracy 0.3793\n",
      "Epoch 83 Batch 320 Loss 0.3456 Accuracy 0.3774\n",
      "Epoch 83 Batch 384 Loss 0.3414 Accuracy 0.3773\n",
      "Epoch 83 Batch 448 Loss 0.3381 Accuracy 0.3765\n",
      "Epoch 83 Batch 512 Loss 0.3335 Accuracy 0.3780\n",
      "Epoch 83 Batch 576 Loss 0.3307 Accuracy 0.3796\n",
      "Epoch 83 Batch 640 Loss 0.3280 Accuracy 0.3803\n",
      "Epoch 83 Batch 704 Loss 0.3231 Accuracy 0.3813\n",
      "Epoch 83 Batch 768 Loss 0.3194 Accuracy 0.3811\n",
      "Epoch 83 Batch 832 Loss 0.3178 Accuracy 0.3819\n",
      "Epoch 83 Batch 896 Loss 0.3151 Accuracy 0.3824\n",
      "Epoch 83 Batch 960 Loss 0.3155 Accuracy 0.3822\n",
      "Epoch 83 Batch 1024 Loss 0.3153 Accuracy 0.3831\n",
      "Epoch 83 Batch 1088 Loss 0.3143 Accuracy 0.3838\n",
      "Epoch 83 Batch 1152 Loss 0.3124 Accuracy 0.3843\n",
      "Epoch 83 Batch 1216 Loss 0.3109 Accuracy 0.3845\n",
      "Epoch 83 Batch 1280 Loss 0.3114 Accuracy 0.3847\n",
      "Epoch 83 Batch 1344 Loss 0.3115 Accuracy 0.3856\n",
      "Epoch 83 Batch 1408 Loss 0.3109 Accuracy 0.3853\n",
      "Epoch 83 Batch 1472 Loss 0.3093 Accuracy 0.3854\n",
      "Epoch 83 Batch 1536 Loss 0.3087 Accuracy 0.3850\n",
      "Epoch 83 Batch 1600 Loss 0.3096 Accuracy 0.3852\n",
      "Epoch 83 Batch 1664 Loss 0.3096 Accuracy 0.3854\n",
      "Epoch 83 Batch 1728 Loss 0.3092 Accuracy 0.3852\n",
      "Epoch 83 Batch 1792 Loss 0.3074 Accuracy 0.3853\n",
      "Epoch 83 Batch 1856 Loss 0.3065 Accuracy 0.3850\n",
      "Epoch 83 Batch 1920 Loss 0.3055 Accuracy 0.3855\n",
      "Epoch 83 Batch 1984 Loss 0.3053 Accuracy 0.3854\n",
      "Epoch 83 Batch 2048 Loss 0.3047 Accuracy 0.3854\n",
      "Epoch 83 Batch 2112 Loss 0.3052 Accuracy 0.3850\n",
      "Epoch 83 Batch 2176 Loss 0.3053 Accuracy 0.3851\n",
      "Epoch 83 Batch 2240 Loss 0.3055 Accuracy 0.3855\n",
      "Epoch 83 Batch 2304 Loss 0.3057 Accuracy 0.3857\n",
      "Epoch 83 Batch 2368 Loss 0.3067 Accuracy 0.3848\n",
      "Epoch 83 Batch 2432 Loss 0.3064 Accuracy 0.3850\n",
      "Epoch 83 Batch 2496 Loss 0.3072 Accuracy 0.3850\n",
      "Epoch 83 Batch 2560 Loss 0.3074 Accuracy 0.3848\n",
      "Epoch 83 Batch 2624 Loss 0.3075 Accuracy 0.3845\n",
      "Epoch 83 Batch 2688 Loss 0.3080 Accuracy 0.3850\n",
      "Epoch 83 Batch 2752 Loss 0.3084 Accuracy 0.3851\n",
      "Epoch 83 Batch 2816 Loss 0.3083 Accuracy 0.3850\n",
      "Epoch 83 Batch 2880 Loss 0.3079 Accuracy 0.3847\n",
      "Epoch 83 Batch 2944 Loss 0.3078 Accuracy 0.3844\n",
      "Epoch 83 Batch 3008 Loss 0.3079 Accuracy 0.3845\n",
      "Epoch 83 Batch 3072 Loss 0.3084 Accuracy 0.3843\n",
      "Epoch 83 Batch 3136 Loss 0.3080 Accuracy 0.3845\n",
      "Epoch 83 Loss 0.3080 Accuracy 0.3845\n",
      "Time taken for 1 epoch: 36.100504875183105 secs\n",
      "\n",
      "Epoch 84 Batch 64 Loss 0.3617 Accuracy 0.3897\n",
      "Epoch 84 Batch 128 Loss 0.3355 Accuracy 0.3854\n",
      "Epoch 84 Batch 192 Loss 0.3223 Accuracy 0.3869\n",
      "Epoch 84 Batch 256 Loss 0.3144 Accuracy 0.3833\n",
      "Epoch 84 Batch 320 Loss 0.3154 Accuracy 0.3832\n",
      "Epoch 84 Batch 384 Loss 0.3191 Accuracy 0.3836\n",
      "Epoch 84 Batch 448 Loss 0.3178 Accuracy 0.3840\n",
      "Epoch 84 Batch 512 Loss 0.3141 Accuracy 0.3855\n",
      "Epoch 84 Batch 576 Loss 0.3101 Accuracy 0.3858\n",
      "Epoch 84 Batch 640 Loss 0.3062 Accuracy 0.3858\n",
      "Epoch 84 Batch 704 Loss 0.3050 Accuracy 0.3872\n",
      "Epoch 84 Batch 768 Loss 0.3079 Accuracy 0.3866\n",
      "Epoch 84 Batch 832 Loss 0.3078 Accuracy 0.3867\n",
      "Epoch 84 Batch 896 Loss 0.3057 Accuracy 0.3858\n",
      "Epoch 84 Batch 960 Loss 0.3099 Accuracy 0.3862\n",
      "Epoch 84 Batch 1024 Loss 0.3093 Accuracy 0.3858\n",
      "Epoch 84 Batch 1088 Loss 0.3094 Accuracy 0.3849\n",
      "Epoch 84 Batch 1152 Loss 0.3098 Accuracy 0.3849\n",
      "Epoch 84 Batch 1216 Loss 0.3105 Accuracy 0.3843\n",
      "Epoch 84 Batch 1280 Loss 0.3118 Accuracy 0.3843\n",
      "Epoch 84 Batch 1344 Loss 0.3105 Accuracy 0.3840\n",
      "Epoch 84 Batch 1408 Loss 0.3109 Accuracy 0.3833\n",
      "Epoch 84 Batch 1472 Loss 0.3110 Accuracy 0.3833\n",
      "Epoch 84 Batch 1536 Loss 0.3099 Accuracy 0.3835\n",
      "Epoch 84 Batch 1600 Loss 0.3108 Accuracy 0.3836\n",
      "Epoch 84 Batch 1664 Loss 0.3114 Accuracy 0.3838\n",
      "Epoch 84 Batch 1728 Loss 0.3100 Accuracy 0.3838\n",
      "Epoch 84 Batch 1792 Loss 0.3110 Accuracy 0.3836\n",
      "Epoch 84 Batch 1856 Loss 0.3108 Accuracy 0.3836\n",
      "Epoch 84 Batch 1920 Loss 0.3122 Accuracy 0.3840\n",
      "Epoch 84 Batch 1984 Loss 0.3127 Accuracy 0.3839\n",
      "Epoch 84 Batch 2048 Loss 0.3132 Accuracy 0.3836\n",
      "Epoch 84 Batch 2112 Loss 0.3137 Accuracy 0.3833\n",
      "Epoch 84 Batch 2176 Loss 0.3138 Accuracy 0.3832\n",
      "Epoch 84 Batch 2240 Loss 0.3139 Accuracy 0.3834\n",
      "Epoch 84 Batch 2304 Loss 0.3134 Accuracy 0.3833\n",
      "Epoch 84 Batch 2368 Loss 0.3132 Accuracy 0.3835\n",
      "Epoch 84 Batch 2432 Loss 0.3124 Accuracy 0.3836\n",
      "Epoch 84 Batch 2496 Loss 0.3117 Accuracy 0.3837\n",
      "Epoch 84 Batch 2560 Loss 0.3110 Accuracy 0.3838\n",
      "Epoch 84 Batch 2624 Loss 0.3104 Accuracy 0.3836\n",
      "Epoch 84 Batch 2688 Loss 0.3103 Accuracy 0.3837\n",
      "Epoch 84 Batch 2752 Loss 0.3097 Accuracy 0.3838\n",
      "Epoch 84 Batch 2816 Loss 0.3093 Accuracy 0.3840\n",
      "Epoch 84 Batch 2880 Loss 0.3098 Accuracy 0.3837\n",
      "Epoch 84 Batch 2944 Loss 0.3100 Accuracy 0.3834\n",
      "Epoch 84 Batch 3008 Loss 0.3097 Accuracy 0.3835\n",
      "Epoch 84 Batch 3072 Loss 0.3092 Accuracy 0.3836\n",
      "Epoch 84 Batch 3136 Loss 0.3083 Accuracy 0.3840\n",
      "Epoch 84 Loss 0.3083 Accuracy 0.3840\n",
      "Time taken for 1 epoch: 36.07705998420715 secs\n",
      "\n",
      "Epoch 85 Batch 64 Loss 0.2853 Accuracy 0.3819\n",
      "Epoch 85 Batch 128 Loss 0.2848 Accuracy 0.3789\n",
      "Epoch 85 Batch 192 Loss 0.2741 Accuracy 0.3875\n",
      "Epoch 85 Batch 256 Loss 0.2679 Accuracy 0.3893\n",
      "Epoch 85 Batch 320 Loss 0.2817 Accuracy 0.3907\n",
      "Epoch 85 Batch 384 Loss 0.2897 Accuracy 0.3908\n",
      "Epoch 85 Batch 448 Loss 0.2915 Accuracy 0.3914\n",
      "Epoch 85 Batch 512 Loss 0.2980 Accuracy 0.3903\n",
      "Epoch 85 Batch 576 Loss 0.2969 Accuracy 0.3913\n",
      "Epoch 85 Batch 640 Loss 0.2969 Accuracy 0.3920\n",
      "Epoch 85 Batch 704 Loss 0.2979 Accuracy 0.3904\n",
      "Epoch 85 Batch 768 Loss 0.2975 Accuracy 0.3903\n",
      "Epoch 85 Batch 832 Loss 0.2980 Accuracy 0.3902\n",
      "Epoch 85 Batch 896 Loss 0.2998 Accuracy 0.3891\n",
      "Epoch 85 Batch 960 Loss 0.2986 Accuracy 0.3891\n",
      "Epoch 85 Batch 1024 Loss 0.3007 Accuracy 0.3883\n",
      "Epoch 85 Batch 1088 Loss 0.2991 Accuracy 0.3871\n",
      "Epoch 85 Batch 1152 Loss 0.2978 Accuracy 0.3862\n",
      "Epoch 85 Batch 1216 Loss 0.2981 Accuracy 0.3858\n",
      "Epoch 85 Batch 1280 Loss 0.2981 Accuracy 0.3859\n",
      "Epoch 85 Batch 1344 Loss 0.2978 Accuracy 0.3860\n",
      "Epoch 85 Batch 1408 Loss 0.2985 Accuracy 0.3856\n",
      "Epoch 85 Batch 1472 Loss 0.2989 Accuracy 0.3851\n",
      "Epoch 85 Batch 1536 Loss 0.2984 Accuracy 0.3850\n",
      "Epoch 85 Batch 1600 Loss 0.2984 Accuracy 0.3851\n",
      "Epoch 85 Batch 1664 Loss 0.2992 Accuracy 0.3851\n",
      "Epoch 85 Batch 1728 Loss 0.2984 Accuracy 0.3855\n",
      "Epoch 85 Batch 1792 Loss 0.2993 Accuracy 0.3861\n",
      "Epoch 85 Batch 1856 Loss 0.2990 Accuracy 0.3856\n",
      "Epoch 85 Batch 1920 Loss 0.2979 Accuracy 0.3856\n",
      "Epoch 85 Batch 1984 Loss 0.2986 Accuracy 0.3855\n",
      "Epoch 85 Batch 2048 Loss 0.2987 Accuracy 0.3852\n",
      "Epoch 85 Batch 2112 Loss 0.2983 Accuracy 0.3857\n",
      "Epoch 85 Batch 2176 Loss 0.2996 Accuracy 0.3855\n",
      "Epoch 85 Batch 2240 Loss 0.2998 Accuracy 0.3855\n",
      "Epoch 85 Batch 2304 Loss 0.2997 Accuracy 0.3856\n",
      "Epoch 85 Batch 2368 Loss 0.2999 Accuracy 0.3852\n",
      "Epoch 85 Batch 2432 Loss 0.3000 Accuracy 0.3851\n",
      "Epoch 85 Batch 2496 Loss 0.3001 Accuracy 0.3853\n",
      "Epoch 85 Batch 2560 Loss 0.3002 Accuracy 0.3856\n",
      "Epoch 85 Batch 2624 Loss 0.3004 Accuracy 0.3859\n",
      "Epoch 85 Batch 2688 Loss 0.2993 Accuracy 0.3858\n",
      "Epoch 85 Batch 2752 Loss 0.2982 Accuracy 0.3860\n",
      "Epoch 85 Batch 2816 Loss 0.2980 Accuracy 0.3865\n",
      "Epoch 85 Batch 2880 Loss 0.2989 Accuracy 0.3860\n",
      "Epoch 85 Batch 2944 Loss 0.2982 Accuracy 0.3860\n",
      "Epoch 85 Batch 3008 Loss 0.2979 Accuracy 0.3857\n",
      "Epoch 85 Batch 3072 Loss 0.2976 Accuracy 0.3853\n",
      "Epoch 85 Batch 3136 Loss 0.2980 Accuracy 0.3856\n",
      "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
      "Epoch 85 Loss 0.2980 Accuracy 0.3856\n",
      "Time taken for 1 epoch: 36.56029415130615 secs\n",
      "\n",
      "Epoch 86 Batch 64 Loss 0.2879 Accuracy 0.3800\n",
      "Epoch 86 Batch 128 Loss 0.2835 Accuracy 0.3886\n",
      "Epoch 86 Batch 192 Loss 0.2864 Accuracy 0.3902\n",
      "Epoch 86 Batch 256 Loss 0.2804 Accuracy 0.3889\n",
      "Epoch 86 Batch 320 Loss 0.2815 Accuracy 0.3894\n",
      "Epoch 86 Batch 384 Loss 0.2841 Accuracy 0.3917\n",
      "Epoch 86 Batch 448 Loss 0.2877 Accuracy 0.3927\n",
      "Epoch 86 Batch 512 Loss 0.2838 Accuracy 0.3909\n",
      "Epoch 86 Batch 576 Loss 0.2851 Accuracy 0.3897\n",
      "Epoch 86 Batch 640 Loss 0.2890 Accuracy 0.3882\n",
      "Epoch 86 Batch 704 Loss 0.2855 Accuracy 0.3890\n",
      "Epoch 86 Batch 768 Loss 0.2866 Accuracy 0.3883\n",
      "Epoch 86 Batch 832 Loss 0.2859 Accuracy 0.3882\n",
      "Epoch 86 Batch 896 Loss 0.2875 Accuracy 0.3873\n",
      "Epoch 86 Batch 960 Loss 0.2888 Accuracy 0.3873\n",
      "Epoch 86 Batch 1024 Loss 0.2879 Accuracy 0.3885\n",
      "Epoch 86 Batch 1088 Loss 0.2863 Accuracy 0.3880\n",
      "Epoch 86 Batch 1152 Loss 0.2852 Accuracy 0.3871\n",
      "Epoch 86 Batch 1216 Loss 0.2853 Accuracy 0.3874\n",
      "Epoch 86 Batch 1280 Loss 0.2862 Accuracy 0.3879\n",
      "Epoch 86 Batch 1344 Loss 0.2873 Accuracy 0.3880\n",
      "Epoch 86 Batch 1408 Loss 0.2873 Accuracy 0.3880\n",
      "Epoch 86 Batch 1472 Loss 0.2860 Accuracy 0.3881\n",
      "Epoch 86 Batch 1536 Loss 0.2865 Accuracy 0.3876\n",
      "Epoch 86 Batch 1600 Loss 0.2861 Accuracy 0.3878\n",
      "Epoch 86 Batch 1664 Loss 0.2862 Accuracy 0.3878\n",
      "Epoch 86 Batch 1728 Loss 0.2847 Accuracy 0.3876\n",
      "Epoch 86 Batch 1792 Loss 0.2863 Accuracy 0.3871\n",
      "Epoch 86 Batch 1856 Loss 0.2862 Accuracy 0.3878\n",
      "Epoch 86 Batch 1920 Loss 0.2861 Accuracy 0.3878\n",
      "Epoch 86 Batch 1984 Loss 0.2864 Accuracy 0.3875\n",
      "Epoch 86 Batch 2048 Loss 0.2879 Accuracy 0.3872\n",
      "Epoch 86 Batch 2112 Loss 0.2874 Accuracy 0.3869\n",
      "Epoch 86 Batch 2176 Loss 0.2877 Accuracy 0.3864\n",
      "Epoch 86 Batch 2240 Loss 0.2884 Accuracy 0.3860\n",
      "Epoch 86 Batch 2304 Loss 0.2880 Accuracy 0.3862\n",
      "Epoch 86 Batch 2368 Loss 0.2873 Accuracy 0.3861\n",
      "Epoch 86 Batch 2432 Loss 0.2870 Accuracy 0.3861\n",
      "Epoch 86 Batch 2496 Loss 0.2878 Accuracy 0.3859\n",
      "Epoch 86 Batch 2560 Loss 0.2886 Accuracy 0.3858\n",
      "Epoch 86 Batch 2624 Loss 0.2886 Accuracy 0.3862\n",
      "Epoch 86 Batch 2688 Loss 0.2884 Accuracy 0.3863\n",
      "Epoch 86 Batch 2752 Loss 0.2895 Accuracy 0.3861\n",
      "Epoch 86 Batch 2816 Loss 0.2902 Accuracy 0.3860\n",
      "Epoch 86 Batch 2880 Loss 0.2901 Accuracy 0.3860\n",
      "Epoch 86 Batch 2944 Loss 0.2904 Accuracy 0.3864\n",
      "Epoch 86 Batch 3008 Loss 0.2901 Accuracy 0.3867\n",
      "Epoch 86 Batch 3072 Loss 0.2898 Accuracy 0.3865\n",
      "Epoch 86 Batch 3136 Loss 0.2894 Accuracy 0.3867\n",
      "Epoch 86 Loss 0.2894 Accuracy 0.3867\n",
      "Time taken for 1 epoch: 37.247456073760986 secs\n",
      "\n",
      "Epoch 87 Batch 64 Loss 0.2691 Accuracy 0.3854\n",
      "Epoch 87 Batch 128 Loss 0.2722 Accuracy 0.3845\n",
      "Epoch 87 Batch 192 Loss 0.2672 Accuracy 0.3842\n",
      "Epoch 87 Batch 256 Loss 0.2770 Accuracy 0.3880\n",
      "Epoch 87 Batch 320 Loss 0.2816 Accuracy 0.3877\n",
      "Epoch 87 Batch 384 Loss 0.2763 Accuracy 0.3886\n",
      "Epoch 87 Batch 448 Loss 0.2766 Accuracy 0.3879\n",
      "Epoch 87 Batch 512 Loss 0.2773 Accuracy 0.3873\n",
      "Epoch 87 Batch 576 Loss 0.2756 Accuracy 0.3894\n",
      "Epoch 87 Batch 640 Loss 0.2774 Accuracy 0.3893\n",
      "Epoch 87 Batch 704 Loss 0.2768 Accuracy 0.3908\n",
      "Epoch 87 Batch 768 Loss 0.2760 Accuracy 0.3892\n",
      "Epoch 87 Batch 832 Loss 0.2760 Accuracy 0.3897\n",
      "Epoch 87 Batch 896 Loss 0.2772 Accuracy 0.3899\n",
      "Epoch 87 Batch 960 Loss 0.2761 Accuracy 0.3899\n",
      "Epoch 87 Batch 1024 Loss 0.2761 Accuracy 0.3890\n",
      "Epoch 87 Batch 1088 Loss 0.2751 Accuracy 0.3897\n",
      "Epoch 87 Batch 1152 Loss 0.2753 Accuracy 0.3898\n",
      "Epoch 87 Batch 1216 Loss 0.2749 Accuracy 0.3886\n",
      "Epoch 87 Batch 1280 Loss 0.2738 Accuracy 0.3889\n",
      "Epoch 87 Batch 1344 Loss 0.2738 Accuracy 0.3883\n",
      "Epoch 87 Batch 1408 Loss 0.2717 Accuracy 0.3887\n",
      "Epoch 87 Batch 1472 Loss 0.2728 Accuracy 0.3893\n",
      "Epoch 87 Batch 1536 Loss 0.2745 Accuracy 0.3891\n",
      "Epoch 87 Batch 1600 Loss 0.2745 Accuracy 0.3890\n",
      "Epoch 87 Batch 1664 Loss 0.2744 Accuracy 0.3889\n",
      "Epoch 87 Batch 1728 Loss 0.2744 Accuracy 0.3887\n",
      "Epoch 87 Batch 1792 Loss 0.2745 Accuracy 0.3889\n",
      "Epoch 87 Batch 1856 Loss 0.2741 Accuracy 0.3884\n",
      "Epoch 87 Batch 1920 Loss 0.2739 Accuracy 0.3885\n",
      "Epoch 87 Batch 1984 Loss 0.2740 Accuracy 0.3886\n",
      "Epoch 87 Batch 2048 Loss 0.2731 Accuracy 0.3891\n",
      "Epoch 87 Batch 2112 Loss 0.2732 Accuracy 0.3894\n",
      "Epoch 87 Batch 2176 Loss 0.2732 Accuracy 0.3892\n",
      "Epoch 87 Batch 2240 Loss 0.2731 Accuracy 0.3893\n",
      "Epoch 87 Batch 2304 Loss 0.2738 Accuracy 0.3893\n",
      "Epoch 87 Batch 2368 Loss 0.2749 Accuracy 0.3890\n",
      "Epoch 87 Batch 2432 Loss 0.2748 Accuracy 0.3887\n",
      "Epoch 87 Batch 2496 Loss 0.2758 Accuracy 0.3883\n",
      "Epoch 87 Batch 2560 Loss 0.2749 Accuracy 0.3885\n",
      "Epoch 87 Batch 2624 Loss 0.2753 Accuracy 0.3887\n",
      "Epoch 87 Batch 2688 Loss 0.2753 Accuracy 0.3888\n",
      "Epoch 87 Batch 2752 Loss 0.2749 Accuracy 0.3886\n",
      "Epoch 87 Batch 2816 Loss 0.2754 Accuracy 0.3886\n",
      "Epoch 87 Batch 2880 Loss 0.2764 Accuracy 0.3887\n",
      "Epoch 87 Batch 2944 Loss 0.2772 Accuracy 0.3885\n",
      "Epoch 87 Batch 3008 Loss 0.2773 Accuracy 0.3885\n",
      "Epoch 87 Batch 3072 Loss 0.2771 Accuracy 0.3885\n",
      "Epoch 87 Batch 3136 Loss 0.2773 Accuracy 0.3888\n",
      "Epoch 87 Loss 0.2773 Accuracy 0.3888\n",
      "Time taken for 1 epoch: 36.87667417526245 secs\n",
      "\n",
      "Epoch 88 Batch 64 Loss 0.2513 Accuracy 0.3865\n",
      "Epoch 88 Batch 128 Loss 0.2595 Accuracy 0.3915\n",
      "Epoch 88 Batch 192 Loss 0.2662 Accuracy 0.3914\n",
      "Epoch 88 Batch 256 Loss 0.2619 Accuracy 0.3918\n",
      "Epoch 88 Batch 320 Loss 0.2639 Accuracy 0.3907\n",
      "Epoch 88 Batch 384 Loss 0.2598 Accuracy 0.3926\n",
      "Epoch 88 Batch 448 Loss 0.2623 Accuracy 0.3904\n",
      "Epoch 88 Batch 512 Loss 0.2628 Accuracy 0.3910\n",
      "Epoch 88 Batch 576 Loss 0.2660 Accuracy 0.3904\n",
      "Epoch 88 Batch 640 Loss 0.2656 Accuracy 0.3901\n",
      "Epoch 88 Batch 704 Loss 0.2681 Accuracy 0.3888\n",
      "Epoch 88 Batch 768 Loss 0.2670 Accuracy 0.3899\n",
      "Epoch 88 Batch 832 Loss 0.2674 Accuracy 0.3903\n",
      "Epoch 88 Batch 896 Loss 0.2662 Accuracy 0.3907\n",
      "Epoch 88 Batch 960 Loss 0.2668 Accuracy 0.3906\n",
      "Epoch 88 Batch 1024 Loss 0.2659 Accuracy 0.3906\n",
      "Epoch 88 Batch 1088 Loss 0.2643 Accuracy 0.3910\n",
      "Epoch 88 Batch 1152 Loss 0.2652 Accuracy 0.3906\n",
      "Epoch 88 Batch 1216 Loss 0.2671 Accuracy 0.3898\n",
      "Epoch 88 Batch 1280 Loss 0.2671 Accuracy 0.3902\n",
      "Epoch 88 Batch 1344 Loss 0.2677 Accuracy 0.3906\n",
      "Epoch 88 Batch 1408 Loss 0.2662 Accuracy 0.3905\n",
      "Epoch 88 Batch 1472 Loss 0.2653 Accuracy 0.3905\n",
      "Epoch 88 Batch 1536 Loss 0.2659 Accuracy 0.3899\n",
      "Epoch 88 Batch 1600 Loss 0.2673 Accuracy 0.3897\n",
      "Epoch 88 Batch 1664 Loss 0.2674 Accuracy 0.3890\n",
      "Epoch 88 Batch 1728 Loss 0.2679 Accuracy 0.3891\n",
      "Epoch 88 Batch 1792 Loss 0.2673 Accuracy 0.3890\n",
      "Epoch 88 Batch 1856 Loss 0.2670 Accuracy 0.3891\n",
      "Epoch 88 Batch 1920 Loss 0.2671 Accuracy 0.3895\n",
      "Epoch 88 Batch 1984 Loss 0.2665 Accuracy 0.3895\n",
      "Epoch 88 Batch 2048 Loss 0.2666 Accuracy 0.3896\n",
      "Epoch 88 Batch 2112 Loss 0.2667 Accuracy 0.3898\n",
      "Epoch 88 Batch 2176 Loss 0.2671 Accuracy 0.3895\n",
      "Epoch 88 Batch 2240 Loss 0.2679 Accuracy 0.3899\n",
      "Epoch 88 Batch 2304 Loss 0.2684 Accuracy 0.3898\n",
      "Epoch 88 Batch 2368 Loss 0.2691 Accuracy 0.3899\n",
      "Epoch 88 Batch 2432 Loss 0.2696 Accuracy 0.3902\n",
      "Epoch 88 Batch 2496 Loss 0.2689 Accuracy 0.3898\n",
      "Epoch 88 Batch 2560 Loss 0.2687 Accuracy 0.3898\n",
      "Epoch 88 Batch 2624 Loss 0.2690 Accuracy 0.3896\n",
      "Epoch 88 Batch 2688 Loss 0.2690 Accuracy 0.3896\n",
      "Epoch 88 Batch 2752 Loss 0.2692 Accuracy 0.3898\n",
      "Epoch 88 Batch 2816 Loss 0.2698 Accuracy 0.3899\n",
      "Epoch 88 Batch 2880 Loss 0.2698 Accuracy 0.3898\n",
      "Epoch 88 Batch 2944 Loss 0.2695 Accuracy 0.3899\n",
      "Epoch 88 Batch 3008 Loss 0.2695 Accuracy 0.3898\n",
      "Epoch 88 Batch 3072 Loss 0.2690 Accuracy 0.3900\n",
      "Epoch 88 Batch 3136 Loss 0.2693 Accuracy 0.3900\n",
      "Epoch 88 Loss 0.2693 Accuracy 0.3900\n",
      "Time taken for 1 epoch: 38.235063791275024 secs\n",
      "\n",
      "Epoch 89 Batch 64 Loss 0.2824 Accuracy 0.3997\n",
      "Epoch 89 Batch 128 Loss 0.2729 Accuracy 0.3861\n",
      "Epoch 89 Batch 192 Loss 0.2673 Accuracy 0.3922\n",
      "Epoch 89 Batch 256 Loss 0.2676 Accuracy 0.3909\n",
      "Epoch 89 Batch 320 Loss 0.2606 Accuracy 0.3909\n",
      "Epoch 89 Batch 384 Loss 0.2697 Accuracy 0.3894\n",
      "Epoch 89 Batch 448 Loss 0.2687 Accuracy 0.3884\n",
      "Epoch 89 Batch 512 Loss 0.2693 Accuracy 0.3891\n",
      "Epoch 89 Batch 576 Loss 0.2708 Accuracy 0.3906\n",
      "Epoch 89 Batch 640 Loss 0.2744 Accuracy 0.3881\n",
      "Epoch 89 Batch 704 Loss 0.2759 Accuracy 0.3888\n",
      "Epoch 89 Batch 768 Loss 0.2768 Accuracy 0.3871\n",
      "Epoch 89 Batch 832 Loss 0.2747 Accuracy 0.3871\n",
      "Epoch 89 Batch 896 Loss 0.2762 Accuracy 0.3869\n",
      "Epoch 89 Batch 960 Loss 0.2756 Accuracy 0.3870\n",
      "Epoch 89 Batch 1024 Loss 0.2751 Accuracy 0.3871\n",
      "Epoch 89 Batch 1088 Loss 0.2765 Accuracy 0.3886\n",
      "Epoch 89 Batch 1152 Loss 0.2777 Accuracy 0.3883\n",
      "Epoch 89 Batch 1216 Loss 0.2789 Accuracy 0.3882\n",
      "Epoch 89 Batch 1280 Loss 0.2767 Accuracy 0.3884\n",
      "Epoch 89 Batch 1344 Loss 0.2760 Accuracy 0.3881\n",
      "Epoch 89 Batch 1408 Loss 0.2761 Accuracy 0.3880\n",
      "Epoch 89 Batch 1472 Loss 0.2756 Accuracy 0.3882\n",
      "Epoch 89 Batch 1536 Loss 0.2751 Accuracy 0.3885\n",
      "Epoch 89 Batch 1600 Loss 0.2763 Accuracy 0.3880\n",
      "Epoch 89 Batch 1664 Loss 0.2764 Accuracy 0.3886\n",
      "Epoch 89 Batch 1728 Loss 0.2764 Accuracy 0.3882\n",
      "Epoch 89 Batch 1792 Loss 0.2758 Accuracy 0.3880\n",
      "Epoch 89 Batch 1856 Loss 0.2745 Accuracy 0.3875\n",
      "Epoch 89 Batch 1920 Loss 0.2743 Accuracy 0.3875\n",
      "Epoch 89 Batch 1984 Loss 0.2736 Accuracy 0.3889\n",
      "Epoch 89 Batch 2048 Loss 0.2739 Accuracy 0.3889\n",
      "Epoch 89 Batch 2112 Loss 0.2733 Accuracy 0.3888\n",
      "Epoch 89 Batch 2176 Loss 0.2731 Accuracy 0.3887\n",
      "Epoch 89 Batch 2240 Loss 0.2725 Accuracy 0.3892\n",
      "Epoch 89 Batch 2304 Loss 0.2721 Accuracy 0.3893\n",
      "Epoch 89 Batch 2368 Loss 0.2728 Accuracy 0.3894\n",
      "Epoch 89 Batch 2432 Loss 0.2730 Accuracy 0.3897\n",
      "Epoch 89 Batch 2496 Loss 0.2735 Accuracy 0.3900\n",
      "Epoch 89 Batch 2560 Loss 0.2733 Accuracy 0.3898\n",
      "Epoch 89 Batch 2624 Loss 0.2729 Accuracy 0.3901\n",
      "Epoch 89 Batch 2688 Loss 0.2731 Accuracy 0.3901\n",
      "Epoch 89 Batch 2752 Loss 0.2721 Accuracy 0.3900\n",
      "Epoch 89 Batch 2816 Loss 0.2713 Accuracy 0.3902\n",
      "Epoch 89 Batch 2880 Loss 0.2714 Accuracy 0.3896\n",
      "Epoch 89 Batch 2944 Loss 0.2707 Accuracy 0.3898\n",
      "Epoch 89 Batch 3008 Loss 0.2704 Accuracy 0.3898\n",
      "Epoch 89 Batch 3072 Loss 0.2696 Accuracy 0.3901\n",
      "Epoch 89 Batch 3136 Loss 0.2695 Accuracy 0.3899\n",
      "Epoch 89 Loss 0.2695 Accuracy 0.3899\n",
      "Time taken for 1 epoch: 35.838435888290405 secs\n",
      "\n",
      "Epoch 90 Batch 64 Loss 0.2502 Accuracy 0.3961\n",
      "Epoch 90 Batch 128 Loss 0.2429 Accuracy 0.3913\n",
      "Epoch 90 Batch 192 Loss 0.2456 Accuracy 0.3909\n",
      "Epoch 90 Batch 256 Loss 0.2571 Accuracy 0.3957\n",
      "Epoch 90 Batch 320 Loss 0.2602 Accuracy 0.3940\n",
      "Epoch 90 Batch 384 Loss 0.2551 Accuracy 0.3945\n",
      "Epoch 90 Batch 448 Loss 0.2579 Accuracy 0.3932\n",
      "Epoch 90 Batch 512 Loss 0.2590 Accuracy 0.3920\n",
      "Epoch 90 Batch 576 Loss 0.2588 Accuracy 0.3910\n",
      "Epoch 90 Batch 640 Loss 0.2633 Accuracy 0.3918\n",
      "Epoch 90 Batch 704 Loss 0.2615 Accuracy 0.3913\n",
      "Epoch 90 Batch 768 Loss 0.2621 Accuracy 0.3908\n",
      "Epoch 90 Batch 832 Loss 0.2608 Accuracy 0.3908\n",
      "Epoch 90 Batch 896 Loss 0.2619 Accuracy 0.3895\n",
      "Epoch 90 Batch 960 Loss 0.2629 Accuracy 0.3895\n",
      "Epoch 90 Batch 1024 Loss 0.2629 Accuracy 0.3886\n",
      "Epoch 90 Batch 1088 Loss 0.2633 Accuracy 0.3888\n",
      "Epoch 90 Batch 1152 Loss 0.2638 Accuracy 0.3888\n",
      "Epoch 90 Batch 1216 Loss 0.2629 Accuracy 0.3895\n",
      "Epoch 90 Batch 1280 Loss 0.2638 Accuracy 0.3900\n",
      "Epoch 90 Batch 1344 Loss 0.2630 Accuracy 0.3895\n",
      "Epoch 90 Batch 1408 Loss 0.2625 Accuracy 0.3903\n",
      "Epoch 90 Batch 1472 Loss 0.2629 Accuracy 0.3901\n",
      "Epoch 90 Batch 1536 Loss 0.2613 Accuracy 0.3906\n",
      "Epoch 90 Batch 1600 Loss 0.2623 Accuracy 0.3907\n",
      "Epoch 90 Batch 1664 Loss 0.2605 Accuracy 0.3910\n",
      "Epoch 90 Batch 1728 Loss 0.2604 Accuracy 0.3912\n",
      "Epoch 90 Batch 1792 Loss 0.2600 Accuracy 0.3906\n",
      "Epoch 90 Batch 1856 Loss 0.2592 Accuracy 0.3906\n",
      "Epoch 90 Batch 1920 Loss 0.2594 Accuracy 0.3906\n",
      "Epoch 90 Batch 1984 Loss 0.2588 Accuracy 0.3907\n",
      "Epoch 90 Batch 2048 Loss 0.2595 Accuracy 0.3903\n",
      "Epoch 90 Batch 2112 Loss 0.2594 Accuracy 0.3904\n",
      "Epoch 90 Batch 2176 Loss 0.2595 Accuracy 0.3908\n",
      "Epoch 90 Batch 2240 Loss 0.2605 Accuracy 0.3907\n",
      "Epoch 90 Batch 2304 Loss 0.2602 Accuracy 0.3905\n",
      "Epoch 90 Batch 2368 Loss 0.2607 Accuracy 0.3905\n",
      "Epoch 90 Batch 2432 Loss 0.2611 Accuracy 0.3906\n",
      "Epoch 90 Batch 2496 Loss 0.2615 Accuracy 0.3905\n",
      "Epoch 90 Batch 2560 Loss 0.2615 Accuracy 0.3906\n",
      "Epoch 90 Batch 2624 Loss 0.2612 Accuracy 0.3910\n",
      "Epoch 90 Batch 2688 Loss 0.2616 Accuracy 0.3909\n",
      "Epoch 90 Batch 2752 Loss 0.2616 Accuracy 0.3909\n",
      "Epoch 90 Batch 2816 Loss 0.2623 Accuracy 0.3910\n",
      "Epoch 90 Batch 2880 Loss 0.2636 Accuracy 0.3911\n",
      "Epoch 90 Batch 2944 Loss 0.2628 Accuracy 0.3911\n",
      "Epoch 90 Batch 3008 Loss 0.2629 Accuracy 0.3908\n",
      "Epoch 90 Batch 3072 Loss 0.2642 Accuracy 0.3906\n",
      "Epoch 90 Batch 3136 Loss 0.2640 Accuracy 0.3909\n",
      "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
      "Epoch 90 Loss 0.2640 Accuracy 0.3909\n",
      "Time taken for 1 epoch: 37.089146852493286 secs\n",
      "\n",
      "Epoch 91 Batch 64 Loss 0.2517 Accuracy 0.4006\n",
      "Epoch 91 Batch 128 Loss 0.2685 Accuracy 0.3871\n",
      "Epoch 91 Batch 192 Loss 0.2608 Accuracy 0.3934\n",
      "Epoch 91 Batch 256 Loss 0.2623 Accuracy 0.3946\n",
      "Epoch 91 Batch 320 Loss 0.2658 Accuracy 0.3970\n",
      "Epoch 91 Batch 384 Loss 0.2701 Accuracy 0.3946\n",
      "Epoch 91 Batch 448 Loss 0.2643 Accuracy 0.3942\n",
      "Epoch 91 Batch 512 Loss 0.2642 Accuracy 0.3946\n",
      "Epoch 91 Batch 576 Loss 0.2628 Accuracy 0.3935\n",
      "Epoch 91 Batch 640 Loss 0.2627 Accuracy 0.3925\n",
      "Epoch 91 Batch 704 Loss 0.2640 Accuracy 0.3915\n",
      "Epoch 91 Batch 768 Loss 0.2627 Accuracy 0.3919\n",
      "Epoch 91 Batch 832 Loss 0.2653 Accuracy 0.3920\n",
      "Epoch 91 Batch 896 Loss 0.2641 Accuracy 0.3913\n",
      "Epoch 91 Batch 960 Loss 0.2624 Accuracy 0.3918\n",
      "Epoch 91 Batch 1024 Loss 0.2629 Accuracy 0.3910\n",
      "Epoch 91 Batch 1088 Loss 0.2628 Accuracy 0.3910\n",
      "Epoch 91 Batch 1152 Loss 0.2615 Accuracy 0.3911\n",
      "Epoch 91 Batch 1216 Loss 0.2634 Accuracy 0.3903\n",
      "Epoch 91 Batch 1280 Loss 0.2637 Accuracy 0.3893\n",
      "Epoch 91 Batch 1344 Loss 0.2674 Accuracy 0.3893\n",
      "Epoch 91 Batch 1408 Loss 0.2671 Accuracy 0.3893\n",
      "Epoch 91 Batch 1472 Loss 0.2665 Accuracy 0.3895\n",
      "Epoch 91 Batch 1536 Loss 0.2666 Accuracy 0.3892\n",
      "Epoch 91 Batch 1600 Loss 0.2670 Accuracy 0.3892\n",
      "Epoch 91 Batch 1664 Loss 0.2678 Accuracy 0.3895\n",
      "Epoch 91 Batch 1728 Loss 0.2680 Accuracy 0.3891\n",
      "Epoch 91 Batch 1792 Loss 0.2690 Accuracy 0.3893\n",
      "Epoch 91 Batch 1856 Loss 0.2674 Accuracy 0.3892\n",
      "Epoch 91 Batch 1920 Loss 0.2683 Accuracy 0.3893\n",
      "Epoch 91 Batch 1984 Loss 0.2677 Accuracy 0.3893\n",
      "Epoch 91 Batch 2048 Loss 0.2668 Accuracy 0.3892\n",
      "Epoch 91 Batch 2112 Loss 0.2662 Accuracy 0.3899\n",
      "Epoch 91 Batch 2176 Loss 0.2660 Accuracy 0.3895\n",
      "Epoch 91 Batch 2240 Loss 0.2660 Accuracy 0.3895\n",
      "Epoch 91 Batch 2304 Loss 0.2666 Accuracy 0.3897\n",
      "Epoch 91 Batch 2368 Loss 0.2671 Accuracy 0.3899\n",
      "Epoch 91 Batch 2432 Loss 0.2677 Accuracy 0.3897\n",
      "Epoch 91 Batch 2496 Loss 0.2682 Accuracy 0.3896\n",
      "Epoch 91 Batch 2560 Loss 0.2687 Accuracy 0.3893\n",
      "Epoch 91 Batch 2624 Loss 0.2680 Accuracy 0.3895\n",
      "Epoch 91 Batch 2688 Loss 0.2672 Accuracy 0.3901\n",
      "Epoch 91 Batch 2752 Loss 0.2662 Accuracy 0.3902\n",
      "Epoch 91 Batch 2816 Loss 0.2658 Accuracy 0.3904\n",
      "Epoch 91 Batch 2880 Loss 0.2660 Accuracy 0.3906\n",
      "Epoch 91 Batch 2944 Loss 0.2657 Accuracy 0.3908\n",
      "Epoch 91 Batch 3008 Loss 0.2651 Accuracy 0.3909\n",
      "Epoch 91 Batch 3072 Loss 0.2641 Accuracy 0.3909\n",
      "Epoch 91 Batch 3136 Loss 0.2633 Accuracy 0.3909\n",
      "Epoch 91 Loss 0.2633 Accuracy 0.3909\n",
      "Time taken for 1 epoch: 36.35501790046692 secs\n",
      "\n",
      "Epoch 92 Batch 64 Loss 0.2186 Accuracy 0.3956\n",
      "Epoch 92 Batch 128 Loss 0.2334 Accuracy 0.3949\n",
      "Epoch 92 Batch 192 Loss 0.2390 Accuracy 0.3965\n",
      "Epoch 92 Batch 256 Loss 0.2372 Accuracy 0.3935\n",
      "Epoch 92 Batch 320 Loss 0.2377 Accuracy 0.3925\n",
      "Epoch 92 Batch 384 Loss 0.2422 Accuracy 0.3926\n",
      "Epoch 92 Batch 448 Loss 0.2451 Accuracy 0.3921\n",
      "Epoch 92 Batch 512 Loss 0.2423 Accuracy 0.3923\n",
      "Epoch 92 Batch 576 Loss 0.2451 Accuracy 0.3890\n",
      "Epoch 92 Batch 640 Loss 0.2437 Accuracy 0.3882\n",
      "Epoch 92 Batch 704 Loss 0.2418 Accuracy 0.3885\n",
      "Epoch 92 Batch 768 Loss 0.2428 Accuracy 0.3893\n",
      "Epoch 92 Batch 832 Loss 0.2413 Accuracy 0.3896\n",
      "Epoch 92 Batch 896 Loss 0.2415 Accuracy 0.3894\n",
      "Epoch 92 Batch 960 Loss 0.2415 Accuracy 0.3894\n",
      "Epoch 92 Batch 1024 Loss 0.2394 Accuracy 0.3908\n",
      "Epoch 92 Batch 1088 Loss 0.2390 Accuracy 0.3908\n",
      "Epoch 92 Batch 1152 Loss 0.2415 Accuracy 0.3904\n",
      "Epoch 92 Batch 1216 Loss 0.2419 Accuracy 0.3904\n",
      "Epoch 92 Batch 1280 Loss 0.2442 Accuracy 0.3904\n",
      "Epoch 92 Batch 1344 Loss 0.2445 Accuracy 0.3901\n",
      "Epoch 92 Batch 1408 Loss 0.2460 Accuracy 0.3896\n",
      "Epoch 92 Batch 1472 Loss 0.2447 Accuracy 0.3894\n",
      "Epoch 92 Batch 1536 Loss 0.2444 Accuracy 0.3897\n",
      "Epoch 92 Batch 1600 Loss 0.2446 Accuracy 0.3899\n",
      "Epoch 92 Batch 1664 Loss 0.2444 Accuracy 0.3905\n",
      "Epoch 92 Batch 1728 Loss 0.2441 Accuracy 0.3909\n",
      "Epoch 92 Batch 1792 Loss 0.2442 Accuracy 0.3917\n",
      "Epoch 92 Batch 1856 Loss 0.2447 Accuracy 0.3913\n",
      "Epoch 92 Batch 1920 Loss 0.2449 Accuracy 0.3918\n",
      "Epoch 92 Batch 1984 Loss 0.2441 Accuracy 0.3918\n",
      "Epoch 92 Batch 2048 Loss 0.2441 Accuracy 0.3922\n",
      "Epoch 92 Batch 2112 Loss 0.2438 Accuracy 0.3922\n",
      "Epoch 92 Batch 2176 Loss 0.2437 Accuracy 0.3924\n",
      "Epoch 92 Batch 2240 Loss 0.2438 Accuracy 0.3926\n",
      "Epoch 92 Batch 2304 Loss 0.2442 Accuracy 0.3928\n",
      "Epoch 92 Batch 2368 Loss 0.2435 Accuracy 0.3935\n",
      "Epoch 92 Batch 2432 Loss 0.2425 Accuracy 0.3936\n",
      "Epoch 92 Batch 2496 Loss 0.2424 Accuracy 0.3935\n",
      "Epoch 92 Batch 2560 Loss 0.2421 Accuracy 0.3937\n",
      "Epoch 92 Batch 2624 Loss 0.2424 Accuracy 0.3933\n",
      "Epoch 92 Batch 2688 Loss 0.2426 Accuracy 0.3934\n",
      "Epoch 92 Batch 2752 Loss 0.2426 Accuracy 0.3931\n",
      "Epoch 92 Batch 2816 Loss 0.2434 Accuracy 0.3938\n",
      "Epoch 92 Batch 2880 Loss 0.2429 Accuracy 0.3936\n",
      "Epoch 92 Batch 2944 Loss 0.2436 Accuracy 0.3936\n",
      "Epoch 92 Batch 3008 Loss 0.2445 Accuracy 0.3936\n",
      "Epoch 92 Batch 3072 Loss 0.2448 Accuracy 0.3935\n",
      "Epoch 92 Batch 3136 Loss 0.2454 Accuracy 0.3938\n",
      "Epoch 92 Loss 0.2454 Accuracy 0.3938\n",
      "Time taken for 1 epoch: 37.660221338272095 secs\n",
      "\n",
      "Epoch 93 Batch 64 Loss 0.2456 Accuracy 0.3832\n",
      "Epoch 93 Batch 128 Loss 0.2404 Accuracy 0.3847\n",
      "Epoch 93 Batch 192 Loss 0.2440 Accuracy 0.3836\n",
      "Epoch 93 Batch 256 Loss 0.2441 Accuracy 0.3875\n",
      "Epoch 93 Batch 320 Loss 0.2387 Accuracy 0.3909\n",
      "Epoch 93 Batch 384 Loss 0.2383 Accuracy 0.3944\n",
      "Epoch 93 Batch 448 Loss 0.2441 Accuracy 0.3952\n",
      "Epoch 93 Batch 512 Loss 0.2440 Accuracy 0.3960\n",
      "Epoch 93 Batch 576 Loss 0.2458 Accuracy 0.3957\n",
      "Epoch 93 Batch 640 Loss 0.2418 Accuracy 0.3951\n",
      "Epoch 93 Batch 704 Loss 0.2444 Accuracy 0.3941\n",
      "Epoch 93 Batch 768 Loss 0.2452 Accuracy 0.3946\n",
      "Epoch 93 Batch 832 Loss 0.2458 Accuracy 0.3949\n",
      "Epoch 93 Batch 896 Loss 0.2473 Accuracy 0.3943\n",
      "Epoch 93 Batch 960 Loss 0.2462 Accuracy 0.3945\n",
      "Epoch 93 Batch 1024 Loss 0.2465 Accuracy 0.3934\n",
      "Epoch 93 Batch 1088 Loss 0.2473 Accuracy 0.3928\n",
      "Epoch 93 Batch 1152 Loss 0.2472 Accuracy 0.3927\n",
      "Epoch 93 Batch 1216 Loss 0.2494 Accuracy 0.3930\n",
      "Epoch 93 Batch 1280 Loss 0.2487 Accuracy 0.3933\n",
      "Epoch 93 Batch 1344 Loss 0.2496 Accuracy 0.3931\n",
      "Epoch 93 Batch 1408 Loss 0.2488 Accuracy 0.3937\n",
      "Epoch 93 Batch 1472 Loss 0.2483 Accuracy 0.3934\n",
      "Epoch 93 Batch 1536 Loss 0.2478 Accuracy 0.3931\n",
      "Epoch 93 Batch 1600 Loss 0.2482 Accuracy 0.3935\n",
      "Epoch 93 Batch 1664 Loss 0.2470 Accuracy 0.3931\n",
      "Epoch 93 Batch 1728 Loss 0.2475 Accuracy 0.3927\n",
      "Epoch 93 Batch 1792 Loss 0.2474 Accuracy 0.3929\n",
      "Epoch 93 Batch 1856 Loss 0.2465 Accuracy 0.3930\n",
      "Epoch 93 Batch 1920 Loss 0.2475 Accuracy 0.3932\n",
      "Epoch 93 Batch 1984 Loss 0.2464 Accuracy 0.3936\n",
      "Epoch 93 Batch 2048 Loss 0.2476 Accuracy 0.3936\n",
      "Epoch 93 Batch 2112 Loss 0.2475 Accuracy 0.3935\n",
      "Epoch 93 Batch 2176 Loss 0.2473 Accuracy 0.3935\n",
      "Epoch 93 Batch 2240 Loss 0.2470 Accuracy 0.3931\n",
      "Epoch 93 Batch 2304 Loss 0.2466 Accuracy 0.3934\n",
      "Epoch 93 Batch 2368 Loss 0.2465 Accuracy 0.3938\n",
      "Epoch 93 Batch 2432 Loss 0.2465 Accuracy 0.3936\n",
      "Epoch 93 Batch 2496 Loss 0.2460 Accuracy 0.3939\n",
      "Epoch 93 Batch 2560 Loss 0.2464 Accuracy 0.3939\n",
      "Epoch 93 Batch 2624 Loss 0.2463 Accuracy 0.3938\n",
      "Epoch 93 Batch 2688 Loss 0.2467 Accuracy 0.3938\n",
      "Epoch 93 Batch 2752 Loss 0.2469 Accuracy 0.3937\n",
      "Epoch 93 Batch 2816 Loss 0.2462 Accuracy 0.3940\n",
      "Epoch 93 Batch 2880 Loss 0.2456 Accuracy 0.3937\n",
      "Epoch 93 Batch 2944 Loss 0.2456 Accuracy 0.3940\n",
      "Epoch 93 Batch 3008 Loss 0.2451 Accuracy 0.3939\n",
      "Epoch 93 Batch 3072 Loss 0.2442 Accuracy 0.3938\n",
      "Epoch 93 Batch 3136 Loss 0.2439 Accuracy 0.3937\n",
      "Epoch 93 Loss 0.2439 Accuracy 0.3937\n",
      "Time taken for 1 epoch: 41.65850329399109 secs\n",
      "\n",
      "Epoch 94 Batch 64 Loss 0.2552 Accuracy 0.3931\n",
      "Epoch 94 Batch 128 Loss 0.2487 Accuracy 0.3890\n",
      "Epoch 94 Batch 192 Loss 0.2390 Accuracy 0.3921\n",
      "Epoch 94 Batch 256 Loss 0.2352 Accuracy 0.3912\n",
      "Epoch 94 Batch 320 Loss 0.2358 Accuracy 0.3869\n",
      "Epoch 94 Batch 384 Loss 0.2424 Accuracy 0.3881\n",
      "Epoch 94 Batch 448 Loss 0.2403 Accuracy 0.3896\n",
      "Epoch 94 Batch 512 Loss 0.2402 Accuracy 0.3910\n",
      "Epoch 94 Batch 576 Loss 0.2368 Accuracy 0.3914\n",
      "Epoch 94 Batch 640 Loss 0.2340 Accuracy 0.3917\n",
      "Epoch 94 Batch 704 Loss 0.2351 Accuracy 0.3918\n",
      "Epoch 94 Batch 768 Loss 0.2344 Accuracy 0.3916\n",
      "Epoch 94 Batch 832 Loss 0.2398 Accuracy 0.3914\n",
      "Epoch 94 Batch 896 Loss 0.2414 Accuracy 0.3929\n",
      "Epoch 94 Batch 960 Loss 0.2435 Accuracy 0.3914\n",
      "Epoch 94 Batch 1024 Loss 0.2436 Accuracy 0.3917\n",
      "Epoch 94 Batch 1088 Loss 0.2436 Accuracy 0.3916\n",
      "Epoch 94 Batch 1152 Loss 0.2436 Accuracy 0.3919\n",
      "Epoch 94 Batch 1216 Loss 0.2446 Accuracy 0.3922\n",
      "Epoch 94 Batch 1280 Loss 0.2434 Accuracy 0.3927\n",
      "Epoch 94 Batch 1344 Loss 0.2459 Accuracy 0.3925\n",
      "Epoch 94 Batch 1408 Loss 0.2433 Accuracy 0.3921\n",
      "Epoch 94 Batch 1472 Loss 0.2443 Accuracy 0.3923\n",
      "Epoch 94 Batch 1536 Loss 0.2429 Accuracy 0.3920\n",
      "Epoch 94 Batch 1600 Loss 0.2428 Accuracy 0.3927\n",
      "Epoch 94 Batch 1664 Loss 0.2426 Accuracy 0.3921\n",
      "Epoch 94 Batch 1728 Loss 0.2434 Accuracy 0.3931\n",
      "Epoch 94 Batch 1792 Loss 0.2415 Accuracy 0.3930\n",
      "Epoch 94 Batch 1856 Loss 0.2414 Accuracy 0.3932\n",
      "Epoch 94 Batch 1920 Loss 0.2412 Accuracy 0.3930\n",
      "Epoch 94 Batch 1984 Loss 0.2427 Accuracy 0.3935\n",
      "Epoch 94 Batch 2048 Loss 0.2431 Accuracy 0.3934\n",
      "Epoch 94 Batch 2112 Loss 0.2424 Accuracy 0.3931\n",
      "Epoch 94 Batch 2176 Loss 0.2421 Accuracy 0.3932\n",
      "Epoch 94 Batch 2240 Loss 0.2426 Accuracy 0.3933\n",
      "Epoch 94 Batch 2304 Loss 0.2420 Accuracy 0.3935\n",
      "Epoch 94 Batch 2368 Loss 0.2411 Accuracy 0.3936\n",
      "Epoch 94 Batch 2432 Loss 0.2412 Accuracy 0.3937\n",
      "Epoch 94 Batch 2496 Loss 0.2411 Accuracy 0.3934\n",
      "Epoch 94 Batch 2560 Loss 0.2409 Accuracy 0.3937\n",
      "Epoch 94 Batch 2624 Loss 0.2421 Accuracy 0.3933\n",
      "Epoch 94 Batch 2688 Loss 0.2415 Accuracy 0.3936\n",
      "Epoch 94 Batch 2752 Loss 0.2413 Accuracy 0.3934\n",
      "Epoch 94 Batch 2816 Loss 0.2415 Accuracy 0.3939\n",
      "Epoch 94 Batch 2880 Loss 0.2415 Accuracy 0.3937\n",
      "Epoch 94 Batch 2944 Loss 0.2415 Accuracy 0.3938\n",
      "Epoch 94 Batch 3008 Loss 0.2416 Accuracy 0.3936\n",
      "Epoch 94 Batch 3072 Loss 0.2421 Accuracy 0.3938\n",
      "Epoch 94 Batch 3136 Loss 0.2417 Accuracy 0.3939\n",
      "Epoch 94 Loss 0.2417 Accuracy 0.3939\n",
      "Time taken for 1 epoch: 36.40151906013489 secs\n",
      "\n",
      "Epoch 95 Batch 64 Loss 0.2216 Accuracy 0.3954\n",
      "Epoch 95 Batch 128 Loss 0.2406 Accuracy 0.3953\n",
      "Epoch 95 Batch 192 Loss 0.2354 Accuracy 0.3990\n",
      "Epoch 95 Batch 256 Loss 0.2380 Accuracy 0.3999\n",
      "Epoch 95 Batch 320 Loss 0.2360 Accuracy 0.3968\n",
      "Epoch 95 Batch 384 Loss 0.2330 Accuracy 0.3955\n",
      "Epoch 95 Batch 448 Loss 0.2373 Accuracy 0.3949\n",
      "Epoch 95 Batch 512 Loss 0.2364 Accuracy 0.3940\n",
      "Epoch 95 Batch 576 Loss 0.2353 Accuracy 0.3949\n",
      "Epoch 95 Batch 640 Loss 0.2395 Accuracy 0.3942\n",
      "Epoch 95 Batch 704 Loss 0.2432 Accuracy 0.3946\n",
      "Epoch 95 Batch 768 Loss 0.2419 Accuracy 0.3943\n",
      "Epoch 95 Batch 832 Loss 0.2436 Accuracy 0.3946\n",
      "Epoch 95 Batch 896 Loss 0.2407 Accuracy 0.3953\n",
      "Epoch 95 Batch 960 Loss 0.2416 Accuracy 0.3942\n",
      "Epoch 95 Batch 1024 Loss 0.2419 Accuracy 0.3946\n",
      "Epoch 95 Batch 1088 Loss 0.2404 Accuracy 0.3949\n",
      "Epoch 95 Batch 1152 Loss 0.2415 Accuracy 0.3947\n",
      "Epoch 95 Batch 1216 Loss 0.2398 Accuracy 0.3954\n",
      "Epoch 95 Batch 1280 Loss 0.2391 Accuracy 0.3949\n",
      "Epoch 95 Batch 1344 Loss 0.2380 Accuracy 0.3950\n",
      "Epoch 95 Batch 1408 Loss 0.2368 Accuracy 0.3952\n",
      "Epoch 95 Batch 1472 Loss 0.2354 Accuracy 0.3957\n",
      "Epoch 95 Batch 1536 Loss 0.2370 Accuracy 0.3958\n",
      "Epoch 95 Batch 1600 Loss 0.2361 Accuracy 0.3950\n",
      "Epoch 95 Batch 1664 Loss 0.2372 Accuracy 0.3945\n",
      "Epoch 95 Batch 1728 Loss 0.2373 Accuracy 0.3943\n",
      "Epoch 95 Batch 1792 Loss 0.2380 Accuracy 0.3944\n",
      "Epoch 95 Batch 1856 Loss 0.2365 Accuracy 0.3941\n",
      "Epoch 95 Batch 1920 Loss 0.2362 Accuracy 0.3941\n",
      "Epoch 95 Batch 1984 Loss 0.2361 Accuracy 0.3938\n",
      "Epoch 95 Batch 2048 Loss 0.2362 Accuracy 0.3936\n",
      "Epoch 95 Batch 2112 Loss 0.2360 Accuracy 0.3938\n",
      "Epoch 95 Batch 2176 Loss 0.2354 Accuracy 0.3940\n",
      "Epoch 95 Batch 2240 Loss 0.2353 Accuracy 0.3946\n",
      "Epoch 95 Batch 2304 Loss 0.2352 Accuracy 0.3944\n",
      "Epoch 95 Batch 2368 Loss 0.2352 Accuracy 0.3943\n",
      "Epoch 95 Batch 2432 Loss 0.2348 Accuracy 0.3942\n",
      "Epoch 95 Batch 2496 Loss 0.2342 Accuracy 0.3946\n",
      "Epoch 95 Batch 2560 Loss 0.2345 Accuracy 0.3945\n",
      "Epoch 95 Batch 2624 Loss 0.2341 Accuracy 0.3943\n",
      "Epoch 95 Batch 2688 Loss 0.2342 Accuracy 0.3945\n",
      "Epoch 95 Batch 2752 Loss 0.2344 Accuracy 0.3947\n",
      "Epoch 95 Batch 2816 Loss 0.2337 Accuracy 0.3947\n",
      "Epoch 95 Batch 2880 Loss 0.2346 Accuracy 0.3946\n",
      "Epoch 95 Batch 2944 Loss 0.2343 Accuracy 0.3952\n",
      "Epoch 95 Batch 3008 Loss 0.2339 Accuracy 0.3950\n",
      "Epoch 95 Batch 3072 Loss 0.2336 Accuracy 0.3951\n",
      "Epoch 95 Batch 3136 Loss 0.2333 Accuracy 0.3950\n",
      "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
      "Epoch 95 Loss 0.2333 Accuracy 0.3950\n",
      "Time taken for 1 epoch: 37.812458992004395 secs\n",
      "\n",
      "Epoch 96 Batch 64 Loss 0.1987 Accuracy 0.4037\n",
      "Epoch 96 Batch 128 Loss 0.1955 Accuracy 0.3956\n",
      "Epoch 96 Batch 192 Loss 0.2002 Accuracy 0.3943\n",
      "Epoch 96 Batch 256 Loss 0.2038 Accuracy 0.3947\n",
      "Epoch 96 Batch 320 Loss 0.2040 Accuracy 0.3965\n",
      "Epoch 96 Batch 384 Loss 0.2062 Accuracy 0.3966\n",
      "Epoch 96 Batch 448 Loss 0.2099 Accuracy 0.3983\n",
      "Epoch 96 Batch 512 Loss 0.2073 Accuracy 0.3986\n",
      "Epoch 96 Batch 576 Loss 0.2120 Accuracy 0.3993\n",
      "Epoch 96 Batch 640 Loss 0.2140 Accuracy 0.3996\n",
      "Epoch 96 Batch 704 Loss 0.2150 Accuracy 0.3986\n",
      "Epoch 96 Batch 768 Loss 0.2153 Accuracy 0.3984\n",
      "Epoch 96 Batch 832 Loss 0.2172 Accuracy 0.3973\n",
      "Epoch 96 Batch 896 Loss 0.2176 Accuracy 0.3984\n",
      "Epoch 96 Batch 960 Loss 0.2169 Accuracy 0.3979\n",
      "Epoch 96 Batch 1024 Loss 0.2171 Accuracy 0.3980\n",
      "Epoch 96 Batch 1088 Loss 0.2166 Accuracy 0.3976\n",
      "Epoch 96 Batch 1152 Loss 0.2172 Accuracy 0.3974\n",
      "Epoch 96 Batch 1216 Loss 0.2177 Accuracy 0.3968\n",
      "Epoch 96 Batch 1280 Loss 0.2192 Accuracy 0.3963\n",
      "Epoch 96 Batch 1344 Loss 0.2196 Accuracy 0.3966\n",
      "Epoch 96 Batch 1408 Loss 0.2203 Accuracy 0.3972\n",
      "Epoch 96 Batch 1472 Loss 0.2191 Accuracy 0.3966\n",
      "Epoch 96 Batch 1536 Loss 0.2197 Accuracy 0.3964\n",
      "Epoch 96 Batch 1600 Loss 0.2205 Accuracy 0.3959\n",
      "Epoch 96 Batch 1664 Loss 0.2199 Accuracy 0.3963\n",
      "Epoch 96 Batch 1728 Loss 0.2212 Accuracy 0.3966\n",
      "Epoch 96 Batch 1792 Loss 0.2213 Accuracy 0.3962\n",
      "Epoch 96 Batch 1856 Loss 0.2226 Accuracy 0.3963\n",
      "Epoch 96 Batch 1920 Loss 0.2229 Accuracy 0.3961\n",
      "Epoch 96 Batch 1984 Loss 0.2243 Accuracy 0.3961\n",
      "Epoch 96 Batch 2048 Loss 0.2251 Accuracy 0.3959\n",
      "Epoch 96 Batch 2112 Loss 0.2246 Accuracy 0.3956\n",
      "Epoch 96 Batch 2176 Loss 0.2250 Accuracy 0.3954\n",
      "Epoch 96 Batch 2240 Loss 0.2251 Accuracy 0.3950\n",
      "Epoch 96 Batch 2304 Loss 0.2250 Accuracy 0.3947\n",
      "Epoch 96 Batch 2368 Loss 0.2250 Accuracy 0.3950\n",
      "Epoch 96 Batch 2432 Loss 0.2266 Accuracy 0.3948\n",
      "Epoch 96 Batch 2496 Loss 0.2273 Accuracy 0.3947\n",
      "Epoch 96 Batch 2560 Loss 0.2279 Accuracy 0.3954\n",
      "Epoch 96 Batch 2624 Loss 0.2282 Accuracy 0.3950\n",
      "Epoch 96 Batch 2688 Loss 0.2279 Accuracy 0.3951\n",
      "Epoch 96 Batch 2752 Loss 0.2291 Accuracy 0.3950\n",
      "Epoch 96 Batch 2816 Loss 0.2294 Accuracy 0.3952\n",
      "Epoch 96 Batch 2880 Loss 0.2289 Accuracy 0.3951\n",
      "Epoch 96 Batch 2944 Loss 0.2291 Accuracy 0.3955\n",
      "Epoch 96 Batch 3008 Loss 0.2294 Accuracy 0.3954\n",
      "Epoch 96 Batch 3072 Loss 0.2303 Accuracy 0.3956\n",
      "Epoch 96 Batch 3136 Loss 0.2306 Accuracy 0.3958\n",
      "Epoch 96 Loss 0.2306 Accuracy 0.3958\n",
      "Time taken for 1 epoch: 36.7550208568573 secs\n",
      "\n",
      "Epoch 97 Batch 64 Loss 0.2760 Accuracy 0.4023\n",
      "Epoch 97 Batch 128 Loss 0.2664 Accuracy 0.3960\n",
      "Epoch 97 Batch 192 Loss 0.2604 Accuracy 0.3955\n",
      "Epoch 97 Batch 256 Loss 0.2576 Accuracy 0.3971\n",
      "Epoch 97 Batch 320 Loss 0.2567 Accuracy 0.3992\n",
      "Epoch 97 Batch 384 Loss 0.2497 Accuracy 0.3990\n",
      "Epoch 97 Batch 448 Loss 0.2481 Accuracy 0.4007\n",
      "Epoch 97 Batch 512 Loss 0.2411 Accuracy 0.4016\n",
      "Epoch 97 Batch 576 Loss 0.2353 Accuracy 0.4012\n",
      "Epoch 97 Batch 640 Loss 0.2338 Accuracy 0.4002\n",
      "Epoch 97 Batch 704 Loss 0.2336 Accuracy 0.4004\n",
      "Epoch 97 Batch 768 Loss 0.2340 Accuracy 0.3994\n",
      "Epoch 97 Batch 832 Loss 0.2331 Accuracy 0.3977\n",
      "Epoch 97 Batch 896 Loss 0.2324 Accuracy 0.3986\n",
      "Epoch 97 Batch 960 Loss 0.2336 Accuracy 0.3972\n",
      "Epoch 97 Batch 1024 Loss 0.2319 Accuracy 0.3969\n",
      "Epoch 97 Batch 1088 Loss 0.2335 Accuracy 0.3975\n",
      "Epoch 97 Batch 1152 Loss 0.2326 Accuracy 0.3971\n",
      "Epoch 97 Batch 1216 Loss 0.2314 Accuracy 0.3969\n",
      "Epoch 97 Batch 1280 Loss 0.2316 Accuracy 0.3966\n",
      "Epoch 97 Batch 1344 Loss 0.2298 Accuracy 0.3967\n",
      "Epoch 97 Batch 1408 Loss 0.2292 Accuracy 0.3964\n",
      "Epoch 97 Batch 1472 Loss 0.2279 Accuracy 0.3964\n",
      "Epoch 97 Batch 1536 Loss 0.2264 Accuracy 0.3959\n",
      "Epoch 97 Batch 1600 Loss 0.2263 Accuracy 0.3955\n",
      "Epoch 97 Batch 1664 Loss 0.2263 Accuracy 0.3958\n",
      "Epoch 97 Batch 1728 Loss 0.2264 Accuracy 0.3954\n",
      "Epoch 97 Batch 1792 Loss 0.2260 Accuracy 0.3948\n",
      "Epoch 97 Batch 1856 Loss 0.2262 Accuracy 0.3949\n",
      "Epoch 97 Batch 1920 Loss 0.2256 Accuracy 0.3950\n",
      "Epoch 97 Batch 1984 Loss 0.2259 Accuracy 0.3954\n",
      "Epoch 97 Batch 2048 Loss 0.2268 Accuracy 0.3954\n",
      "Epoch 97 Batch 2112 Loss 0.2274 Accuracy 0.3957\n",
      "Epoch 97 Batch 2176 Loss 0.2277 Accuracy 0.3957\n",
      "Epoch 97 Batch 2240 Loss 0.2271 Accuracy 0.3956\n",
      "Epoch 97 Batch 2304 Loss 0.2282 Accuracy 0.3954\n",
      "Epoch 97 Batch 2368 Loss 0.2290 Accuracy 0.3957\n",
      "Epoch 97 Batch 2432 Loss 0.2298 Accuracy 0.3960\n",
      "Epoch 97 Batch 2496 Loss 0.2296 Accuracy 0.3956\n",
      "Epoch 97 Batch 2560 Loss 0.2301 Accuracy 0.3954\n",
      "Epoch 97 Batch 2624 Loss 0.2306 Accuracy 0.3954\n",
      "Epoch 97 Batch 2688 Loss 0.2302 Accuracy 0.3954\n",
      "Epoch 97 Batch 2752 Loss 0.2313 Accuracy 0.3953\n",
      "Epoch 97 Batch 2816 Loss 0.2308 Accuracy 0.3955\n",
      "Epoch 97 Batch 2880 Loss 0.2304 Accuracy 0.3958\n",
      "Epoch 97 Batch 2944 Loss 0.2300 Accuracy 0.3958\n",
      "Epoch 97 Batch 3008 Loss 0.2299 Accuracy 0.3959\n",
      "Epoch 97 Batch 3072 Loss 0.2299 Accuracy 0.3956\n",
      "Epoch 97 Batch 3136 Loss 0.2302 Accuracy 0.3955\n",
      "Epoch 97 Loss 0.2302 Accuracy 0.3955\n",
      "Time taken for 1 epoch: 36.318448066711426 secs\n",
      "\n",
      "Epoch 98 Batch 64 Loss 0.2407 Accuracy 0.3891\n",
      "Epoch 98 Batch 128 Loss 0.2323 Accuracy 0.4015\n",
      "Epoch 98 Batch 192 Loss 0.2357 Accuracy 0.3966\n",
      "Epoch 98 Batch 256 Loss 0.2377 Accuracy 0.3951\n",
      "Epoch 98 Batch 320 Loss 0.2275 Accuracy 0.3973\n",
      "Epoch 98 Batch 384 Loss 0.2247 Accuracy 0.3963\n",
      "Epoch 98 Batch 448 Loss 0.2296 Accuracy 0.3967\n",
      "Epoch 98 Batch 512 Loss 0.2307 Accuracy 0.3973\n",
      "Epoch 98 Batch 576 Loss 0.2298 Accuracy 0.3979\n",
      "Epoch 98 Batch 640 Loss 0.2280 Accuracy 0.3981\n",
      "Epoch 98 Batch 704 Loss 0.2267 Accuracy 0.3969\n",
      "Epoch 98 Batch 768 Loss 0.2265 Accuracy 0.3973\n",
      "Epoch 98 Batch 832 Loss 0.2255 Accuracy 0.3978\n",
      "Epoch 98 Batch 896 Loss 0.2269 Accuracy 0.3982\n",
      "Epoch 98 Batch 960 Loss 0.2250 Accuracy 0.3977\n",
      "Epoch 98 Batch 1024 Loss 0.2240 Accuracy 0.3964\n",
      "Epoch 98 Batch 1088 Loss 0.2237 Accuracy 0.3960\n",
      "Epoch 98 Batch 1152 Loss 0.2236 Accuracy 0.3962\n",
      "Epoch 98 Batch 1216 Loss 0.2217 Accuracy 0.3965\n",
      "Epoch 98 Batch 1280 Loss 0.2229 Accuracy 0.3973\n",
      "Epoch 98 Batch 1344 Loss 0.2237 Accuracy 0.3975\n",
      "Epoch 98 Batch 1408 Loss 0.2227 Accuracy 0.3974\n",
      "Epoch 98 Batch 1472 Loss 0.2235 Accuracy 0.3969\n",
      "Epoch 98 Batch 1536 Loss 0.2231 Accuracy 0.3967\n",
      "Epoch 98 Batch 1600 Loss 0.2228 Accuracy 0.3968\n",
      "Epoch 98 Batch 1664 Loss 0.2235 Accuracy 0.3970\n",
      "Epoch 98 Batch 1728 Loss 0.2230 Accuracy 0.3977\n",
      "Epoch 98 Batch 1792 Loss 0.2232 Accuracy 0.3977\n",
      "Epoch 98 Batch 1856 Loss 0.2231 Accuracy 0.3977\n",
      "Epoch 98 Batch 1920 Loss 0.2219 Accuracy 0.3976\n",
      "Epoch 98 Batch 1984 Loss 0.2223 Accuracy 0.3979\n",
      "Epoch 98 Batch 2048 Loss 0.2226 Accuracy 0.3983\n",
      "Epoch 98 Batch 2112 Loss 0.2218 Accuracy 0.3983\n",
      "Epoch 98 Batch 2176 Loss 0.2216 Accuracy 0.3980\n",
      "Epoch 98 Batch 2240 Loss 0.2214 Accuracy 0.3977\n",
      "Epoch 98 Batch 2304 Loss 0.2231 Accuracy 0.3976\n",
      "Epoch 98 Batch 2368 Loss 0.2229 Accuracy 0.3980\n",
      "Epoch 98 Batch 2432 Loss 0.2227 Accuracy 0.3979\n",
      "Epoch 98 Batch 2496 Loss 0.2224 Accuracy 0.3979\n",
      "Epoch 98 Batch 2560 Loss 0.2227 Accuracy 0.3977\n",
      "Epoch 98 Batch 2624 Loss 0.2228 Accuracy 0.3977\n",
      "Epoch 98 Batch 2688 Loss 0.2232 Accuracy 0.3977\n",
      "Epoch 98 Batch 2752 Loss 0.2226 Accuracy 0.3978\n",
      "Epoch 98 Batch 2816 Loss 0.2236 Accuracy 0.3973\n",
      "Epoch 98 Batch 2880 Loss 0.2237 Accuracy 0.3973\n",
      "Epoch 98 Batch 2944 Loss 0.2236 Accuracy 0.3972\n",
      "Epoch 98 Batch 3008 Loss 0.2234 Accuracy 0.3969\n",
      "Epoch 98 Batch 3072 Loss 0.2225 Accuracy 0.3971\n",
      "Epoch 98 Batch 3136 Loss 0.2225 Accuracy 0.3972\n",
      "Epoch 98 Loss 0.2225 Accuracy 0.3972\n",
      "Time taken for 1 epoch: 36.026466846466064 secs\n",
      "\n",
      "Epoch 99 Batch 64 Loss 0.1839 Accuracy 0.3901\n",
      "Epoch 99 Batch 128 Loss 0.1821 Accuracy 0.3933\n",
      "Epoch 99 Batch 192 Loss 0.1920 Accuracy 0.3956\n",
      "Epoch 99 Batch 256 Loss 0.2007 Accuracy 0.3918\n",
      "Epoch 99 Batch 320 Loss 0.2037 Accuracy 0.3895\n",
      "Epoch 99 Batch 384 Loss 0.2054 Accuracy 0.3913\n",
      "Epoch 99 Batch 448 Loss 0.2086 Accuracy 0.3918\n",
      "Epoch 99 Batch 512 Loss 0.2079 Accuracy 0.3918\n",
      "Epoch 99 Batch 576 Loss 0.2072 Accuracy 0.3907\n",
      "Epoch 99 Batch 640 Loss 0.2059 Accuracy 0.3915\n",
      "Epoch 99 Batch 704 Loss 0.2059 Accuracy 0.3927\n",
      "Epoch 99 Batch 768 Loss 0.2027 Accuracy 0.3933\n",
      "Epoch 99 Batch 832 Loss 0.2018 Accuracy 0.3933\n",
      "Epoch 99 Batch 896 Loss 0.2019 Accuracy 0.3944\n",
      "Epoch 99 Batch 960 Loss 0.2035 Accuracy 0.3945\n",
      "Epoch 99 Batch 1024 Loss 0.2041 Accuracy 0.3943\n",
      "Epoch 99 Batch 1088 Loss 0.2056 Accuracy 0.3946\n",
      "Epoch 99 Batch 1152 Loss 0.2065 Accuracy 0.3954\n",
      "Epoch 99 Batch 1216 Loss 0.2095 Accuracy 0.3949\n",
      "Epoch 99 Batch 1280 Loss 0.2087 Accuracy 0.3952\n",
      "Epoch 99 Batch 1344 Loss 0.2087 Accuracy 0.3957\n",
      "Epoch 99 Batch 1408 Loss 0.2064 Accuracy 0.3961\n",
      "Epoch 99 Batch 1472 Loss 0.2062 Accuracy 0.3967\n",
      "Epoch 99 Batch 1536 Loss 0.2075 Accuracy 0.3965\n",
      "Epoch 99 Batch 1600 Loss 0.2062 Accuracy 0.3965\n",
      "Epoch 99 Batch 1664 Loss 0.2071 Accuracy 0.3961\n",
      "Epoch 99 Batch 1728 Loss 0.2069 Accuracy 0.3968\n",
      "Epoch 99 Batch 1792 Loss 0.2064 Accuracy 0.3967\n",
      "Epoch 99 Batch 1856 Loss 0.2068 Accuracy 0.3968\n",
      "Epoch 99 Batch 1920 Loss 0.2091 Accuracy 0.3966\n",
      "Epoch 99 Batch 1984 Loss 0.2090 Accuracy 0.3970\n",
      "Epoch 99 Batch 2048 Loss 0.2086 Accuracy 0.3971\n",
      "Epoch 99 Batch 2112 Loss 0.2085 Accuracy 0.3976\n",
      "Epoch 99 Batch 2176 Loss 0.2084 Accuracy 0.3979\n",
      "Epoch 99 Batch 2240 Loss 0.2086 Accuracy 0.3976\n",
      "Epoch 99 Batch 2304 Loss 0.2088 Accuracy 0.3973\n",
      "Epoch 99 Batch 2368 Loss 0.2094 Accuracy 0.3977\n",
      "Epoch 99 Batch 2432 Loss 0.2098 Accuracy 0.3977\n",
      "Epoch 99 Batch 2496 Loss 0.2104 Accuracy 0.3979\n",
      "Epoch 99 Batch 2560 Loss 0.2104 Accuracy 0.3981\n",
      "Epoch 99 Batch 2624 Loss 0.2104 Accuracy 0.3983\n",
      "Epoch 99 Batch 2688 Loss 0.2105 Accuracy 0.3985\n",
      "Epoch 99 Batch 2752 Loss 0.2109 Accuracy 0.3985\n",
      "Epoch 99 Batch 2816 Loss 0.2119 Accuracy 0.3985\n",
      "Epoch 99 Batch 2880 Loss 0.2119 Accuracy 0.3983\n",
      "Epoch 99 Batch 2944 Loss 0.2113 Accuracy 0.3985\n",
      "Epoch 99 Batch 3008 Loss 0.2118 Accuracy 0.3985\n",
      "Epoch 99 Batch 3072 Loss 0.2119 Accuracy 0.3985\n",
      "Epoch 99 Batch 3136 Loss 0.2117 Accuracy 0.3986\n",
      "Epoch 99 Loss 0.2117 Accuracy 0.3986\n",
      "Time taken for 1 epoch: 37.36767268180847 secs\n",
      "\n",
      "Epoch 100 Batch 64 Loss 0.1962 Accuracy 0.4019\n",
      "Epoch 100 Batch 128 Loss 0.1987 Accuracy 0.4029\n",
      "Epoch 100 Batch 192 Loss 0.2073 Accuracy 0.4041\n",
      "Epoch 100 Batch 256 Loss 0.2148 Accuracy 0.4003\n",
      "Epoch 100 Batch 320 Loss 0.2081 Accuracy 0.3995\n",
      "Epoch 100 Batch 384 Loss 0.2095 Accuracy 0.3999\n",
      "Epoch 100 Batch 448 Loss 0.2116 Accuracy 0.3979\n",
      "Epoch 100 Batch 512 Loss 0.2116 Accuracy 0.3996\n",
      "Epoch 100 Batch 576 Loss 0.2114 Accuracy 0.3998\n",
      "Epoch 100 Batch 640 Loss 0.2155 Accuracy 0.4006\n",
      "Epoch 100 Batch 704 Loss 0.2142 Accuracy 0.4007\n",
      "Epoch 100 Batch 768 Loss 0.2121 Accuracy 0.4001\n",
      "Epoch 100 Batch 832 Loss 0.2121 Accuracy 0.4006\n",
      "Epoch 100 Batch 896 Loss 0.2122 Accuracy 0.4006\n",
      "Epoch 100 Batch 960 Loss 0.2117 Accuracy 0.3999\n",
      "Epoch 100 Batch 1024 Loss 0.2109 Accuracy 0.4013\n",
      "Epoch 100 Batch 1088 Loss 0.2152 Accuracy 0.4014\n",
      "Epoch 100 Batch 1152 Loss 0.2135 Accuracy 0.4016\n",
      "Epoch 100 Batch 1216 Loss 0.2139 Accuracy 0.4006\n",
      "Epoch 100 Batch 1280 Loss 0.2150 Accuracy 0.4010\n",
      "Epoch 100 Batch 1344 Loss 0.2148 Accuracy 0.4008\n",
      "Epoch 100 Batch 1408 Loss 0.2144 Accuracy 0.4008\n",
      "Epoch 100 Batch 1472 Loss 0.2146 Accuracy 0.4009\n",
      "Epoch 100 Batch 1536 Loss 0.2144 Accuracy 0.4007\n",
      "Epoch 100 Batch 1600 Loss 0.2148 Accuracy 0.4011\n",
      "Epoch 100 Batch 1664 Loss 0.2145 Accuracy 0.4007\n",
      "Epoch 100 Batch 1728 Loss 0.2133 Accuracy 0.4007\n",
      "Epoch 100 Batch 1792 Loss 0.2140 Accuracy 0.4007\n",
      "Epoch 100 Batch 1856 Loss 0.2150 Accuracy 0.4001\n",
      "Epoch 100 Batch 1920 Loss 0.2145 Accuracy 0.4000\n",
      "Epoch 100 Batch 1984 Loss 0.2135 Accuracy 0.4000\n",
      "Epoch 100 Batch 2048 Loss 0.2139 Accuracy 0.3994\n",
      "Epoch 100 Batch 2112 Loss 0.2131 Accuracy 0.3998\n",
      "Epoch 100 Batch 2176 Loss 0.2138 Accuracy 0.3995\n",
      "Epoch 100 Batch 2240 Loss 0.2138 Accuracy 0.3989\n",
      "Epoch 100 Batch 2304 Loss 0.2134 Accuracy 0.3988\n",
      "Epoch 100 Batch 2368 Loss 0.2128 Accuracy 0.3985\n",
      "Epoch 100 Batch 2432 Loss 0.2119 Accuracy 0.3980\n",
      "Epoch 100 Batch 2496 Loss 0.2130 Accuracy 0.3981\n",
      "Epoch 100 Batch 2560 Loss 0.2135 Accuracy 0.3981\n",
      "Epoch 100 Batch 2624 Loss 0.2134 Accuracy 0.3984\n",
      "Epoch 100 Batch 2688 Loss 0.2126 Accuracy 0.3981\n",
      "Epoch 100 Batch 2752 Loss 0.2119 Accuracy 0.3982\n",
      "Epoch 100 Batch 2816 Loss 0.2115 Accuracy 0.3983\n",
      "Epoch 100 Batch 2880 Loss 0.2119 Accuracy 0.3983\n",
      "Epoch 100 Batch 2944 Loss 0.2117 Accuracy 0.3985\n",
      "Epoch 100 Batch 3008 Loss 0.2122 Accuracy 0.3987\n",
      "Epoch 100 Batch 3072 Loss 0.2119 Accuracy 0.3986\n",
      "Epoch 100 Batch 3136 Loss 0.2119 Accuracy 0.3985\n",
      "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
      "Epoch 100 Loss 0.2119 Accuracy 0.3985\n",
      "Time taken for 1 epoch: 36.59155893325806 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  for batch in range(batch_size, padded_smiles.shape[0], batch_size):\n",
    "    inp = padded_smiles[batch - batch_size : batch]\n",
    "    train_step(inp, inp)\n",
    "    \n",
    "    if batch % batch_size == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "  np.random.shuffle(padded_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd812d-f1c2-4786-befc-a9b5410d9b66",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449aeeeb-4b18-4bbd-a577-5970a0c688f6",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a224dd0a-658e-4d9c-9288-ce4ea6229aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 34, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 34, 17, 17, 17, 17, 17, 17, 17, 17, 17, 34, 34, 17, 34, 34, 34, 34, 17, 34, 34, 34, 17, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 28, 28, 28, 28, 34, 28, 28, 28, 28, 28, 34, 34, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 13, 13, 28, 28, 28, 28, 28, 13, 28, 28, 28, 28, 28, 28, 28, 28]\n",
      "(((((((((((((((((((((((((((((((((((S()(((((((((((S(((((((((SS(SSSS(SSS(SSSSSSSSSSSSSClClClClSClClClClClSSClClClClClClClClClClCl[SEP][SEP]ClClClClCl[SEP]ClClClClClClClCl\n"
     ]
    }
   ],
   "source": [
    "def generate_smiles(transformer):\n",
    "    start_token = 12\n",
    "    end_token = 13\n",
    "\n",
    "    input_sequence = np.random.randint(0, 60, size=(1, 123))\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_sequence, input_sequence)\n",
    "    enc_output = transformer.encoder(input_sequence, False, enc_padding_mask)\n",
    "\n",
    "    generated_smiles = []\n",
    "    for _ in range(123):\n",
    "        input_sequence = np.random.randint(0, 510, size=(1, 123))\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_sequence, input_sequence)\n",
    "        enc_output = transformer.encoder(input_sequence, False, enc_padding_mask)\n",
    "    \n",
    "        decoder_output, _ = transformer.decoder(input_sequence, enc_output, False, combined_mask, dec_padding_mask)\n",
    "        final_output = transformer.final_layer(decoder_output)\n",
    "        \n",
    "        predicted_token = np.argmax(final_output[:, -1, :], axis=-1)\n",
    "        # print(predicted_token)\n",
    "        generated_smiles.append(predicted_token[0])\n",
    "\n",
    "        if predicted_token[0] == end_token:\n",
    "            break\n",
    "\n",
    "        input_sequence = generated_smiles\n",
    "        input_sequence = pad_sequences([input_sequence], maxlen=123, padding='post')\n",
    "        input_sequence = tf.cast(input_sequence, dtype=\"float32\")       \n",
    "\n",
    "    return generated_smiles\n",
    "\n",
    "input_sequence = np.random.randint(0, 510, size=(1, 123))\n",
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_sequence, input_sequence)\n",
    "enc_output = transformer.encoder(input_sequence, False, enc_padding_mask)\n",
    "decoder_output, _ = transformer.decoder(input_sequence, enc_output, False, combined_mask, dec_padding_mask)\n",
    "final_output = transformer.final_layer(decoder_output)\n",
    "\n",
    "smiles = []\n",
    "for row in final_output[0]:\n",
    "    smiles.append(np.argmax(row))\n",
    "\n",
    "print(smiles)\n",
    "print( tokenizer.decode(smiles).replace(' ', '') )\n",
    "\n",
    "# for _ in range(50):\n",
    "#     print( tokenizer.decode(generate_smiles(transformer)).replace(' ', '') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4457840-a546-40c9-9bf7-8f80b40c05cb",
   "metadata": {},
   "source": [
    "#### Using Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39262322-20ee-4643-9b62-01f7648f9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_smiles = \"COc1cccc2c3c(=O)n([C@@H]4C(C)(C)[C@@H]5CC[C@@]4(C)C5)ccc3n(CCN3CCOCC3)c12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4201e6b-209a-4046-8290-9fe70be25c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 123), dtype=int32, numpy=\n",
       "array([[12, 16, 19, 15, 20, 15, 15, 15, 15, 21, 15, 26, 15, 17, 22, 19,\n",
       "        18, 25, 17, 35, 32, 16, 17, 16, 18, 17, 16, 18, 35, 43, 16, 16,\n",
       "        56, 32, 17, 16, 18, 16, 43, 18, 15, 15, 15, 26, 25, 17, 16, 16,\n",
       "        23, 26, 16, 16, 19, 16, 16, 26, 18, 15, 20, 21, 13,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import SmilesTokenizer\n",
    "\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "tokenized_test = tokenizer.encode(test_smiles)\n",
    "tokenized_test_padded = pad_sequences([tokenized_test], maxlen=123, padding='post')\n",
    "tokenized_test_tensor = tf.constant(tokenized_test_padded)\n",
    "\n",
    "tokenized_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96390b96-ba4f-4790-8119-ecdeb2f0d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(tokenized_test_tensor, tokenized_test_tensor)\n",
    "\n",
    "predictions, _ = transformer(tokenized_test_tensor, tokenized_test_tensor, \n",
    "                                False, \n",
    "                                enc_padding_mask, \n",
    "                                combined_mask, \n",
    "                                dec_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "171d602d-8dac-4858-95a5-a33deed9a8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('COc1ccc22c3c(=O)n([C@@H]4C(C)(C)[C@@H]4CC[C@@]4(C)C5)ccccn(CCN3CCOC33)c12',\n",
       " 'COc1cccc2c3c(=O)n([C@@H]4C(C)(C)[C@@H]5CC[C@@]4(C)C5)ccc3n(CCN3CCOCC3)c12')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for token in range(123):\n",
    "    preds.append( int( tf.argmax( predictions[:, token, :], axis=-1 )[0] ) )\n",
    "pred_smiles = tokenizer.decode(preds).replace(' ', '')\n",
    "\n",
    "pred_smiles[:pred_smiles.find('[SEP]')], test_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6b01b-1bef-439f-b771-9eeafde5a904",
   "metadata": {},
   "source": [
    "#### Encoder Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "820a95a7-7a92-4c68-af58-0c219a4fc7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('COc1cccc2c3c(=O)n([C@@H]4C(C)(C)[C@@H]5CC[C@@]4(C)C5)ccc3n(CCN3CCOCC3)c12',\n",
       " <tf.Tensor: shape=(1, 123), dtype=int32, numpy=\n",
       " array([[12, 16, 19, 15, 20, 15, 15, 15, 15, 21, 15, 26, 15, 17, 22, 19,\n",
       "         18, 25, 17, 35, 32, 16, 17, 16, 18, 17, 16, 18, 35, 43, 16, 16,\n",
       "         56, 32, 17, 16, 18, 16, 43, 18, 15, 15, 15, 26, 25, 17, 16, 16,\n",
       "         23, 26, 16, 16, 19, 16, 16, 26, 18, 15, 20, 21, 13,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import SmilesTokenizer\n",
    "\n",
    "test_smiles = \"COc1cccc2c3c(=O)n([C@@H]4C(C)(C)[C@@H]5CC[C@@]4(C)C5)ccc3n(CCN3CCOCC3)c12\"\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "tokenized_test = tokenizer.encode(test_smiles)\n",
    "tokenized_test_padded = pad_sequences([tokenized_test], maxlen=123, padding='post')\n",
    "tokenized_test_tensor = tf.constant(tokenized_test_padded)\n",
    "\n",
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(tokenized_test_tensor, tokenized_test_tensor)\n",
    "\n",
    "test_smiles, tokenized_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "657df5cf-e0c8-40ea-84af-c954cb0285de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 123, 32), dtype=float32, numpy=\n",
       " array([[[-7.5602907e-01,  9.8976588e-01,  3.8249230e-01, ...,\n",
       "           8.8623172e-01, -7.7111667e-01,  4.2068535e-01],\n",
       "         [-4.5703232e-01, -4.1441080e-01, -8.0486238e-02, ...,\n",
       "           3.2696372e-01,  1.2151197e+00, -5.1542324e-01],\n",
       "         [-2.9639983e-01, -4.8903784e-01,  1.1130489e+00, ...,\n",
       "           8.6220801e-01,  2.9730543e-01, -1.8603158e-01],\n",
       "         ...,\n",
       "         [ 3.2808669e-03, -6.5462244e-01, -1.0051614e-01, ...,\n",
       "           2.8973863e-01, -2.6029491e-01,  4.2959553e-01],\n",
       "         [-7.1666911e-03, -7.1080154e-01, -5.6212269e-02, ...,\n",
       "           2.3785900e-01, -2.8803337e-01,  4.1768509e-01],\n",
       "         [-5.7289515e-02, -7.3378289e-01,  1.8717349e-04, ...,\n",
       "           2.0797640e-01, -3.1194839e-01,  3.9206657e-01]]], dtype=float32)>,\n",
       " TensorShape([1, 123, 32]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output = transformer.encoder(tokenized_test_tensor, False, enc_padding_mask)\n",
    "\n",
    "enc_output, enc_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1e4e9-a07b-413d-bd08-92b7179627ea",
   "metadata": {},
   "source": [
    "#### Decoder Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0bd7f48c-c00a-426d-853b-fd747f720df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 123, 32])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import SmilesTokenizer\n",
    "\n",
    "test_smiles = \"COc1cccc2c3c(=O)n([C@@H]4C(C)(C)[C@@H]5CC[C@@]4(C)C5)ccc3n(CCN3CCOCC3)c12\"\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "tokenized_test = tokenizer.encode(test_smiles)\n",
    "tokenized_test_padded = pad_sequences([tokenized_test], maxlen=123, padding='post')\n",
    "tokenized_test_tensor = tf.constant(tokenized_test_padded)\n",
    "\n",
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(tokenized_test_tensor, tokenized_test_tensor)\n",
    "enc_output = transformer.encoder(tokenized_test_tensor, False, enc_padding_mask)\n",
    "\n",
    "enc_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0174cf7-54bd-4a68-bf62-97f9aaaa12f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 123) (1, 123, 32) (1, 1, 123, 123) (1, 1, 1, 123)\n",
      "(1, 123)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 123, 510), dtype=float32, numpy=\n",
       "array([[[-17.852684 , -18.149971 , -18.008026 , ..., -17.878656 ,\n",
       "         -17.98216  , -17.889906 ],\n",
       "        [-19.29191  , -19.641819 , -19.401522 , ..., -19.469503 ,\n",
       "         -19.326366 , -19.449482 ],\n",
       "        [-20.745901 , -20.88299  , -20.733458 , ..., -20.643974 ,\n",
       "         -20.70992  , -20.718563 ],\n",
       "        ...,\n",
       "        [-14.977754 , -15.05572  , -14.617012 , ..., -14.735315 ,\n",
       "         -14.626514 , -14.753969 ],\n",
       "        [-15.126015 , -15.224129 , -14.801893 , ..., -14.895902 ,\n",
       "         -14.794842 , -14.946787 ],\n",
       "        [-15.134567 , -15.252741 , -14.823038 , ..., -14.926474 ,\n",
       "         -14.8180065, -14.976839 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenized_test_tensor.shape, enc_output.shape, combined_mask.shape, dec_padding_mask.shape)\n",
    "\n",
    "dec_output, attention_weights = transformer.decoder(tokenized_test_tensor, enc_output, False, combined_mask, dec_padding_mask)\n",
    "\n",
    "print(tokenized_test_tensor.shape)\n",
    "\n",
    "final_output = transformer.final_layer(dec_output)\n",
    "\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a28f9419-dff2-4891-b942-046d824019b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('COc1ccc22c3c(=O)n([C@@H]4C(C)(C)[C@@H]4CC[C@@]4(C)C5)ccccn(CCN3CCOC33)c12',\n",
       " 'COc1cccc2c3c(=O)n([C@@H]4C(C)(C)[C@@H]5CC[C@@]4(C)C5)ccc3n(CCN3CCOCC3)c12')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for token in range(123):\n",
    "    preds.append( int( tf.argmax( final_output[:, token, :], axis=-1 )[0] ) )\n",
    "pred_smiles = tokenizer.decode(preds).replace(' ', '')\n",
    "\n",
    "pred_smiles[:pred_smiles.find('[SEP]')], test_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97790e06-772b-4987-b521-f611fface3c6",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e50171f5-e667-4d6b-9909-89ff4858249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['CCCCC/C=C\\\\C/C=C\\\\C/C=C\\\\C/C=C\\\\CCCC(=O)NCCc1ccoc1',\n",
       "  'CCCCC/C=C\\\\C/C=C\\\\C/C=C\\\\C/C=C\\\\CCCC(=O)NCc1ccoc1',\n",
       "  'CCCCC/C=C\\\\C/C=C\\\\C/C=C\\\\C/C=C\\\\CCCC(=O)NCc1cccn1C',\n",
       "  'Cc1c(C(=O)c2cccc3ccccc23)c2cccc3c2n1[C@H](CN1CCOCC1)CO3',\n",
       "  'COc1ccccc1CNC(=O)c1nn(CCN2CCOCC2)c2c(OC)cccc12'],\n",
       " 2723)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb2_smiles = open('./data/X_SMILES.txt', 'r')\n",
    "cb2_smiles = cb2_smiles.read().splitlines()\n",
    "\n",
    "cb2_smiles[:5], len(cb2_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "00d0f6bd-ef7c-47eb-9ee6-d54fa23f98e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['NC(=O)[C@H]1CS[C@@H]2CC[C@]3(CCCN3C(=O)[C@@H]3CCCN3)C(=O)N12',\n",
       "  'CC1Cc2cccc3c2N1C(=O)C(N1CCN(Cc2ccc(Cl)cc2)CC1)CC3',\n",
       "  'CC1(C)Cc2cccc3c2N1C(=O)C(N1CCN(Cc2ccc(Cl)cc2)CC1)CC3',\n",
       "  'Nc1cccc(-c2ccc(CCN3CCN(c4cccc5cccnc45)CC3)cc2)n1',\n",
       "  'Cc1ccc(CN2CCN(C3CCc4cccc5c4N(CC5)C3=O)CC2)cc1'],\n",
       " 7252)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2_smiles = open('./data/D2_SMILES.txt', 'r')\n",
    "d2_smiles = d2_smiles.read().splitlines()\n",
    "\n",
    "d2_smiles[:5], len(d2_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "255d2f92-7202-4466-83ce-70ac3e28c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 SMILES processed\n",
      "200 SMILES processed\n",
      "300 SMILES processed\n",
      "400 SMILES processed\n",
      "500 SMILES processed\n",
      "600 SMILES processed\n",
      "700 SMILES processed\n",
      "800 SMILES processed\n",
      "900 SMILES processed\n",
      "1000 SMILES processed\n",
      "1100 SMILES processed\n",
      "1200 SMILES processed\n",
      "1300 SMILES processed\n",
      "1400 SMILES processed\n",
      "1500 SMILES processed\n",
      "1600 SMILES processed\n",
      "1700 SMILES processed\n",
      "1800 SMILES processed\n",
      "1900 SMILES processed\n",
      "2000 SMILES processed\n",
      "2100 SMILES processed\n",
      "2200 SMILES processed\n",
      "2300 SMILES processed\n",
      "2400 SMILES processed\n",
      "2500 SMILES processed\n",
      "2600 SMILES processed\n",
      "2700 SMILES processed\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import SmilesTokenizer\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "\n",
    "TRAINING = False\n",
    "LENGTH = 128\n",
    "DFF = 32\n",
    "preprocessed_cb2 = []\n",
    "\n",
    "curr = 1\n",
    "for smiles in cb2_smiles:\n",
    "    tokenized = tokenizer.encode(smiles)\n",
    "    padded = pad_sequences([tokenized], maxlen=LENGTH, padding='post')\n",
    "    smiles_tensor = tf.constant(padded)\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(smiles_tensor, smiles_tensor)\n",
    "    enc_output = transformer.encoder(smiles_tensor, TRAINING, enc_padding_mask)\n",
    "    enc_output = enc_output.numpy()\n",
    "    enc_output = enc_output.reshape((LENGTH, DFF))\n",
    "    \n",
    "    preprocessed_cb2.append(enc_output)\n",
    "\n",
    "    if curr % 100 == 0:\n",
    "        print(f\"{curr} SMILES processed\")\n",
    "\n",
    "    curr += 1\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f5aa51d-bccb-4ed7-a339-92079ea42a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2723, 128, 32), (128, 32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_cb2 = np.array(preprocessed_cb2)\n",
    "preprocessed_cb2.shape, preprocessed_cb2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a854e8a0-1bce-42ee-b8a1-0b62b96a8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/processed_cb2', preprocessed_cb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ef71caea-2b29-46c5-b7ee-fd36b8f2a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 SMILES processed\n",
      "200 SMILES processed\n",
      "300 SMILES processed\n",
      "400 SMILES processed\n",
      "500 SMILES processed\n",
      "600 SMILES processed\n",
      "700 SMILES processed\n",
      "800 SMILES processed\n",
      "900 SMILES processed\n",
      "1000 SMILES processed\n",
      "1100 SMILES processed\n",
      "1200 SMILES processed\n",
      "1300 SMILES processed\n",
      "1400 SMILES processed\n",
      "1500 SMILES processed\n",
      "1600 SMILES processed\n",
      "1700 SMILES processed\n",
      "1800 SMILES processed\n",
      "1900 SMILES processed\n",
      "2000 SMILES processed\n",
      "2100 SMILES processed\n",
      "2200 SMILES processed\n",
      "2300 SMILES processed\n",
      "2400 SMILES processed\n",
      "2500 SMILES processed\n",
      "2600 SMILES processed\n",
      "2700 SMILES processed\n",
      "2800 SMILES processed\n",
      "2900 SMILES processed\n",
      "3000 SMILES processed\n",
      "3100 SMILES processed\n",
      "3200 SMILES processed\n",
      "3300 SMILES processed\n",
      "3400 SMILES processed\n",
      "3500 SMILES processed\n",
      "3600 SMILES processed\n",
      "3700 SMILES processed\n",
      "3800 SMILES processed\n",
      "3900 SMILES processed\n",
      "4000 SMILES processed\n",
      "4100 SMILES processed\n",
      "4200 SMILES processed\n",
      "4300 SMILES processed\n",
      "4400 SMILES processed\n",
      "4500 SMILES processed\n",
      "4600 SMILES processed\n",
      "4700 SMILES processed\n",
      "4800 SMILES processed\n",
      "4900 SMILES processed\n",
      "5000 SMILES processed\n",
      "5100 SMILES processed\n",
      "5200 SMILES processed\n",
      "5300 SMILES processed\n",
      "5400 SMILES processed\n",
      "5500 SMILES processed\n",
      "5600 SMILES processed\n",
      "5700 SMILES processed\n",
      "5800 SMILES processed\n",
      "5900 SMILES processed\n",
      "6000 SMILES processed\n",
      "6100 SMILES processed\n",
      "6200 SMILES processed\n",
      "6300 SMILES processed\n",
      "6400 SMILES processed\n",
      "6500 SMILES processed\n",
      "6600 SMILES processed\n",
      "6700 SMILES processed\n",
      "6800 SMILES processed\n",
      "6900 SMILES processed\n",
      "7000 SMILES processed\n",
      "7100 SMILES processed\n",
      "7200 SMILES processed\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import SmilesTokenizer\n",
    "tokenizer = SmilesTokenizer(\"./data/vocab.txt\")\n",
    "\n",
    "TRAINING = False\n",
    "LENGTH = 128\n",
    "DFF = 32\n",
    "preprocessed_d2 = []\n",
    "\n",
    "curr = 1\n",
    "for smiles in d2_smiles:\n",
    "    tokenized = tokenizer.encode(smiles)\n",
    "    padded = pad_sequences([tokenized], maxlen=LENGTH, padding='post')\n",
    "    smiles_tensor = tf.constant(padded)\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(smiles_tensor, smiles_tensor)\n",
    "    enc_output = transformer.encoder(smiles_tensor, TRAINING, enc_padding_mask)\n",
    "    enc_output = enc_output.numpy()\n",
    "    enc_output = enc_output.reshape((LENGTH, DFF))\n",
    "    \n",
    "    preprocessed_d2.append(enc_output)\n",
    "\n",
    "    if curr % 100 == 0:\n",
    "        print(f\"{curr} SMILES processed\")\n",
    "\n",
    "    curr += 1\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "caad2ed0-1cfd-426a-856e-0265cf1d12b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7252, 128, 32), (128, 32))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_d2 = np.array(preprocessed_d2)\n",
    "preprocessed_d2.shape, preprocessed_d2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "27f9aeb9-7ce8-4880-bea9-c5a6be6874c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/processed_d2', preprocessed_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e4ae2-1103-4e5c-bdad-0c928c8628ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Previous Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb71f8-ddd3-4865-9bbd-221e9ebe5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "padded_smiles = pad_sequences(tokenized_smiles, padding='post')\n",
    "padded_smiles = tf.cast(padded_smiles, dtype=\"float32\")\n",
    "padded_smiles = np.stack(padded_smiles, axis=0)\n",
    "\n",
    "padded_smiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e995a-dabf-4be5-a4a6-0c7435172318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee46a0d-fedd-4fdb-9580-8848c743f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809598a-c669-45b7-b45f-817d255c1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc16fa-cfab-4841-9819-cc4ddd6c0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8882c-1e5d-4878-a974-e15e702bcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67b949-adee-4155-8086-94a00e3f08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb35c48-681d-47d4-a065-ddbe8105b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d15e9-18e4-4c27-8f7f-954623e5956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4a3ae-98d2-45b3-ad6d-8188f7d67bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ff635-f0fa-4a6f-a35d-7d219373e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb46492-1ad2-4b6f-a232-58ca712bcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 510\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74608193-ebeb-454b-8c54-03b6e80388c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.constant(d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = tf.constant(warmup_steps, dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)  # Convert step to float32\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8617361-20ec-4f9c-97cc-b4b118b47f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cff6ee-634a-464a-ab55-fa14e34086f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33462096-0cae-45b2-af26-e0ccd9701cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da7104-2a15-4e1b-b928-478f26db5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.MeanSquaredError(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd2d0f-67af-44ee-bc03-fc249c3550b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, input_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=input_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095af69a-97c0-40cb-8f34-30618bc04fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273b6ec-d6bf-4fcf-911b-6d7e2dd8bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a668b-4505-415e-8155-1abda37451e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "MAX_LEN = 123\n",
    "\n",
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, MAX_LEN), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, MAX_LEN), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        # Create padding mask (True for non-padded elements, False for padded elements)\n",
    "        padding_mask = tf.math.logical_not(tf.math.equal(tar_real, 0))\n",
    "        # Apply the padding mask to the ground truth and predictions\n",
    "        tar_real_masked = tf.boolean_mask(tar_real, padding_mask)\n",
    "        predictions_masked = tf.boolean_mask(predictions, padding_mask)\n",
    "        \n",
    "        # Calculate the mean squared error with masking\n",
    "        loss = tf.keras.losses.mean_squared_error(tar_real_masked, predictions_masked)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913f14d-9162-4434-b709-6f8f5d19771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  for i in range(0, len(padded_smiles), batch_size):\n",
    "    inp_batch = padded_smiles[i : i + batch_size]\n",
    "    train_step(inp_batch, inp_batch)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   start = time.time()\n",
    "  \n",
    "#   train_loss.reset_states()\n",
    "#   train_accuracy.reset_states()\n",
    "  \n",
    "#   # inp -> portuguese, tar -> english\n",
    "#   for (32, (inp)) in enumerate(padded_smiles):\n",
    "#     print(inp.shape)\n",
    "      \n",
    "#     train_step(tf.cast(inp, dtype=tf.int64), tf.cast(inp, dtype=tf.int64))\n",
    "    \n",
    "#     if batch % 50 == 0:\n",
    "#       print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "#           epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "#   if (epoch + 1) % 5 == 0:\n",
    "#     ckpt_save_path = ckpt_manager.save()\n",
    "#     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "#                                                          ckpt_save_path))\n",
    "    \n",
    "#   print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "#                                                 train_loss.result(), \n",
    "#                                                 train_accuracy.result()))\n",
    "\n",
    "#   print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbeb70-dbcf-49de-8c61-3ed77acebc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6c790-fd15-45b8-99e8-9be1fabd2b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bb86d-751c-4e8d-aee1-6b6b9e829eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829d48e-6ba2-4d51-900f-607b39903d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7366e0-302b-41c9-849d-e4fca6e9c92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61bdd5-290b-49a4-b1ae-25a93ff83946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c4cec-9169-4327-8bfe-563ad846ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = self.positional_encoding(max_len, d_model)\n",
    "\n",
    "    def positional_encoding(self, max_len, d_model):\n",
    "        pos = tf.range(max_len, dtype=tf.float32)\n",
    "        i = tf.range(d_model, dtype=tf.float32)\n",
    "        angle_rates = 1 / tf.pow(10000, (2 * i // 2) / d_model)\n",
    "        angles = pos[:, tf.newaxis] * angle_rates[tf.newaxis, :]\n",
    "        angle_rads = angles * tf.constant(np.pi / 180.0)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.encoding[:, :tf.shape(x)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8c03f-7037-4b99-8118-d1a68b506a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = self.point_wise_feed_forward_network(d_model, d_ff)\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def point_wise_feed_forward_network(self, d_model, d_ff):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layer_norm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layer_norm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c79123-a096-4ffd-9669-23d8bb2fb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, target_vocab_size, max_len, dropout_rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = self.point_wise_feed_forward_network(d_model, d_ff)\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = layers.Dropout(dropout_rate)\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def point_wise_feed_forward_network(self, d_model, d_ff):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, encoder_output, padding_mask, look_ahead_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout2(attn1)\n",
    "        out1 = self.layer_norm1(x + attn1)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(encoder_output, encoder_output, out1, padding_mask)\n",
    "        attn2 = self.dropout3(attn2)\n",
    "        out2 = self.layer_norm2(out1 + attn2)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out3 = self.layer_norm3(out2 + ffn_output)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1904d9e-a5e4-4186-ae53-cac5cc891f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, d_model, num_heads, d_ff, input_vocab_size, target_vocab_size, max_len, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(d_model, num_heads, d_ff, dropout_rate)\n",
    "        self.decoder = TransformerDecoder(d_model, num_heads, d_ff, target_vocab_size, max_len, dropout_rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inp, padding_mask, look_ahead_mask = inputs['inp_data'], inputs['padding_mask'], inputs['look_ahead_mask']\n",
    "        encoder_output = self.encoder(inp)\n",
    "        decoder_output, _, _ = self.decoder(inp, encoder_output, padding_mask, look_ahead_mask)\n",
    "        final_output = self.final_layer(decoder_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4064b3-1a83-4ae3-951c-e7789c757c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and masking functions\n",
    "def prepare_data(inputs, max_len):\n",
    "    inp_data = pad_sequences(inputs, padding='post')\n",
    "    inp_data = tf.cast(inp_data, dtype=\"float32\")\n",
    "    padding_mask = create_padding_mask(inp_data)\n",
    "    look_ahead_mask = create_look_ahead_masks(inp_data)\n",
    "    return inp_data, padding_mask, look_ahead_mask\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    mask = tf.math.equal(seq, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask[tf.newaxis, tf.newaxis, :size, :size]\n",
    "\n",
    "def create_look_ahead_masks(input_data):\n",
    "    max_len = tf.shape(input_data)[1]\n",
    "    mask = create_look_ahead_mask(max_len)\n",
    "    return tf.tile(mask, [tf.shape(input_data)[0], 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ae359-bde3-4fbb-8625-0805a5adb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = 510\n",
    "max_len = max(len(seq) for seq in tokenized_smiles)\n",
    "\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47e11c-c071-4a50-93d8-d2a3c1141526",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64  # You can adjust this based on your needs\n",
    "num_heads = 4  # You can adjust this based on your needs\n",
    "d_ff = 128  # You can adjust this based on your needs\n",
    "dropout_rate = 0.2  # You can adjust this based on your needs\n",
    "\n",
    "# Prepare data and train the model\n",
    "inp_data, padding_mask, look_ahead_mask = prepare_data(tokenized_smiles, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652c40e-446c-47a5-9258-826babe11be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Transformer model\n",
    "transformer = Transformer(d_model, num_heads, d_ff, input_vocab_size, input_vocab_size, max_len, dropout_rate)\n",
    "\n",
    "# Define loss function (unsupervised loss, e.g., mean squared error or binary cross-entropy)\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "input_data = {\n",
    "    \"inp_data\": inp_data,\n",
    "    \"padding_mask\": padding_mask,\n",
    "    \"look_ahead_mask\": look_ahead_mask\n",
    "}\n",
    "\n",
    "print(inp_data.shape, padding_mask.shape, look_ahead_mask.shape)\n",
    "\n",
    "# Train the model on the data (use the same inp_data as both input and target)\n",
    "transformer.fit({'inp_data': inp_data, 'padding_mask': padding_mask, 'look_ahead_mask': look_ahead_mask}, inp_data, batch_size=3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806ba96-9b2d-411e-9dcd-23d9a4f9d673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d49872-718f-4d7d-8145-4eaf62b6aead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf4d1c-a531-46bf-b4b7-02884a332ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c967937-96ae-4969-919f-181b24619a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81d0a1-a2ab-47af-87a6-23f6123d2247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d5ef9-6189-4125-8880-a861e5df675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = self.positional_encoding(max_len, d_model)\n",
    "\n",
    "    def positional_encoding(self, max_len, d_model):\n",
    "        pos = tf.range(max_len, dtype=tf.float32)\n",
    "        i = tf.range(d_model, dtype=tf.float32)\n",
    "        angle_rates = 1 / tf.pow(10000, (2 * i // 2) / d_model)\n",
    "        angles = pos[:, tf.newaxis] * angle_rates[tf.newaxis, :]\n",
    "        angle_rads = angles * tf.constant(np.pi / 180.0)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = self.point_wise_feed_forward_network(d_model, d_ff)\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def point_wise_feed_forward_network(self, d_model, d_ff):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layer_norm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layer_norm2(out1 + ffn_output)\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, target_vocab_size, max_len, dropout_rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = self.point_wise_feed_forward_network(d_model, d_ff)\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = layers.Dropout(dropout_rate)\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def point_wise_feed_forward_network(self, d_model, d_ff):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, encoder_output, padding_mask, look_ahead_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout2(attn1)\n",
    "        out1 = self.layer_norm1(x + attn1)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(encoder_output, encoder_output, out1, padding_mask)\n",
    "        attn2 = self.dropout3(attn2)\n",
    "        out2 = self.layer_norm2(out1 + attn2)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out3 = self.layer_norm3(out2 + ffn_output)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, d_model, num_heads, d_ff, input_vocab_size, target_vocab_size, max_len, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(d_model, num_heads, d_ff, dropout_rate)\n",
    "        self.decoder = TransformerDecoder(d_model, num_heads, d_ff, target_vocab_size, max_len, dropout_rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inp, padding_mask, look_ahead_mask = inputs\n",
    "        encoder_output = self.encoder(inp)\n",
    "        decoder_output, _, _ = self.decoder(inp, encoder_output, padding_mask, look_ahead_mask)\n",
    "        final_output = self.final_layer(decoder_output)\n",
    "        return final_output\n",
    "\n",
    "# Data preparation and masking functions\n",
    "def prepare_data(inputs, max_len):\n",
    "    inp_data = pad_sequences(inputs, padding='post')\n",
    "    padding_mask = create_padding_mask(inp_data)\n",
    "    look_ahead_mask = create_look_ahead_masks(inp_data)\n",
    "    return inp_data, padding_mask, look_ahead_mask\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    mask = tf.math.equal(seq, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask[tf.newaxis, tf.newaxis, :size, :size]\n",
    "\n",
    "def create_look_ahead_masks(input_data):\n",
    "    max_len = tf.shape(input_data)[1]\n",
    "    mask = create_look_ahead_mask(max_len)\n",
    "    return tf.tile(mask, [tf.shape(input_data)[0], 1, 1, 1])\n",
    "\n",
    "input_vocab_size = 510\n",
    "max_len = max(len(seq) for seq in tokenized_smiles)\n",
    "\n",
    "max_len\n",
    "\n",
    "d_model = 64  # You can adjust this based on your needs\n",
    "num_heads = 4  # You can adjust this based on your needs\n",
    "d_ff = 128  # You can adjust this based on your needs\n",
    "dropout_rate = 0.2  # You can adjust this based on your needs\n",
    "\n",
    "# Prepare data and train the model\n",
    "inp_data, padding_mask, look_ahead_mask = prepare_data(tokenized_smiles, max_len)\n",
    "\n",
    "# Create the Transformer model\n",
    "transformer = Transformer(d_model, num_heads, d_ff, input_vocab_size, input_vocab_size, max_len, dropout_rate)\n",
    "\n",
    "# Define loss function (unsupervised loss, e.g., mean squared error or binary cross-entropy)\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "input_data = {\n",
    "    \"inp_data\": inp_data,\n",
    "    \"padding_mask\": padding_mask,\n",
    "    \"look_ahead_mask\": look_ahead_mask\n",
    "}\n",
    "\n",
    "print(inp_data.shape, padding_mask.shape, look_ahead_mask.shape)\n",
    "\n",
    "# Train the model on the data (use the same inp_data as both input and target)\n",
    "transformer.fit({'inp_data': inp_data, 'padding_mask': padding_mask, 'look_ahead_mask': look_ahead_mask}, inp_data, batch_size=3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f32951-0bc5-4c42-8ac1-93d2357ff39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e54ac-1a4c-41b3-acaa-61400bc775db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c63d624-fbe4-4e0c-8832-bde40196bf0e",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad258e-b398-483f-9359-dfa5cbc9cb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
